<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="generator" content="ExDoc v0.39.1">
    <meta name="project" content="hornbeam v1.0.0">


    <title>distributed-ml — hornbeam v1.0.0</title>

    <link rel="stylesheet" href="dist/html-erlang-7YNYEB7F.css" />

    <script defer src="dist/sidebar_items-DCB24D7B.js"></script>
    <script defer src="docs_config.js"></script>
    <script defer src="dist/html-HBZYRXZS.js"></script>

  </head>
  <body>
    <script>(()=>{var t="ex_doc:settings",e="dark";var o="dark",s="light";var E="sidebar_state",n="closed";var r="sidebar_width";var a="sidebar-open";var i=new URLSearchParams(window.location.search),S=i.get("theme")||JSON.parse(localStorage.getItem(t)||"{}").theme;(S===o||S!==s&&window.matchMedia("(prefers-color-scheme: dark)").matches)&&document.body.classList.add(e);var d=sessionStorage.getItem(E),A=d!==n&&!window.matchMedia(`screen and (max-width: ${768}px)`).matches;document.body.classList.toggle(a,A);var c=sessionStorage.getItem(r);c&&document.body.style.setProperty("--sidebarWidth",`${c}px`);var p=/(Macintosh|iPhone|iPad|iPod)/.test(window.navigator.userAgent);document.documentElement.classList.toggle("apple-os",p);})();
</script>

<div class="body-wrapper">

<button id="sidebar-menu" class="sidebar-button sidebar-toggle" aria-label="toggle sidebar" aria-controls="sidebar">
  <i class="ri-menu-line ri-lg" title="Collapse/expand sidebar"></i>
</button>

<nav id="sidebar" class="sidebar">

  <div class="sidebar-header">
    <div class="sidebar-projectInfo">

      <div>
        <a href="https://hornbeam.dev" class="sidebar-projectName" translate="no">
hornbeam
        </a>
        <div class="sidebar-projectVersion" translate="no">
          v1.0.0
        </div>
      </div>
    </div>
    <ul id="sidebar-list-nav" class="sidebar-list-nav" role="tablist" data-extras=""></ul>
  </div>
</nav>

<output role="status" id="toast"></output>

<main class="content page-extra" id="main" data-type="extras">
  <div id="content" class="content-inner">
    <div class="top-search">
      <div class="search-settings">
        <form class="search-bar" action="search.html">
          <label class="search-label">
            <span class="sr-only">Search documentation of hornbeam</span>
            <div class="search-input-wrapper">
              <input name="q" type="text" class="search-input" placeholder="Press / to search" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" />
              <button type="button" tabindex="-1" class="search-close-button" aria-hidden="true">
                <i class="ri-close-line ri-lg" title="Cancel search"></i>
              </button>
            </div>
          </label>
        </form>
        <div class="autocomplete">
        </div>
        <div class="engine-selector" data-multiple="false">
          <button type="button" class="engine-button" aria-label="Select search engine" aria-haspopup="true" aria-expanded="false">
            <i class="ri-search-2-line" aria-hidden="true"></i>
            <span class="engine-name">Default</span>
            <i class="ri-arrow-down-s-line" aria-hidden="true"></i>
          </button>
          <div class="engine-dropdown" hidden role="menu">

              <button type="button"
                      class="engine-option"
                      data-engine-url="search.html?q="
                      role="menuitemradio"
                      aria-checked="true">
                <span class="name">Default</span>
                <span class="help">In-browser search</span>
              </button>

          </div>
        </div>
        <button class="icon-settings display-settings">
          <i class="ri-settings-3-line"></i>
          <span class="sr-only">Settings</span>
        </button>
      </div>
    </div>

<div id="top-content">
  <div class="heading-with-actions top-heading">
    <h1>distributed-ml</h1>


      <a href="https://github.com/benoitc/hornbeam/blob/v1.0.0/docs/examples/distributed-ml.md#L1" title="View Source" class="icon-action" rel="help">
        <i class="ri-code-s-slash-line" aria-hidden="true"></i>
        <span class="sr-only">View Source</span>
      </a>

  </div>


<hr class="thin"/><p>title: Distributed ML
description: Distribute ML inference across an Erlang cluster
order: 24</p><hr class="thin"/><h1>Distributed ML Example</h1><p>This example shows how to distribute ML inference across multiple Erlang nodes, leveraging the BEAM's built-in clustering.</p><h2 id="architecture" class="section-heading"><a href="#architecture" class="hover-link"><i class="ri-link-m" aria-hidden="true"></i></a><span class="text">Architecture</span></h2><pre><code class="makeup erlang" translate="no"><span class="w">                    </span><span class="err">┌</span><span class="err">─</span><span class="err">─</span><span class="err">─</span><span class="err">─</span><span class="err">─</span><span class="err">─</span><span class="err">─</span><span class="err">─</span><span class="err">─</span><span class="err">─</span><span class="err">─</span><span class="err">─</span><span class="err">─</span><span class="err">─</span><span class="err">─</span><span class="err">─</span><span class="err">─</span><span class="err">┐</span><span class="w">
                    </span><span class="err">│</span><span class="w">   </span><span class="n">Web</span><span class="w"> </span><span class="n">Server</span><span class="w">    </span><span class="err">│</span><span class="w">
                    </span><span class="err">│</span><span class="w">  </span><span class="p" data-group-id="2379363751-1">(</span><span class="n">Hornbeam</span><span class="p" data-group-id="2379363751-1">)</span><span class="w">     </span><span class="err">│</span><span class="w">
                    </span><span class="err">└</span><span class="err">─</span><span class="err">─</span><span class="err">─</span><span class="err">─</span><span class="err">─</span><span class="err">─</span><span class="err">─</span><span class="err">─</span><span class="err">┬</span><span class="err">─</span><span class="err">─</span><span class="err">─</span><span class="err">─</span><span class="err">─</span><span class="err">─</span><span class="err">─</span><span class="err">─</span><span class="err">┘</span><span class="w">
                             </span><span class="err">│</span><span class="w">
              </span><span class="err">┌</span><span class="err">─</span><span class="err">─</span><span class="err">─</span><span class="err">─</span><span class="err">─</span><span class="err">─</span><span class="err">─</span><span class="err">─</span><span class="err">─</span><span class="err">─</span><span class="err">─</span><span class="err">─</span><span class="err">─</span><span class="err">─</span><span class="err">┼</span><span class="err">─</span><span class="err">─</span><span class="err">─</span><span class="err">─</span><span class="err">─</span><span class="err">─</span><span class="err">─</span><span class="err">─</span><span class="err">─</span><span class="err">─</span><span class="err">─</span><span class="err">─</span><span class="err">─</span><span class="err">─</span><span class="err">┐</span><span class="w">
              </span><span class="err">│</span><span class="w">              </span><span class="err">│</span><span class="w">              </span><span class="err">│</span><span class="w">
       </span><span class="err">┌</span><span class="err">─</span><span class="err">─</span><span class="err">─</span><span class="err">─</span><span class="err">─</span><span class="err">─</span><span class="err">▼</span><span class="err">─</span><span class="err">─</span><span class="err">─</span><span class="err">─</span><span class="err">─</span><span class="err">┐</span><span class="w"> </span><span class="err">┌</span><span class="err">─</span><span class="err">─</span><span class="err">─</span><span class="err">─</span><span class="err">─</span><span class="err">─</span><span class="err">▼</span><span class="err">─</span><span class="err">─</span><span class="err">─</span><span class="err">─</span><span class="err">─</span><span class="err">┐</span><span class="w"> </span><span class="err">┌</span><span class="err">─</span><span class="err">─</span><span class="err">─</span><span class="err">─</span><span class="err">─</span><span class="err">─</span><span class="err">▼</span><span class="err">─</span><span class="err">─</span><span class="err">─</span><span class="err">─</span><span class="err">─</span><span class="err">┐</span><span class="w">
       </span><span class="err">│</span><span class="w">  </span><span class="n">GPU</span><span class="w"> </span><span class="n">Node</span><span class="w">  </span><span class="err">│</span><span class="w"> </span><span class="err">│</span><span class="w">  </span><span class="n">GPU</span><span class="w"> </span><span class="n">Node</span><span class="w">  </span><span class="err">│</span><span class="w"> </span><span class="err">│</span><span class="w">  </span><span class="n">GPU</span><span class="w"> </span><span class="n">Node</span><span class="w">  </span><span class="err">│</span><span class="w">
       </span><span class="err">│</span><span class="w">  </span><span class="ss">worker1@</span><span class="w">  </span><span class="err">│</span><span class="w"> </span><span class="err">│</span><span class="w">  </span><span class="ss">worker2@</span><span class="w">  </span><span class="err">│</span><span class="w"> </span><span class="err">│</span><span class="w">  </span><span class="ss">worker3@</span><span class="w">  </span><span class="err">│</span><span class="w">
       </span><span class="err">└</span><span class="err">─</span><span class="err">─</span><span class="err">─</span><span class="err">─</span><span class="err">─</span><span class="err">─</span><span class="err">─</span><span class="err">─</span><span class="err">─</span><span class="err">─</span><span class="err">─</span><span class="err">─</span><span class="err">┘</span><span class="w"> </span><span class="err">└</span><span class="err">─</span><span class="err">─</span><span class="err">─</span><span class="err">─</span><span class="err">─</span><span class="err">─</span><span class="err">─</span><span class="err">─</span><span class="err">─</span><span class="err">─</span><span class="err">─</span><span class="err">─</span><span class="err">┘</span><span class="w"> </span><span class="err">└</span><span class="err">─</span><span class="err">─</span><span class="err">─</span><span class="err">─</span><span class="err">─</span><span class="err">─</span><span class="err">─</span><span class="err">─</span><span class="err">─</span><span class="err">─</span><span class="err">─</span><span class="err">─</span><span class="err">┘</span></code></pre><h2 id="web-server-hornbeam" class="section-heading"><a href="#web-server-hornbeam" class="hover-link"><i class="ri-link-m" aria-hidden="true"></i></a><span class="text">Web Server (Hornbeam)</span></h2><pre><code class="python"># app.py
from fastapi import FastAPI
from pydantic import BaseModel
from hornbeam_erlang import rpc_call, nodes, node
from hornbeam_ml import cached_inference
import asyncio
from typing import Optional

app = FastAPI()

# ============================================================
# Models
# ============================================================

class InferenceRequest(BaseModel):
    texts: list[str]
    model: str = &quot;default&quot;

class InferenceResponse(BaseModel):
    embeddings: list[list[float]]
    node: str
    cached: int

# ============================================================
# Distributed Inference
# ============================================================

def get_gpu_nodes():
    &quot;&quot;&quot;Get available GPU worker nodes.&quot;&quot;&quot;
    return [n for n in nodes() if n.startswith('gpu') or n.startswith('worker')]

def select_node(nodes_list):
    &quot;&quot;&quot;Select node with least load (round-robin for simplicity).&quot;&quot;&quot;
    if not nodes_list:
        return None
    # Simple round-robin
    from hornbeam_erlang import state_incr
    idx = state_incr('node_selector') % len(nodes_list)
    return nodes_list[idx]

@app.post(&quot;/infer&quot;, response_model=InferenceResponse)
async def distributed_inference(request: InferenceRequest):
    &quot;&quot;&quot;Run inference on a GPU node.&quot;&quot;&quot;
    gpu_nodes = get_gpu_nodes()

    if not gpu_nodes:
        # Fallback to local inference
        from sentence_transformers import SentenceTransformer
        model = SentenceTransformer('all-MiniLM-L6-v2')
        embeddings = model.encode(request.texts)
        return InferenceResponse(
            embeddings=embeddings.tolist(),
            node=node(),
            cached=0
        )

    # Select a GPU node
    target_node = select_node(gpu_nodes)

    # Call remote node
    result = rpc_call(
        target_node,
        'ml_worker',
        'encode',
        [request.texts, request.model],
        timeout_ms=60000
    )

    return InferenceResponse(
        embeddings=result['embeddings'],
        node=target_node,
        cached=result.get('cached', 0)
    )

@app.post(&quot;/infer/parallel&quot;)
async def parallel_inference(request: InferenceRequest):
    &quot;&quot;&quot;Distribute across all GPU nodes in parallel.&quot;&quot;&quot;
    gpu_nodes = get_gpu_nodes()

    if not gpu_nodes:
        raise HTTPException(status_code=503, detail=&quot;No GPU nodes available&quot;)

    # Split texts across nodes
    n_nodes = len(gpu_nodes)
    chunk_size = (len(request.texts) + n_nodes - 1) // n_nodes
    chunks = [
        request.texts[i:i + chunk_size]
        for i in range(0, len(request.texts), chunk_size)
    ]

    # Call all nodes in parallel
    import concurrent.futures

    def call_node(node, texts):
        return rpc_call(
            node,
            'ml_worker',
            'encode',
            [texts, request.model],
            timeout_ms=60000
        )

    results = []
    with concurrent.futures.ThreadPoolExecutor(max_workers=n_nodes) as executor:
        futures = {
            executor.submit(call_node, node, chunk): node
            for node, chunk in zip(gpu_nodes, chunks)
            if chunk
        }

        for future in concurrent.futures.as_completed(futures):
            result = future.result()
            results.extend(result['embeddings'])

    return {
        'embeddings': results,
        'nodes_used': len(gpu_nodes)
    }

# ============================================================
# Cluster Status
# ============================================================

@app.get(&quot;/cluster&quot;)
async def cluster_status():
    &quot;&quot;&quot;Get cluster status.&quot;&quot;&quot;
    gpu_nodes = get_gpu_nodes()

    status = {
        'this_node': node(),
        'gpu_nodes': [],
        'total_nodes': len(nodes())
    }

    for gpu_node in gpu_nodes:
        try:
            info = rpc_call(gpu_node, 'ml_worker', 'info', [], timeout_ms=5000)
            status['gpu_nodes'].append({
                'node': gpu_node,
                'status': 'online',
                **info
            })
        except Exception as e:
            status['gpu_nodes'].append({
                'node': gpu_node,
                'status': 'offline',
                'error': str(e)
            })

    return status</code></pre><h2 id="gpu-worker-node" class="section-heading"><a href="#gpu-worker-node" class="hover-link"><i class="ri-link-m" aria-hidden="true"></i></a><span class="text">GPU Worker Node</span></h2><h3 id="erlang-module" class="section-heading"><a href="#erlang-module" class="hover-link"><i class="ri-link-m" aria-hidden="true"></i></a><span class="text">Erlang Module</span></h3><pre><code class="makeup erlang" translate="no"><span class="c1">%% ml_worker.erl</span><span class="w">
</span><span class="p">-</span><span class="na">module</span><span class="p" data-group-id="7052429107-1">(</span><span class="ss">ml_worker</span><span class="p" data-group-id="7052429107-1">)</span><span class="p">.</span><span class="w">
</span><span class="p">-</span><span class="na">export</span><span class="p" data-group-id="7052429107-2">(</span><span class="p" data-group-id="7052429107-3">[</span><span class="ss">encode</span><span class="p">/</span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="ss">info</span><span class="p">/</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="ss">predict</span><span class="p">/</span><span class="mi">2</span><span class="p" data-group-id="7052429107-3">]</span><span class="p" data-group-id="7052429107-2">)</span><span class="p">.</span><span class="w">

</span><span class="nf">encode</span><span class="p" data-group-id="7052429107-4">(</span><span class="n">Texts</span><span class="p">,</span><span class="w"> </span><span class="n">Model</span><span class="p" data-group-id="7052429107-4">)</span><span class="w"> </span><span class="p">-&gt;</span><span class="w">
    </span><span class="c1">%% Call Python with caching</span><span class="w">
    </span><span class="n">Result</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nc">py</span><span class="p">:</span><span class="nf">call</span><span class="p" data-group-id="7052429107-5">(</span><span class="ss">&#39;ml_service&#39;</span><span class="p">,</span><span class="w"> </span><span class="ss">&#39;encode&#39;</span><span class="p">,</span><span class="w"> </span><span class="p" data-group-id="7052429107-6">[</span><span class="n">Texts</span><span class="p">,</span><span class="w"> </span><span class="n">Model</span><span class="p" data-group-id="7052429107-6">]</span><span class="p" data-group-id="7052429107-5">)</span><span class="p">,</span><span class="w">
    </span><span class="n">Result</span><span class="p">.</span><span class="w">

</span><span class="nf">predict</span><span class="p" data-group-id="7052429107-7">(</span><span class="n">Input</span><span class="p">,</span><span class="w"> </span><span class="n">Model</span><span class="p" data-group-id="7052429107-7">)</span><span class="w"> </span><span class="p">-&gt;</span><span class="w">
    </span><span class="nc">py</span><span class="p">:</span><span class="nf">call</span><span class="p" data-group-id="7052429107-8">(</span><span class="ss">&#39;ml_service&#39;</span><span class="p">,</span><span class="w"> </span><span class="ss">&#39;predict&#39;</span><span class="p">,</span><span class="w"> </span><span class="p" data-group-id="7052429107-9">[</span><span class="n">Input</span><span class="p">,</span><span class="w"> </span><span class="n">Model</span><span class="p" data-group-id="7052429107-9">]</span><span class="p" data-group-id="7052429107-8">)</span><span class="p">.</span><span class="w">

</span><span class="nf">info</span><span class="p" data-group-id="7052429107-10">(</span><span class="p" data-group-id="7052429107-10">)</span><span class="w"> </span><span class="p">-&gt;</span><span class="w">
    </span><span class="p" data-group-id="7052429107-11">#{</span><span class="w">
        </span><span class="ss">model</span><span class="w"> </span><span class="p">=&gt;</span><span class="w"> </span><span class="nc">py</span><span class="p">:</span><span class="nf">call</span><span class="p" data-group-id="7052429107-12">(</span><span class="ss">&#39;ml_service&#39;</span><span class="p">,</span><span class="w"> </span><span class="ss">&#39;get_model_info&#39;</span><span class="p">,</span><span class="w"> </span><span class="p" data-group-id="7052429107-13">[</span><span class="p" data-group-id="7052429107-13">]</span><span class="p" data-group-id="7052429107-12">)</span><span class="p">,</span><span class="w">
        </span><span class="nb">memory</span><span class="w"> </span><span class="p">=&gt;</span><span class="w"> </span><span class="nc">py</span><span class="p">:</span><span class="nf">memory_stats</span><span class="p" data-group-id="7052429107-14">(</span><span class="p" data-group-id="7052429107-14">)</span><span class="p">,</span><span class="w">
        </span><span class="ss">cache</span><span class="w"> </span><span class="p">=&gt;</span><span class="w"> </span><span class="nc">py</span><span class="p">:</span><span class="nf">call</span><span class="p" data-group-id="7052429107-15">(</span><span class="ss">&#39;ml_service&#39;</span><span class="p">,</span><span class="w"> </span><span class="ss">&#39;cache_stats&#39;</span><span class="p">,</span><span class="w"> </span><span class="p" data-group-id="7052429107-16">[</span><span class="p" data-group-id="7052429107-16">]</span><span class="p" data-group-id="7052429107-15">)</span><span class="w">
    </span><span class="p" data-group-id="7052429107-11">}</span><span class="p">.</span></code></pre><h3 id="python-service" class="section-heading"><a href="#python-service" class="hover-link"><i class="ri-link-m" aria-hidden="true"></i></a><span class="text">Python Service</span></h3><pre><code class="python"># ml_service.py
from sentence_transformers import SentenceTransformer
from hornbeam_ml import cached_inference, cache_stats as get_cache_stats
from hornbeam_erlang import state_get, state_set

# Load model at import
models = {}

def get_model(name='default'):
    if name not in models:
        model_map = {
            'default': 'all-MiniLM-L6-v2',
            'large': 'all-mpnet-base-v2',
            'multilingual': 'paraphrase-multilingual-MiniLM-L12-v2'
        }
        model_name = model_map.get(name, name)
        models[name] = SentenceTransformer(model_name)
    return models[name]

def encode(texts, model_name='default'):
    &quot;&quot;&quot;Encode texts with caching.&quot;&quot;&quot;
    model = get_model(model_name)

    embeddings = []
    cached_count = 0

    for text in texts:
        cache_key = f'{model_name}:{hash(text)}'
        cached = state_get(f'emb:{cache_key}')

        if cached:
            embeddings.append(cached)
            cached_count += 1
        else:
            emb = model.encode(text).tolist()
            state_set(f'emb:{cache_key}', emb)
            embeddings.append(emb)

    return {
        'embeddings': embeddings,
        'cached': cached_count
    }

def predict(input_data, model_name='default'):
    &quot;&quot;&quot;Run prediction.&quot;&quot;&quot;
    model = get_model(model_name)
    return model.encode(input_data).tolist()

def get_model_info():
    &quot;&quot;&quot;Get loaded model info.&quot;&quot;&quot;
    return {
        'loaded_models': list(models.keys()),
        'default_dimensions': get_model().get_sentence_embedding_dimension()
    }

def cache_stats():
    return get_cache_stats()</code></pre><h2 id="starting-the-cluster" class="section-heading"><a href="#starting-the-cluster" class="hover-link"><i class="ri-link-m" aria-hidden="true"></i></a><span class="text">Starting the Cluster</span></h2><h3 id="start-gpu-worker-nodes" class="section-heading"><a href="#start-gpu-worker-nodes" class="hover-link"><i class="ri-link-m" aria-hidden="true"></i></a><span class="text">Start GPU Worker Nodes</span></h3><pre><code class="makeup bash" translate="no"><span class=""># On gpu-server-1
</span><span class="">erl -name worker1@gpu-server-1 -setcookie mysecret
</span><span class="">
</span><span class=""># In the shell
</span><span class="">application:ensure_all_started(erlang_python).
</span></code></pre><h3 id="start-web-server" class="section-heading"><a href="#start-web-server" class="hover-link"><i class="ri-link-m" aria-hidden="true"></i></a><span class="text">Start Web Server</span></h3><pre><code class="makeup bash" translate="no"><span class=""># On web-server
</span><span class="">erl -name web@web-server -setcookie mysecret
</span><span class="">
</span><span class=""># Connect to GPU nodes
</span><span class="">net_adm:ping(&#39;worker1@gpu-server-1&#39;).
</span><span class="">net_adm:ping(&#39;worker2@gpu-server-2&#39;).
</span><span class="">
</span><span class=""># Start Hornbeam
</span><span class="">hornbeam:start(&quot;app:app&quot;, #{
</span><span class="">    worker_class =&gt; asgi,
</span><span class="">    pythonpath =&gt; [&quot;distributed_ml&quot;]
</span><span class="">}).
</span></code></pre><h2 id="load-balancing-strategies" class="section-heading"><a href="#load-balancing-strategies" class="hover-link"><i class="ri-link-m" aria-hidden="true"></i></a><span class="text">Load Balancing Strategies</span></h2><h3 id="1-round-robin-simple" class="section-heading"><a href="#1-round-robin-simple" class="hover-link"><i class="ri-link-m" aria-hidden="true"></i></a><span class="text">1. Round-Robin (Simple)</span></h3><pre><code class="python">def select_node_round_robin(nodes_list):
    idx = state_incr('node_rr') % len(nodes_list)
    return nodes_list[idx]</code></pre><h3 id="2-least-loaded" class="section-heading"><a href="#2-least-loaded" class="hover-link"><i class="ri-link-m" aria-hidden="true"></i></a><span class="text">2. Least Loaded</span></h3><pre><code class="python">def select_node_least_loaded(nodes_list):
    loads = []
    for n in nodes_list:
        try:
            load = rpc_call(n, 'ml_worker', 'get_load', [], timeout_ms=1000)
            loads.append((n, load))
        except:
            loads.append((n, float('inf')))

    return min(loads, key=lambda x: x[1])[0]</code></pre><h3 id="3-consistent-hashing-for-caching" class="section-heading"><a href="#3-consistent-hashing-for-caching" class="hover-link"><i class="ri-link-m" aria-hidden="true"></i></a><span class="text">3. Consistent Hashing (for caching)</span></h3><pre><code class="python">import hashlib

def select_node_consistent(nodes_list, key):
    &quot;&quot;&quot;Select node based on key hash for cache locality.&quot;&quot;&quot;
    hash_val = int(hashlib.md5(key.encode()).hexdigest(), 16)
    idx = hash_val % len(nodes_list)
    return sorted(nodes_list)[idx]</code></pre><h2 id="fault-tolerance" class="section-heading"><a href="#fault-tolerance" class="hover-link"><i class="ri-link-m" aria-hidden="true"></i></a><span class="text">Fault Tolerance</span></h2><pre><code class="python">async def resilient_inference(texts, retries=2):
    &quot;&quot;&quot;Inference with automatic failover.&quot;&quot;&quot;
    gpu_nodes = get_gpu_nodes()

    for attempt in range(retries + 1):
        if not gpu_nodes:
            break

        target = select_node(gpu_nodes)

        try:
            result = rpc_call(
                target,
                'ml_worker',
                'encode',
                [texts, 'default'],
                timeout_ms=30000
            )
            return result
        except Exception as e:
            print(f&quot;Node {target} failed: {e}&quot;)
            gpu_nodes.remove(target)

    # All nodes failed, try local
    return local_inference(texts)</code></pre><h2 id="monitoring" class="section-heading"><a href="#monitoring" class="hover-link"><i class="ri-link-m" aria-hidden="true"></i></a><span class="text">Monitoring</span></h2><pre><code class="python">@app.get(&quot;/metrics&quot;)
async def metrics():
    &quot;&quot;&quot;Cluster-wide metrics.&quot;&quot;&quot;
    from hornbeam_erlang import state_get

    return {
        'requests': {
            'total': state_get('metrics:requests') or 0,
            'distributed': state_get('metrics:distributed') or 0,
            'local_fallback': state_get('metrics:local') or 0
        },
        'cluster': {
            'nodes': len(get_gpu_nodes()),
            'node_selector_count': state_get('node_selector') or 0
        }
    }</code></pre><h2 id="next-steps" class="section-heading"><a href="#next-steps" class="hover-link"><i class="ri-link-m" aria-hidden="true"></i></a><span class="text">Next Steps</span></h2><ul><li><a href="../guides/ml-integration">ML Integration Guide</a> - Caching patterns</li><li><a href="../guides/erlang-integration">Erlang Integration Guide</a> - RPC details</li></ul>

</div>

<div class="bottom-actions" id="bottom-actions">
  <div class="bottom-actions-item">

      <a href="embedding-service.html" class="bottom-actions-button" rel="prev">
        <span class="subheader">
          ← Previous Page
        </span>
        <span class="title">
embedding-service
        </span>
      </a>

  </div>
  <div class="bottom-actions-item">

      <a href="configuration.html" class="bottom-actions-button" rel="next">
        <span class="subheader">
          Next Page →
        </span>
        <span class="title">
configuration
        </span>
      </a>

  </div>
</div>
    <footer class="footer">
      <p>

          <span class="line">
            <a href="https://hex.pm/packages/hornbeam/1.0.0" class="footer-hex-package">Hex Package</a>

            <a href="https://preview.hex.pm/preview/hornbeam/1.0.0">Hex Preview</a>

              (<a href="https://preview.hex.pm/preview/hornbeam/1.0.0/show/docs/examples/distributed-ml.md">current file</a>)

          </span>

        <span class="line">
          <button class="a-main footer-button display-quick-switch" title="Search HexDocs packages">
            Search HexDocs
          </button>

            <a href="hornbeam.epub" title="ePub version">
              Download ePub version
            </a>

        </span>
      </p>

      <p class="built-using">
        Built using
        <a href="https://github.com/elixir-lang/ex_doc" title="ExDoc" target="_blank" rel="help noopener" translate="no">ExDoc</a> (v0.39.1) for the

          <a href="https://erlang.org" title="Erlang" target="_blank" translate="no">Erlang programming language</a>

      </p>

    </footer>
  </div>
</main>
</div>

  </body>
</html>
