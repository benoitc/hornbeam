searchData={"items":[{"type":"module","title":"hornbeam","doc":"Main API for hornbeam WSGI/ASGI server. Hornbeam is an Erlang-based WSGI/ASGI server that uses erlang-python for Python execution and Cowboy for HTTP handling. Quick Start <span class=\"w\">   </span><span class=\"c1\">%% Start with a WSGI application</span><span class=\"w\">\n   </span><span class=\"nc\">hornbeam</span><span class=\"p\">:</span><span class=\"nf\">start</span><span class=\"p\" data-group-id=\"5021875160-1\">(</span><span class=\"s\">&quot;myapp:application&quot;</span><span class=\"p\" data-group-id=\"5021875160-1\">)</span><span class=\"p\">.</span><span class=\"w\">\n  \n   </span><span class=\"c1\">%% Start with options</span><span class=\"w\">\n   </span><span class=\"nc\">hornbeam</span><span class=\"p\">:</span><span class=\"nf\">start</span><span class=\"p\" data-group-id=\"5021875160-2\">(</span><span class=\"s\">&quot;myapp:application&quot;</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"p\" data-group-id=\"5021875160-3\">#{</span><span class=\"w\">\n       </span><span class=\"ss\">bind</span><span class=\"w\"> </span><span class=\"p\">=&gt;</span><span class=\"w\"> </span><span class=\"s\">&quot;0.0.0.0:8000&quot;</span><span class=\"p\">,</span><span class=\"w\">\n       </span><span class=\"ss\">workers</span><span class=\"w\"> </span><span class=\"p\">=&gt;</span><span class=\"w\"> </span><span class=\"mi\">4</span><span class=\"w\">\n   </span><span class=\"p\" data-group-id=\"5021875160-3\">}</span><span class=\"p\" data-group-id=\"5021875160-2\">)</span><span class=\"p\">.</span><span class=\"w\">\n  \n   </span><span class=\"c1\">%% Start ASGI app with lifespan</span><span class=\"w\">\n   </span><span class=\"nc\">hornbeam</span><span class=\"p\">:</span><span class=\"nf\">start</span><span class=\"p\" data-group-id=\"5021875160-4\">(</span><span class=\"s\">&quot;myapp:app&quot;</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"p\" data-group-id=\"5021875160-5\">#{</span><span class=\"w\">\n       </span><span class=\"ss\">worker_class</span><span class=\"w\"> </span><span class=\"p\">=&gt;</span><span class=\"w\"> </span><span class=\"ss\">asgi</span><span class=\"p\">,</span><span class=\"w\">\n       </span><span class=\"ss\">lifespan</span><span class=\"w\"> </span><span class=\"p\">=&gt;</span><span class=\"w\"> </span><span class=\"ss\">on</span><span class=\"w\">\n   </span><span class=\"p\" data-group-id=\"5021875160-5\">}</span><span class=\"p\" data-group-id=\"5021875160-4\">)</span><span class=\"p\">.</span><span class=\"w\">\n  \n   </span><span class=\"c1\">%% Register Erlang functions callable from Python</span><span class=\"w\">\n   </span><span class=\"nc\">hornbeam</span><span class=\"p\">:</span><span class=\"nf\">register_function</span><span class=\"p\" data-group-id=\"5021875160-6\">(</span><span class=\"ss\">my_func</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"nf\">fun</span><span class=\"p\" data-group-id=\"5021875160-7\">(</span><span class=\"p\" data-group-id=\"5021875160-8\">[</span><span class=\"n\">Arg</span><span class=\"p\" data-group-id=\"5021875160-8\">]</span><span class=\"p\" data-group-id=\"5021875160-7\">)</span><span class=\"w\"> </span><span class=\"p\">-&gt;</span><span class=\"w\"> </span><span class=\"nf\">process</span><span class=\"p\" data-group-id=\"5021875160-9\">(</span><span class=\"n\">Arg</span><span class=\"p\" data-group-id=\"5021875160-9\">)</span><span class=\"w\"> </span><span class=\"k\">end</span><span class=\"p\" data-group-id=\"5021875160-6\">)</span><span class=\"p\">.</span><span class=\"w\">\n  \n   </span><span class=\"c1\">%% Stop the server</span><span class=\"w\">\n   </span><span class=\"nc\">hornbeam</span><span class=\"p\">:</span><span class=\"nf\">stop</span><span class=\"p\" data-group-id=\"5021875160-10\">(</span><span class=\"p\" data-group-id=\"5021875160-10\">)</span><span class=\"p\">.</span>","ref":"hornbeam.html"},{"type":"type","title":"hornbeam.app_spec/0","doc":"","ref":"hornbeam.html#t:app_spec/0"},{"type":"type","title":"hornbeam.options/0","doc":"","ref":"hornbeam.html#t:options/0"},{"type":"function","title":"hornbeam.register_function/2","doc":"Register an Erlang function to be callable from Python. The function should accept a list of arguments and return a term. Example: <span class=\"w\">  </span><span class=\"nc\">hornbeam</span><span class=\"p\">:</span><span class=\"nf\">register_function</span><span class=\"p\" data-group-id=\"3134170642-1\">(</span><span class=\"ss\">cache_get</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"nf\">fun</span><span class=\"p\" data-group-id=\"3134170642-2\">(</span><span class=\"p\" data-group-id=\"3134170642-3\">[</span><span class=\"n\">Key</span><span class=\"p\" data-group-id=\"3134170642-3\">]</span><span class=\"p\" data-group-id=\"3134170642-2\">)</span><span class=\"w\"> </span><span class=\"p\">-&gt;</span><span class=\"w\">\n      </span><span class=\"k\">case</span><span class=\"w\"> </span><span class=\"nc\">ets</span><span class=\"p\">:</span><span class=\"nf\">lookup</span><span class=\"p\" data-group-id=\"3134170642-4\">(</span><span class=\"ss\">my_cache</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">Key</span><span class=\"p\" data-group-id=\"3134170642-4\">)</span><span class=\"w\"> </span><span class=\"k\">of</span><span class=\"w\">\n          </span><span class=\"p\" data-group-id=\"3134170642-5\">[</span><span class=\"p\" data-group-id=\"3134170642-6\">{</span><span class=\"p\">_</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">Value</span><span class=\"p\" data-group-id=\"3134170642-6\">}</span><span class=\"p\" data-group-id=\"3134170642-5\">]</span><span class=\"w\"> </span><span class=\"p\">-&gt;</span><span class=\"w\"> </span><span class=\"n\">Value</span><span class=\"p\">;</span><span class=\"w\">\n          </span><span class=\"p\" data-group-id=\"3134170642-7\">[</span><span class=\"p\" data-group-id=\"3134170642-7\">]</span><span class=\"w\"> </span><span class=\"p\">-&gt;</span><span class=\"w\"> </span><span class=\"ss\">none</span><span class=\"w\">\n      </span><span class=\"k\">end</span><span class=\"w\">\n  </span><span class=\"k\">end</span><span class=\"p\" data-group-id=\"3134170642-1\">)</span><span class=\"p\">.</span>","ref":"hornbeam.html#register_function/2"},{"type":"function","title":"hornbeam.register_function/3","doc":"Register an Erlang module:function to be callable from Python.","ref":"hornbeam.html#register_function/3"},{"type":"function","title":"hornbeam.start/1","doc":"Start hornbeam with a WSGI/ASGI application. The application spec is in the format \"module:callable\" (e.g., \"myapp:application\").","ref":"hornbeam.html#start/1"},{"type":"function","title":"hornbeam.start/2","doc":"Start hornbeam with a WSGI/ASGI application and options. Options: bind  - Address to bind to (default: \"127.0.0.1:8000\") workers  - Number of Python workers (default: 4) worker_class  - wsgi or asgi (default: wsgi) timeout  - Request timeout in ms (default: 30000) keepalive  - Keep-alive timeout in seconds (default: 2) max_requests  - Max requests per worker before restart (default: 1000) max_concurrent  - Max concurrent requests queued (default: 10000) preload_app  - Preload app before forking workers (default: false) pythonpath  - Additional Python paths (default: [\".\"]) lifespan  - Lifespan protocol: auto, on, off (default: auto) websocket_timeout  - WebSocket idle timeout in ms (default: 60000) websocket_max_frame_size  - Max WebSocket frame size (default: 16MB) routes  - Custom Cowboy routes [{Path, Handler, Opts}] (default: [])","ref":"hornbeam.html#start/2"},{"type":"function","title":"hornbeam.stop/0","doc":"Stop hornbeam server.","ref":"hornbeam.html#stop/0"},{"type":"function","title":"hornbeam.unregister_function/1","doc":"Unregister a previously registered function.","ref":"hornbeam.html#unregister_function/1"},{"type":"module","title":"hornbeam_app","doc":"Hornbeam OTP application.","ref":"hornbeam_app.html"},{"type":"function","title":"hornbeam_app.start/2","doc":"","ref":"hornbeam_app.html#start/2"},{"type":"function","title":"hornbeam_app.stop/1","doc":"","ref":"hornbeam_app.html#stop/1"},{"type":"module","title":"hornbeam_asgi","doc":"ASGI scope builder and protocol handling. Builds ASGI 3.0 compliant scope dictionaries from Cowboy requests. Supports HTTP/1.1, HTTP/2, and WebSocket scopes with proper extensions. HTTP Scope Contains: type, asgi, http_version, method, scheme, path, query_string, headers, server, client, root_path, state, extensions Extensions - http.response.trailers: Trailer support for HTTP/2 - http.response.push: Server push for HTTP/2","ref":"hornbeam_asgi.html"},{"type":"type","title":"hornbeam_asgi.scope_opts/0","doc":"","ref":"hornbeam_asgi.html#t:scope_opts/0"},{"type":"function","title":"hornbeam_asgi.build_scope/1","doc":"Build an ASGI scope dictionary from a Cowboy request.","ref":"hornbeam_asgi.html#build_scope/1"},{"type":"function","title":"hornbeam_asgi.build_scope/2","doc":"Build an ASGI scope dictionary with options. Options: - root_path: ASGI root_path (default: empty binary) - state: Shared state dict from lifespan (default: empty map) - extensions: Additional extensions to include","ref":"hornbeam_asgi.html#build_scope/2"},{"type":"module","title":"hornbeam_callbacks","doc":"Erlang callback registry for Python apps. This module manages callbacks that Python apps can invoke. It wraps the erlang-python function registration with additional features like validation and logging.","ref":"hornbeam_callbacks.html"},{"type":"function","title":"hornbeam_callbacks.call/2","doc":"Call a registered callback synchronously.","ref":"hornbeam_callbacks.html#call/2"},{"type":"function","title":"hornbeam_callbacks.cast/2","doc":"Call a registered callback asynchronously (fire and forget).","ref":"hornbeam_callbacks.html#cast/2"},{"type":"function","title":"hornbeam_callbacks.code_change/3","doc":"","ref":"hornbeam_callbacks.html#code_change/3"},{"type":"function","title":"hornbeam_callbacks.handle_call/3","doc":"","ref":"hornbeam_callbacks.html#handle_call/3"},{"type":"function","title":"hornbeam_callbacks.handle_cast/2","doc":"","ref":"hornbeam_callbacks.html#handle_cast/2"},{"type":"function","title":"hornbeam_callbacks.handle_info/2","doc":"","ref":"hornbeam_callbacks.html#handle_info/2"},{"type":"function","title":"hornbeam_callbacks.init/1","doc":"","ref":"hornbeam_callbacks.html#init/1"},{"type":"function","title":"hornbeam_callbacks.list/0","doc":"List all registered callbacks.","ref":"hornbeam_callbacks.html#list/0"},{"type":"function","title":"hornbeam_callbacks.register/2","doc":"Register a callback function. The function should accept a list of arguments.","ref":"hornbeam_callbacks.html#register/2"},{"type":"function","title":"hornbeam_callbacks.register/3","doc":"Register a module:function as a callback.","ref":"hornbeam_callbacks.html#register/3"},{"type":"function","title":"hornbeam_callbacks.start_link/0","doc":"Start the callback manager.","ref":"hornbeam_callbacks.html#start_link/0"},{"type":"function","title":"hornbeam_callbacks.terminate/2","doc":"","ref":"hornbeam_callbacks.html#terminate/2"},{"type":"function","title":"hornbeam_callbacks.unregister/1","doc":"Unregister a callback.","ref":"hornbeam_callbacks.html#unregister/1"},{"type":"module","title":"hornbeam_config","doc":"Hornbeam configuration management. This module manages configuration for the hornbeam server. Configuration is stored in ETS for fast concurrent access. Configuration Options Server - bind: Address to bind to (default: \"127.0.0.1:8000\") - ssl: Enable SSL/TLS (default: false) - certfile: Path to SSL certificate file - keyfile: Path to SSL private key file Protocol - worker_class: wsgi or asgi (default: wsgi) - http_version: List of supported HTTP versions (default: ['HTTP/1.1', 'HTTP/2']) Workers - workers: Number of Python workers (default: 4) - timeout: Request timeout in ms (default: 30000) - keepalive: Keep-alive timeout in seconds (default: 2) - max_requests: Max requests per worker before restart (default: 1000) - preload_app: Preload app before forking workers (default: false) Request Limits - max_request_line_size: Max request line size (default: 4094) - max_header_size: Max header size (default: 8190) - max_headers: Max number of headers (default: 100) ASGI - root_path: ASGI root_path (default: \"\") - lifespan: Lifespan protocol mode: auto, on, off (default: auto) WebSocket - websocket_timeout: WebSocket idle timeout in ms (default: 60000) - websocket_max_frame_size: Max WebSocket frame size (default: 16MB) Python - pythonpath: Additional Python paths (default: [\".\", \"examples\"])","ref":"hornbeam_config.html"},{"type":"function","title":"hornbeam_config.code_change/3","doc":"","ref":"hornbeam_config.html#code_change/3"},{"type":"function","title":"hornbeam_config.defaults/0","doc":"Get default configuration.","ref":"hornbeam_config.html#defaults/0"},{"type":"function","title":"hornbeam_config.get_config/0","doc":"Get all configuration.","ref":"hornbeam_config.html#get_config/0"},{"type":"function","title":"hornbeam_config.get_config/1","doc":"Get a configuration value.","ref":"hornbeam_config.html#get_config/1"},{"type":"function","title":"hornbeam_config.get_config/2","doc":"Get a configuration value with default.","ref":"hornbeam_config.html#get_config/2"},{"type":"function","title":"hornbeam_config.handle_call/3","doc":"","ref":"hornbeam_config.html#handle_call/3"},{"type":"function","title":"hornbeam_config.handle_cast/2","doc":"","ref":"hornbeam_config.html#handle_cast/2"},{"type":"function","title":"hornbeam_config.handle_info/2","doc":"","ref":"hornbeam_config.html#handle_info/2"},{"type":"function","title":"hornbeam_config.init/1","doc":"","ref":"hornbeam_config.html#init/1"},{"type":"function","title":"hornbeam_config.set_config/1","doc":"Set the entire configuration.","ref":"hornbeam_config.html#set_config/1"},{"type":"function","title":"hornbeam_config.set_config/2","doc":"Set a configuration value.","ref":"hornbeam_config.html#set_config/2"},{"type":"function","title":"hornbeam_config.start_link/0","doc":"","ref":"hornbeam_config.html#start_link/0"},{"type":"function","title":"hornbeam_config.terminate/2","doc":"","ref":"hornbeam_config.html#terminate/2"},{"type":"function","title":"hornbeam_config.update_config/1","doc":"Update configuration with new values (merge).","ref":"hornbeam_config.html#update_config/1"},{"type":"module","title":"hornbeam_dist","doc":"Distributed Erlang RPC for Python apps. This module lets Python apps call functions on remote Erlang nodes. Use cases: - Distributed ML inference across GPU nodes - Sharding data processing across cluster - Calling specialized services on different nodes Erlang distribution provides: - Transparent RPC (call any node in cluster) - Node discovery and monitoring - Fault-tolerant (handle node failures)","ref":"hornbeam_dist.html"},{"type":"function","title":"hornbeam_dist.connect/1","doc":"Connect to a node.","ref":"hornbeam_dist.html#connect/1"},{"type":"function","title":"hornbeam_dist.connected_nodes/0","doc":"Get list of connected nodes.","ref":"hornbeam_dist.html#connected_nodes/0"},{"type":"function","title":"hornbeam_dist.disconnect/1","doc":"Disconnect from a node.","ref":"hornbeam_dist.html#disconnect/1"},{"type":"function","title":"hornbeam_dist.node/0","doc":"Get this node's name.","ref":"hornbeam_dist.html#node/0"},{"type":"function","title":"hornbeam_dist.nodes/0","doc":"Get list of all known nodes (including disconnected).","ref":"hornbeam_dist.html#nodes/0"},{"type":"function","title":"hornbeam_dist.ping/1","doc":"Ping a node to check if it's alive.","ref":"hornbeam_dist.html#ping/1"},{"type":"function","title":"hornbeam_dist.rpc_call/5","doc":"Call a function on a remote node synchronously. Returns {ok, Result} or {error, Reason}.","ref":"hornbeam_dist.html#rpc_call/5"},{"type":"function","title":"hornbeam_dist.rpc_cast/4","doc":"Call a function on a remote node asynchronously (fire and forget).","ref":"hornbeam_dist.html#rpc_cast/4"},{"type":"module","title":"hornbeam_handler","doc":"Cowboy HTTP handler for hornbeam. This module handles HTTP requests and routes them to either WSGI or ASGI handlers based on configuration. It also handles WebSocket upgrades for ASGI applications.","ref":"hornbeam_handler.html"},{"type":"function","title":"hornbeam_handler.init/2","doc":"","ref":"hornbeam_handler.html#init/2"},{"type":"module","title":"hornbeam_hooks","doc":"Dirty execution API for hornbeam. Provides a hooks-style API for executing registered handlers. Uses gen_server for safe registration and persistent_term for fast lookups. Usage <span class=\"w\">      </span><span class=\"ss\">erlang</span><span class=\"w\">\n   </span><span class=\"c1\">%% Register a handler (MFA or fun)</span><span class=\"w\">\n   </span><span class=\"nc\">hornbeam_hooks</span><span class=\"p\">:</span><span class=\"nf\">reg</span><span class=\"p\" data-group-id=\"2244263177-1\">(</span><span class=\"p\" data-group-id=\"2244263177-2\">&lt;&lt;</span><span class=\"s\">&quot;myapp.ml:MLApp&quot;</span><span class=\"p\" data-group-id=\"2244263177-2\">&gt;&gt;</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"ss\">my_module</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"ss\">my_fun</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"mi\">3</span><span class=\"p\" data-group-id=\"2244263177-1\">)</span><span class=\"p\">.</span><span class=\"w\">\n   </span><span class=\"nc\">hornbeam_hooks</span><span class=\"p\">:</span><span class=\"nf\">reg</span><span class=\"p\" data-group-id=\"2244263177-3\">(</span><span class=\"p\" data-group-id=\"2244263177-4\">&lt;&lt;</span><span class=\"s\">&quot;myapp.ml:MLApp&quot;</span><span class=\"p\" data-group-id=\"2244263177-4\">&gt;&gt;</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"nf\">fun</span><span class=\"p\" data-group-id=\"2244263177-5\">(</span><span class=\"n\">Action</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">Args</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">Kwargs</span><span class=\"p\" data-group-id=\"2244263177-5\">)</span><span class=\"w\"> </span><span class=\"p\">-&gt;</span><span class=\"w\"> </span><span class=\"p\">.</span><span class=\"p\">.</span><span class=\"p\">.</span><span class=\"w\"> </span><span class=\"k\">end</span><span class=\"p\" data-group-id=\"2244263177-3\">)</span><span class=\"p\">.</span><span class=\"w\">\n  \n   </span><span class=\"c1\">%% Execute</span><span class=\"w\">\n   </span><span class=\"p\" data-group-id=\"2244263177-6\">{</span><span class=\"ss\">ok</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">Result</span><span class=\"p\" data-group-id=\"2244263177-6\">}</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"nc\">hornbeam_hooks</span><span class=\"p\">:</span><span class=\"nf\">execute</span><span class=\"p\" data-group-id=\"2244263177-7\">(</span><span class=\"p\" data-group-id=\"2244263177-8\">&lt;&lt;</span><span class=\"s\">&quot;myapp.ml:MLApp&quot;</span><span class=\"p\" data-group-id=\"2244263177-8\">&gt;&gt;</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"p\" data-group-id=\"2244263177-9\">&lt;&lt;</span><span class=\"s\">&quot;action&quot;</span><span class=\"p\" data-group-id=\"2244263177-9\">&gt;&gt;</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">Args</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">Kwargs</span><span class=\"p\" data-group-id=\"2244263177-7\">)</span><span class=\"p\">.</span> From Python: <span class=\"w\">      </span><span class=\"ss\">python</span><span class=\"w\">\n   </span><span class=\"ss\">from</span><span class=\"w\"> </span><span class=\"ss\">hornbeam_erlang</span><span class=\"w\"> </span><span class=\"ss\">import</span><span class=\"w\"> </span><span class=\"ss\">execute</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"ss\">stream</span><span class=\"w\">\n  \n   </span><span class=\"ss\">result</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"nf\">execute</span><span class=\"p\" data-group-id=\"9380848883-1\">(</span><span class=\"s\">&quot;myapp.ml:MLApp&quot;</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"s\">&quot;inference&quot;</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"p\" data-group-id=\"9380848883-2\">[</span><span class=\"ss\">data</span><span class=\"p\" data-group-id=\"9380848883-2\">]</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"p\" data-group-id=\"9380848883-3\">{</span><span class=\"s\">&quot;model&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s\">&quot;gpt-4&quot;</span><span class=\"p\" data-group-id=\"9380848883-3\">}</span><span class=\"p\" data-group-id=\"9380848883-1\">)</span><span class=\"w\">\n  \n   </span><span class=\"ss\">for</span><span class=\"w\"> </span><span class=\"ss\">chunk</span><span class=\"w\"> </span><span class=\"ss\">in</span><span class=\"w\"> </span><span class=\"nf\">stream</span><span class=\"p\" data-group-id=\"9380848883-4\">(</span><span class=\"s\">&quot;myapp.llm:LLMApp&quot;</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"s\">&quot;generate&quot;</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"p\" data-group-id=\"9380848883-5\">[</span><span class=\"ss\">prompt</span><span class=\"p\" data-group-id=\"9380848883-5\">]</span><span class=\"p\" data-group-id=\"9380848883-4\">)</span><span class=\"p\">:</span><span class=\"w\">\n       </span><span class=\"ss\">yield</span><span class=\"w\"> </span><span class=\"ss\">chunk</span>","ref":"hornbeam_hooks.html"},{"type":"function","title":"hornbeam_hooks.all/0","doc":"List all registered app paths.","ref":"hornbeam_hooks.html#all/0"},{"type":"function","title":"hornbeam_hooks.await_result/2","doc":"Wait for async task result.","ref":"hornbeam_hooks.html#await_result/2"},{"type":"function","title":"hornbeam_hooks.code_change/3","doc":"","ref":"hornbeam_hooks.html#code_change/3"},{"type":"function","title":"hornbeam_hooks.error/1","doc":"Wrap an error.","ref":"hornbeam_hooks.html#error/1"},{"type":"function","title":"hornbeam_hooks.execute/4","doc":"Execute an action on a registered app. If no handler is registered, calls Python directly.","ref":"hornbeam_hooks.html#execute/4"},{"type":"function","title":"hornbeam_hooks.execute_async/4","doc":"Execute async - returns task ID (binary for Python round-trip compatibility).","ref":"hornbeam_hooks.html#execute_async/4"},{"type":"function","title":"hornbeam_hooks.find/1","doc":"Find handler for an app path (fast lookup via persistent_term).","ref":"hornbeam_hooks.html#find/1"},{"type":"function","title":"hornbeam_hooks.handle_call/3","doc":"","ref":"hornbeam_hooks.html#handle_call/3"},{"type":"function","title":"hornbeam_hooks.handle_cast/2","doc":"","ref":"hornbeam_hooks.html#handle_cast/2"},{"type":"function","title":"hornbeam_hooks.handle_info/2","doc":"","ref":"hornbeam_hooks.html#handle_info/2"},{"type":"function","title":"hornbeam_hooks.init/1","doc":"","ref":"hornbeam_hooks.html#init/1"},{"type":"function","title":"hornbeam_hooks.ok/1","doc":"Wrap a successful result.","ref":"hornbeam_hooks.html#ok/1"},{"type":"function","title":"hornbeam_hooks.reg/2","doc":"Register a handler function for an app path.","ref":"hornbeam_hooks.html#reg/2"},{"type":"function","title":"hornbeam_hooks.reg/4","doc":"Register a module:function as handler.","ref":"hornbeam_hooks.html#reg/4"},{"type":"function","title":"hornbeam_hooks.reg_python/1","doc":"Register a Python handler for an app path. The actual handler is stored in Python; this just marks it for callback.","ref":"hornbeam_hooks.html#reg_python/1"},{"type":"function","title":"hornbeam_hooks.start_link/0","doc":"","ref":"hornbeam_hooks.html#start_link/0"},{"type":"function","title":"hornbeam_hooks.stream/4","doc":"Stream execution - returns a generator function. Call the returned function repeatedly until it returns 'done'.","ref":"hornbeam_hooks.html#stream/4"},{"type":"function","title":"hornbeam_hooks.stream_chunk/1","doc":"Return a stream chunk (for use in generator functions).","ref":"hornbeam_hooks.html#stream_chunk/1"},{"type":"function","title":"hornbeam_hooks.stream_done/0","doc":"Signal end of stream (for use in generator functions).","ref":"hornbeam_hooks.html#stream_done/0"},{"type":"function","title":"hornbeam_hooks.stream_from_fun/1","doc":"Create a stream generator from a function. The function should return {value, Chunk} or done. Example: <span class=\"w\">     </span><span class=\"ss\">erlang</span><span class=\"w\">\n  </span><span class=\"nf\">handler</span><span class=\"p\" data-group-id=\"5525437608-1\">(</span><span class=\"ss\">stream</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"p\" data-group-id=\"5525437608-2\">&lt;&lt;</span><span class=\"s\">&quot;count&quot;</span><span class=\"p\" data-group-id=\"5525437608-2\">&gt;&gt;</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"p\" data-group-id=\"5525437608-3\">[</span><span class=\"n\">Max</span><span class=\"p\" data-group-id=\"5525437608-3\">]</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"p\">_</span><span class=\"n\">Kwargs</span><span class=\"p\" data-group-id=\"5525437608-1\">)</span><span class=\"w\"> </span><span class=\"p\">-&gt;</span><span class=\"w\">\n      </span><span class=\"n\">Ref</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"nf\">make_ref</span><span class=\"p\" data-group-id=\"5525437608-4\">(</span><span class=\"p\" data-group-id=\"5525437608-4\">)</span><span class=\"p\">,</span><span class=\"w\">\n      </span><span class=\"nf\">put</span><span class=\"p\" data-group-id=\"5525437608-5\">(</span><span class=\"n\">Ref</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"mi\">0</span><span class=\"p\" data-group-id=\"5525437608-5\">)</span><span class=\"p\">,</span><span class=\"w\">\n      </span><span class=\"nc\">hornbeam_hooks</span><span class=\"p\">:</span><span class=\"nf\">stream_from_fun</span><span class=\"p\" data-group-id=\"5525437608-6\">(</span><span class=\"nf\">fun</span><span class=\"p\" data-group-id=\"5525437608-7\">(</span><span class=\"p\" data-group-id=\"5525437608-7\">)</span><span class=\"w\"> </span><span class=\"p\">-&gt;</span><span class=\"w\">\n          </span><span class=\"n\">N</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"nf\">get</span><span class=\"p\" data-group-id=\"5525437608-8\">(</span><span class=\"n\">Ref</span><span class=\"p\" data-group-id=\"5525437608-8\">)</span><span class=\"p\">,</span><span class=\"w\">\n          </span><span class=\"k\">if</span><span class=\"w\"> </span><span class=\"n\">N</span><span class=\"w\"> </span><span class=\"o\">&gt;=</span><span class=\"w\"> </span><span class=\"n\">Max</span><span class=\"w\"> </span><span class=\"p\">-&gt;</span><span class=\"w\">\n              </span><span class=\"nf\">erase</span><span class=\"p\" data-group-id=\"5525437608-9\">(</span><span class=\"n\">Ref</span><span class=\"p\" data-group-id=\"5525437608-9\">)</span><span class=\"p\">,</span><span class=\"w\">\n              </span><span class=\"ss\">done</span><span class=\"p\">;</span><span class=\"w\">\n          </span><span class=\"ss\">true</span><span class=\"w\"> </span><span class=\"p\">-&gt;</span><span class=\"w\">\n              </span><span class=\"nf\">put</span><span class=\"p\" data-group-id=\"5525437608-10\">(</span><span class=\"n\">Ref</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">N</span><span class=\"w\"> </span><span class=\"o\">+</span><span class=\"w\"> </span><span class=\"mi\">1</span><span class=\"p\" data-group-id=\"5525437608-10\">)</span><span class=\"p\">,</span><span class=\"w\">\n              </span><span class=\"p\" data-group-id=\"5525437608-11\">{</span><span class=\"ss\">value</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">N</span><span class=\"p\" data-group-id=\"5525437608-11\">}</span><span class=\"w\">\n          </span><span class=\"k\">end</span><span class=\"w\">\n      </span><span class=\"k\">end</span><span class=\"p\" data-group-id=\"5525437608-6\">)</span><span class=\"p\">.</span>","ref":"hornbeam_hooks.html#stream_from_fun/1"},{"type":"function","title":"hornbeam_hooks.stream_from_list/1","doc":"Create a stream generator from a list. Returns {ok, GeneratorFun} for use in stream handlers. Example: <span class=\"w\">     </span><span class=\"ss\">erlang</span><span class=\"w\">\n  </span><span class=\"nf\">handler</span><span class=\"p\" data-group-id=\"4125688620-1\">(</span><span class=\"ss\">stream</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"p\" data-group-id=\"4125688620-2\">&lt;&lt;</span><span class=\"s\">&quot;generate&quot;</span><span class=\"p\" data-group-id=\"4125688620-2\">&gt;&gt;</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">Args</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"p\">_</span><span class=\"n\">Kwargs</span><span class=\"p\" data-group-id=\"4125688620-1\">)</span><span class=\"w\"> </span><span class=\"p\">-&gt;</span><span class=\"w\">\n      </span><span class=\"n\">Chunks</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"p\" data-group-id=\"4125688620-3\">[</span><span class=\"s\">&quot;Hello&quot;</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"s\">&quot; &quot;</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"s\">&quot;World&quot;</span><span class=\"p\" data-group-id=\"4125688620-3\">]</span><span class=\"p\">,</span><span class=\"w\">\n      </span><span class=\"nc\">hornbeam_hooks</span><span class=\"p\">:</span><span class=\"nf\">stream_from_list</span><span class=\"p\" data-group-id=\"4125688620-4\">(</span><span class=\"n\">Chunks</span><span class=\"p\" data-group-id=\"4125688620-4\">)</span><span class=\"p\">.</span>","ref":"hornbeam_hooks.html#stream_from_list/1"},{"type":"function","title":"hornbeam_hooks.stream_next_ref/1","doc":"Call next on a stored generator ref.","ref":"hornbeam_hooks.html#stream_next_ref/1"},{"type":"function","title":"hornbeam_hooks.terminate/2","doc":"","ref":"hornbeam_hooks.html#terminate/2"},{"type":"function","title":"hornbeam_hooks.unreg/1","doc":"Unregister a handler.","ref":"hornbeam_hooks.html#unreg/1"},{"type":"module","title":"hornbeam_lifespan","doc":"ASGI Lifespan protocol manager. This module implements the ASGI lifespan protocol for application startup and shutdown events. It allows ASGI applications to perform initialization (loading ML models, connecting to databases) at startup and cleanup at shutdown. Lifespan Protocol The lifespan protocol consists of these events: Startup: <ol> <li>Server sends: type = \"lifespan.startup\"</li> <li>App responds: type = \"lifespan.startup.complete\" or type = \"lifespan.startup.failed\"</li> </ol> Shutdown: <ol> <li>Server sends: type = \"lifespan.shutdown\"</li> <li>App responds: type = \"lifespan.shutdown.complete\"</li> </ol> Configuration The lifespan setting can be: <ul> <li>auto: Detect if app supports lifespan (default)</li> <li>on: Require lifespan support, fail if not supported</li> <li>off: Disable lifespan protocol</li> </ul>","ref":"hornbeam_lifespan.html"},{"type":"function","title":"hornbeam_lifespan.code_change/3","doc":"","ref":"hornbeam_lifespan.html#code_change/3"},{"type":"function","title":"hornbeam_lifespan.get_state/0","doc":"Get the lifespan state (shared across requests).","ref":"hornbeam_lifespan.html#get_state/0"},{"type":"function","title":"hornbeam_lifespan.handle_call/3","doc":"","ref":"hornbeam_lifespan.html#handle_call/3"},{"type":"function","title":"hornbeam_lifespan.handle_cast/2","doc":"","ref":"hornbeam_lifespan.html#handle_cast/2"},{"type":"function","title":"hornbeam_lifespan.handle_info/2","doc":"","ref":"hornbeam_lifespan.html#handle_info/2"},{"type":"function","title":"hornbeam_lifespan.init/1","doc":"","ref":"hornbeam_lifespan.html#init/1"},{"type":"function","title":"hornbeam_lifespan.is_running/0","doc":"Check if lifespan is running.","ref":"hornbeam_lifespan.html#is_running/0"},{"type":"function","title":"hornbeam_lifespan.shutdown/0","doc":"Run lifespan shutdown protocol.","ref":"hornbeam_lifespan.html#shutdown/0"},{"type":"function","title":"hornbeam_lifespan.start_link/0","doc":"Start the lifespan manager.","ref":"hornbeam_lifespan.html#start_link/0"},{"type":"function","title":"hornbeam_lifespan.start_link/1","doc":"Start the lifespan manager with options.","ref":"hornbeam_lifespan.html#start_link/1"},{"type":"function","title":"hornbeam_lifespan.startup/0","doc":"Run lifespan startup protocol. This should be called after the application is loaded but before accepting requests.","ref":"hornbeam_lifespan.html#startup/0"},{"type":"function","title":"hornbeam_lifespan.startup/1","doc":"Run lifespan startup with options.","ref":"hornbeam_lifespan.html#startup/1"},{"type":"function","title":"hornbeam_lifespan.terminate/2","doc":"","ref":"hornbeam_lifespan.html#terminate/2"},{"type":"module","title":"hornbeam_ml","doc":"ML utilities for hornbeam. This module provides Erlang-side ML utilities: - Cache management for ML results - Batch request collection - Cache statistics","ref":"hornbeam_ml.html"},{"type":"function","title":"hornbeam_ml.cache_hit/0","doc":"Record a cache hit.","ref":"hornbeam_ml.html#cache_hit/0"},{"type":"function","title":"hornbeam_ml.cache_miss/0","doc":"Record a cache miss.","ref":"hornbeam_ml.html#cache_miss/0"},{"type":"function","title":"hornbeam_ml.cache_stats/0","doc":"Get cache statistics.","ref":"hornbeam_ml.html#cache_stats/0"},{"type":"function","title":"hornbeam_ml.clear_cache/1","doc":"Clear all cached entries with a given prefix.","ref":"hornbeam_ml.html#clear_cache/1"},{"type":"function","title":"hornbeam_ml.get_cached/2","doc":"Get a cached ML result.","ref":"hornbeam_ml.html#get_cached/2"},{"type":"function","title":"hornbeam_ml.set_cached/3","doc":"Store an ML result in cache.","ref":"hornbeam_ml.html#set_cached/3"},{"type":"module","title":"hornbeam_pubsub","doc":"Pub/Sub messaging for Python apps. This module provides publish/subscribe messaging backed by Erlang's pg (process groups). It enables: - Real-time updates to web clients - Event broadcasting - Cross-node messaging (distributed pg) Note: This is primarily for Erlang-side subscriptions. For Python web handlers, use WebSocket connections with topic routing.","ref":"hornbeam_pubsub.html"},{"type":"function","title":"hornbeam_pubsub.code_change/3","doc":"","ref":"hornbeam_pubsub.html#code_change/3"},{"type":"function","title":"hornbeam_pubsub.get_local_members/1","doc":"Get local members subscribed to a topic.","ref":"hornbeam_pubsub.html#get_local_members/1"},{"type":"function","title":"hornbeam_pubsub.get_members/1","doc":"Get all members subscribed to a topic (all nodes).","ref":"hornbeam_pubsub.html#get_members/1"},{"type":"function","title":"hornbeam_pubsub.handle_call/3","doc":"","ref":"hornbeam_pubsub.html#handle_call/3"},{"type":"function","title":"hornbeam_pubsub.handle_cast/2","doc":"","ref":"hornbeam_pubsub.html#handle_cast/2"},{"type":"function","title":"hornbeam_pubsub.handle_info/2","doc":"","ref":"hornbeam_pubsub.html#handle_info/2"},{"type":"function","title":"hornbeam_pubsub.init/1","doc":"","ref":"hornbeam_pubsub.html#init/1"},{"type":"function","title":"hornbeam_pubsub.publish/2","doc":"Publish a message to all subscribers of a topic. Returns the number of processes the message was sent to.","ref":"hornbeam_pubsub.html#publish/2"},{"type":"function","title":"hornbeam_pubsub.start_link/0","doc":"Start the pubsub manager.","ref":"hornbeam_pubsub.html#start_link/0"},{"type":"function","title":"hornbeam_pubsub.subscribe/1","doc":"Subscribe the calling process to a topic.","ref":"hornbeam_pubsub.html#subscribe/1"},{"type":"function","title":"hornbeam_pubsub.subscribe/2","doc":"Subscribe a specific process to a topic.","ref":"hornbeam_pubsub.html#subscribe/2"},{"type":"function","title":"hornbeam_pubsub.terminate/2","doc":"","ref":"hornbeam_pubsub.html#terminate/2"},{"type":"function","title":"hornbeam_pubsub.unsubscribe/1","doc":"Unsubscribe the calling process from a topic.","ref":"hornbeam_pubsub.html#unsubscribe/1"},{"type":"function","title":"hornbeam_pubsub.unsubscribe/2","doc":"Unsubscribe a specific process from a topic.","ref":"hornbeam_pubsub.html#unsubscribe/2"},{"type":"module","title":"hornbeam_state","doc":"Shared state ETS for Python apps. This module provides a high-performance, concurrent-safe shared state store backed by ETS. Python apps can use this for: - Caching (ML model results, API responses) - Counters (request counts, rate limiting) - Session data - Any data that needs to be shared across requests ETS provides: - Millions of concurrent reads without blocking - Atomic updates (counters, compare-and-swap) - No GIL limitations (unlike Python dicts)","ref":"hornbeam_state.html"},{"type":"function","title":"hornbeam_state.clear/0","doc":"Clear all state.","ref":"hornbeam_state.html#clear/0"},{"type":"function","title":"hornbeam_state.code_change/3","doc":"","ref":"hornbeam_state.html#code_change/3"},{"type":"function","title":"hornbeam_state.decr/2","doc":"Atomically decrement a counter.","ref":"hornbeam_state.html#decr/2"},{"type":"function","title":"hornbeam_state.delete/1","doc":"Delete a key.","ref":"hornbeam_state.html#delete/1"},{"type":"function","title":"hornbeam_state.get/1","doc":"Get a value by key. Returns the value or undefined if not found.","ref":"hornbeam_state.html#get/1"},{"type":"function","title":"hornbeam_state.get_multi/1","doc":"Get multiple keys at once. Returns a map of key => value for keys that exist.","ref":"hornbeam_state.html#get_multi/1"},{"type":"function","title":"hornbeam_state.handle_call/3","doc":"","ref":"hornbeam_state.html#handle_call/3"},{"type":"function","title":"hornbeam_state.handle_cast/2","doc":"","ref":"hornbeam_state.html#handle_cast/2"},{"type":"function","title":"hornbeam_state.handle_info/2","doc":"","ref":"hornbeam_state.html#handle_info/2"},{"type":"function","title":"hornbeam_state.incr/2","doc":"Atomically increment a counter. If the key doesn't exist, it's initialized to Delta. Returns the new value.","ref":"hornbeam_state.html#incr/2"},{"type":"function","title":"hornbeam_state.init/1","doc":"","ref":"hornbeam_state.html#init/1"},{"type":"function","title":"hornbeam_state.keys/0","doc":"Get all keys in the state store.","ref":"hornbeam_state.html#keys/0"},{"type":"function","title":"hornbeam_state.keys/1","doc":"Get keys matching a prefix (binary keys only).","ref":"hornbeam_state.html#keys/1"},{"type":"function","title":"hornbeam_state.set/2","doc":"Set a key-value pair.","ref":"hornbeam_state.html#set/2"},{"type":"function","title":"hornbeam_state.set_multi/1","doc":"Set multiple key-value pairs at once.","ref":"hornbeam_state.html#set_multi/1"},{"type":"function","title":"hornbeam_state.size/0","doc":"Get the number of entries.","ref":"hornbeam_state.html#size/0"},{"type":"function","title":"hornbeam_state.start_link/0","doc":"Start the shared state manager.","ref":"hornbeam_state.html#start_link/0"},{"type":"function","title":"hornbeam_state.terminate/2","doc":"","ref":"hornbeam_state.html#terminate/2"},{"type":"module","title":"hornbeam_sup","doc":"Hornbeam top-level supervisor. Supervises all hornbeam services: - hornbeam_config: Configuration management - hornbeam_state: Shared state ETS - hornbeam_tasks: Async task spawning - hornbeam_callbacks: Erlang callback registry - hornbeam_pubsub: Pub/sub messaging - hornbeam_lifespan: ASGI lifespan management - hornbeam_hooks: Hooks-style execution API","ref":"hornbeam_sup.html"},{"type":"function","title":"hornbeam_sup.init/1","doc":"","ref":"hornbeam_sup.html#init/1"},{"type":"function","title":"hornbeam_sup.start_link/0","doc":"","ref":"hornbeam_sup.html#start_link/0"},{"type":"module","title":"hornbeam_tasks","doc":"Async task spawning for Python apps. This module lets Python web apps spawn background Erlang tasks. Each task runs in its own Erlang process, providing: - Non-blocking execution (return immediately to Python) - Fault isolation (task crash doesn't affect web request) - Supervision (tasks are supervised, can be monitored) Use cases: - Background order processing - Sending emails/notifications - ML inference jobs - Any slow operation that shouldn't block the response","ref":"hornbeam_tasks.html"},{"type":"function","title":"hornbeam_tasks.await/2","doc":"Wait for task completion with timeout. Returns {ok, Result} | {error, Reason} | {error, timeout}","ref":"hornbeam_tasks.html#await/2"},{"type":"function","title":"hornbeam_tasks.cancel/1","doc":"Cancel a task.","ref":"hornbeam_tasks.html#cancel/1"},{"type":"function","title":"hornbeam_tasks.code_change/3","doc":"","ref":"hornbeam_tasks.html#code_change/3"},{"type":"function","title":"hornbeam_tasks.handle_call/3","doc":"","ref":"hornbeam_tasks.html#handle_call/3"},{"type":"function","title":"hornbeam_tasks.handle_cast/2","doc":"","ref":"hornbeam_tasks.html#handle_cast/2"},{"type":"function","title":"hornbeam_tasks.handle_info/2","doc":"","ref":"hornbeam_tasks.html#handle_info/2"},{"type":"function","title":"hornbeam_tasks.init/1","doc":"","ref":"hornbeam_tasks.html#init/1"},{"type":"function","title":"hornbeam_tasks.list/0","doc":"List all tasks.","ref":"hornbeam_tasks.html#list/0"},{"type":"function","title":"hornbeam_tasks.register_handler/2","doc":"Register a task handler function. The function should accept a list of args and return a result.","ref":"hornbeam_tasks.html#register_handler/2"},{"type":"function","title":"hornbeam_tasks.result/1","doc":"Get task result (non-blocking). Returns {ok, Result} | {error, Reason} | pending | not_found","ref":"hornbeam_tasks.html#result/1"},{"type":"function","title":"hornbeam_tasks.spawn/2","doc":"Spawn a new async task. Returns the task ID immediately.","ref":"hornbeam_tasks.html#spawn/2"},{"type":"function","title":"hornbeam_tasks.spawn/3","doc":"Spawn a task with options. Options: - timeout: Max execution time in ms (default: infinity)","ref":"hornbeam_tasks.html#spawn/3"},{"type":"function","title":"hornbeam_tasks.start_link/0","doc":"Start the task manager.","ref":"hornbeam_tasks.html#start_link/0"},{"type":"function","title":"hornbeam_tasks.status/1","doc":"Get task status. Returns: pending | running | completed | failed | cancelled | not_found","ref":"hornbeam_tasks.html#status/1"},{"type":"function","title":"hornbeam_tasks.terminate/2","doc":"","ref":"hornbeam_tasks.html#terminate/2"},{"type":"function","title":"hornbeam_tasks.unregister_handler/1","doc":"Unregister a task handler.","ref":"hornbeam_tasks.html#unregister_handler/1"},{"type":"module","title":"hornbeam_websocket","doc":"Cowboy WebSocket handler for ASGI applications. This module implements the Cowboy websocket behavior and bridges to ASGI WebSocket applications through the hornbeam_websocket_runner Python module. ASGI WebSocket Protocol The handler translates between Cowboy WebSocket and ASGI WebSocket events: Erlang -> Python (receive): - websocket.connect: On WebSocket upgrade - websocket.receive: On text/binary frame - websocket.disconnect: On close Python -> Erlang (send): - websocket.accept: Accept connection - websocket.send: Send text/binary frame - websocket.close: Close connection","ref":"hornbeam_websocket.html"},{"type":"function","title":"hornbeam_websocket.init/2","doc":"Initialize WebSocket connection. This is called during HTTP request handling to decide whether to upgrade to WebSocket.","ref":"hornbeam_websocket.html#init/2"},{"type":"function","title":"hornbeam_websocket.terminate/3","doc":"Handle WebSocket termination.","ref":"hornbeam_websocket.html#terminate/3"},{"type":"function","title":"hornbeam_websocket.websocket_handle/2","doc":"Handle incoming WebSocket frames.","ref":"hornbeam_websocket.html#websocket_handle/2"},{"type":"function","title":"hornbeam_websocket.websocket_info/2","doc":"Handle Erlang messages sent to the WebSocket process.","ref":"hornbeam_websocket.html#websocket_info/2"},{"type":"function","title":"hornbeam_websocket.websocket_init/1","doc":"WebSocket initialization after upgrade. Sends websocket.connect event to Python app.","ref":"hornbeam_websocket.html#websocket_init/1"},{"type":"module","title":"hornbeam_wsgi","doc":"WSGI environment builder and protocol handling. Builds a PEP 3333 compliant WSGI environ dictionary from a Cowboy request. This module creates environ dicts with all required and recommended WSGI variables for full PEP 3333 compliance. Required CGI Variables - REQUEST_METHOD - SCRIPT_NAME - PATH_INFO - QUERY_STRING - CONTENT_TYPE (if present) - CONTENT_LENGTH (if present) - SERVER_NAME - SERVER_PORT - SERVER_PROTOCOL - HTTP_* (for each request header) Required WSGI Variables - wsgi.version: (1, 0) - wsgi.url_scheme: \"http\" or \"https\" - wsgi.input: Request body - wsgi.errors: \"stderr\" (routed to logging in Python) - wsgi.multithread: true - wsgi.multiprocess: true - wsgi.run_once: false Recommended Extensions - wsgi.file_wrapper: FileWrapper class - wsgi.input_terminated: true - wsgi.early_hints: callback function Additional Variables - REMOTE_ADDR: Client IP address - REMOTE_PORT: Client port - REMOTE_HOST: Client hostname (if available)","ref":"hornbeam_wsgi.html"},{"type":"type","title":"hornbeam_wsgi.environ_opts/0","doc":"","ref":"hornbeam_wsgi.html#t:environ_opts/0"},{"type":"function","title":"hornbeam_wsgi.build_environ/1","doc":"Build a WSGI environ dictionary from a Cowboy request. Creates a dictionary containing all standard WSGI variables.","ref":"hornbeam_wsgi.html#build_environ/1"},{"type":"function","title":"hornbeam_wsgi.build_environ/2","doc":"Build a WSGI environ dictionary with options. Options: - script_name: Override SCRIPT_NAME (default: empty binary) - root_path: Root path for the application - proxy_allow_from: IPs allowed to send proxy headers - proxy_headers: Headers to check for proxied client info","ref":"hornbeam_wsgi.html#build_environ/2"},{"type":"extras","title":"Hornbeam","doc":"# Hornbeam\n\n**Hornbeam** is an Erlang-based WSGI/ASGI server that combines Python's web and ML capabilities with Erlang's strengths:\n\n- **Python handles**: Web apps (WSGI/ASGI), ML models, data processing\n- **Erlang handles**: Scaling (millions of connections), concurrency (no GIL), distribution (cluster RPC), fault tolerance, shared state (ETS)\n\nThe name combines \"horn\" (unicorn, like gunicorn) with \"BEAM\" (Erlang VM).","ref":"readme.html"},{"type":"extras","title":"Features - Hornbeam","doc":"- **WSGI Support**: Run standard WSGI Python applications\n- **ASGI Support**: Run async ASGI Python applications (FastAPI, Starlette, etc.)\n- **WebSocket**: Full WebSocket support for real-time apps\n- **HTTP/2**: Via Cowboy, with multiplexing and server push\n- **Shared State**: ETS-backed state accessible from Python (concurrent-safe)\n- **Distributed RPC**: Call functions on remote Erlang nodes\n- **Pub/Sub**: pg-based publish/subscribe messaging\n- **ML Integration**: Cache ML inference results in ETS\n- **Lifespan**: ASGI lifespan protocol for app startup/shutdown\n- **Hot Reload**: Leverage Erlang's hot code reloading","ref":"readme.html#features"},{"type":"extras","title":"Quick Start - Hornbeam","doc":"```erlang\n%% Start with a WSGI application\nhornbeam:start(\"myapp:application\").\n\n%% Start ASGI app (FastAPI, Starlette, etc.)\nhornbeam:start(\"main:app\", #{worker_class => asgi}).\n\n%% With all options\nhornbeam:start(\"myapp:application\", #{\n    bind => \"0.0.0.0:8000\",\n    workers => 4,\n    worker_class => asgi,\n    lifespan => auto\n}).\n```","ref":"readme.html#quick-start"},{"type":"extras","title":"Installation - Hornbeam","doc":"Add hornbeam to your `rebar.config`:\n\n```erlang\n{deps, [\n    {hornbeam, {git, \"https://github.com/benoitc/hornbeam.git\", {branch, \"main\"}}}\n]}.\n```","ref":"readme.html#installation"},{"type":"extras","title":"Python Integration - Hornbeam","doc":"","ref":"readme.html#python-integration"},{"type":"extras","title":"Shared State (ETS) - Hornbeam","doc":"Python apps can use Erlang ETS for high-concurrency shared state:\n\n```python\nfrom hornbeam_erlang import state_get, state_set, state_incr\n\ndef application(environ, start_response):\n    # Atomic counter (millions of concurrent increments)\n    views = state_incr(f'views:{path}')\n\n    # Get/set cached data\n    data = state_get('my_key')\n    if data is None:\n        data = compute_expensive()\n        state_set('my_key', data)\n\n    start_response('200 OK', [('Content-Type', 'text/plain')])\n    return [f'Views: {views}'.encode()]\n```","ref":"readme.html#shared-state-ets"},{"type":"extras","title":"Distributed RPC - Hornbeam","doc":"Call functions on remote Erlang nodes:\n\n```python\nfrom hornbeam_erlang import rpc_call, nodes\n\ndef application(environ, start_response):\n    # Get connected nodes\n    connected = nodes()\n\n    # Call ML model on GPU node\n    result = rpc_call(\n        'gpu@ml-server',      # Remote node\n        'ml_model',           # Module\n        'predict',            # Function\n        [data],               # Args\n        timeout_ms=30000\n    )\n\n    start_response('200 OK', [('Content-Type', 'application/json')])\n    return [json.dumps(result).encode()]\n```","ref":"readme.html#distributed-rpc"},{"type":"extras","title":"ML Caching - Hornbeam","doc":"Use ETS to cache ML inference results:\n\n```python\nfrom hornbeam_ml import cached_inference, cache_stats\n\ndef application(environ, start_response):\n    # Automatically cached by input hash\n    embedding = cached_inference(model.encode, text)\n\n    # Check cache stats\n    stats = cache_stats()  # {'hits': 100, 'misses': 10, 'hit_rate': 0.91}\n\n    start_response('200 OK', [('Content-Type', 'application/json')])\n    return [json.dumps({'embedding': embedding}).encode()]\n```","ref":"readme.html#ml-caching"},{"type":"extras","title":"Pub/Sub Messaging - Hornbeam","doc":"```python\nfrom hornbeam_erlang import publish\n\ndef application(environ, start_response):\n    # Publish to topic (all subscribers notified)\n    count = publish('updates', {'type': 'new_item', 'id': 123})\n\n    start_response('200 OK', [('Content-Type', 'application/json')])\n    return [json.dumps({'subscribers_notified': count}).encode()]\n```","ref":"readme.html#pub-sub-messaging"},{"type":"extras","title":"Examples - Hornbeam","doc":"","ref":"readme.html#examples"},{"type":"extras","title":"Hello World (WSGI) - Hornbeam","doc":"```python\n# examples/hello_wsgi/app.py\ndef application(environ, start_response):\n    start_response('200 OK', [('Content-Type', 'text/plain')])\n    return [b'Hello from Hornbeam!']\n```\n\n```erlang\nhornbeam:start(\"app:application\", #{pythonpath => [\"examples/hello_wsgi\"]}).\n```","ref":"readme.html#hello-world-wsgi"},{"type":"extras","title":"Hello World (ASGI) - Hornbeam","doc":"```python\n# examples/hello_asgi/app.py\nasync def application(scope, receive, send):\n    await send({\n        'type': 'http.response.start',\n        'status': 200,\n        'headers': [[b'content-type', b'text/plain']],\n    })\n    await send({\n        'type': 'http.response.body',\n        'body': b'Hello from Hornbeam ASGI!',\n    })\n```\n\n```erlang\nhornbeam:start(\"app:application\", #{\n    worker_class => asgi,\n    pythonpath => [\"examples/hello_asgi\"]\n}).\n```","ref":"readme.html#hello-world-asgi"},{"type":"extras","title":"WebSocket Chat - Hornbeam","doc":"```python\n# examples/websocket_chat/app.py\nasync def app(scope, receive, send):\n    if scope['type'] == 'websocket':\n        await send({'type': 'websocket.accept'})\n\n        while True:\n            message = await receive()\n            if message['type'] == 'websocket.disconnect':\n                break\n            if message['type'] == 'websocket.receive':\n                # Echo back\n                await send({\n                    'type': 'websocket.send',\n                    'text': message.get('text', '')\n                })\n```\n\n```erlang\nhornbeam:start(\"app:app\", #{\n    worker_class => asgi,\n    pythonpath => [\"examples/websocket_chat\"]\n}).\n```","ref":"readme.html#websocket-chat"},{"type":"extras","title":"Embedding Service with ETS Caching - Hornbeam","doc":"See `examples/embedding_service/` for a complete ML embedding service using Erlang ETS for caching.","ref":"readme.html#embedding-service-with-ets-caching"},{"type":"extras","title":"Distributed ML Inference - Hornbeam","doc":"See `examples/distributed_rpc/` for distributing ML inference across a cluster.","ref":"readme.html#distributed-ml-inference"},{"type":"extras","title":"Running with Gunicorn (for comparison) - Hornbeam","doc":"All examples are designed to work with gunicorn too (with fallback functions):\n\n```bash\n# With gunicorn (single process, no Erlang features)\ncd examples/hello_wsgi\ngunicorn app:application\n\n# With hornbeam (Erlang concurrency, shared state, distribution)\nrebar3 shell\n> hornbeam:start(\"app:application\", #{pythonpath => [\"examples/hello_wsgi\"]}).\n```","ref":"readme.html#running-with-gunicorn-for-comparison"},{"type":"extras","title":"Configuration - Hornbeam","doc":"","ref":"readme.html#configuration"},{"type":"extras","title":"Via hornbeam:start/2 - Hornbeam","doc":"```erlang\nhornbeam:start(\"myapp:application\", #{\n    %% Server\n    bind => <<\"0.0.0.0:8000\">>,\n    ssl => false,\n    certfile => undefined,\n    keyfile => undefined,\n\n    %% Protocol\n    worker_class => wsgi,  % wsgi | asgi\n    http_version => ['HTTP/1.1', 'HTTP/2'],\n\n    %% Workers\n    workers => 4,\n    timeout => 30000,\n    keepalive => 2,\n    max_requests => 1000,\n\n    %% ASGI\n    lifespan => auto,  % auto | on | off\n\n    %% WebSocket\n    websocket_timeout => 60000,\n    websocket_max_frame_size => 16777216,  % 16MB\n\n    %% Python\n    pythonpath => [<<\".\">>]\n}).\n```","ref":"readme.html#via-hornbeam-start-2"},{"type":"extras","title":"Via sys.config - Hornbeam","doc":"```erlang\n[\n    {hornbeam, [\n        {bind, \"127.0.0.1:8000\"},\n        {workers, 4},\n        {worker_class, wsgi},\n        {timeout, 30000},\n        {pythonpath, [\".\"]}\n    ]}\n].\n```","ref":"readme.html#via-sys-config"},{"type":"extras","title":"API Reference - Hornbeam","doc":"","ref":"readme.html#api-reference"},{"type":"extras","title":"hornbeam module - Hornbeam","doc":"| Function | Description |\n|----------|-------------|\n| `start(AppSpec)` | Start server with WSGI/ASGI app |\n| `start(AppSpec, Options)` | Start server with options |\n| `stop()` | Stop the server |\n| `register_function(Name, Fun)` | Register Erlang function callable from Python |\n| `register_function(Name, Module, Function)` | Register module:function |\n| `unregister_function(Name)` | Unregister a function |","ref":"readme.html#hornbeam-module"},{"type":"extras","title":"Python hornbeam_erlang module - Hornbeam","doc":"| Function | Description |\n|----------|-------------|\n| `state_get(key)` | Get value from ETS (None if not found) |\n| `state_set(key, value)` | Set value in ETS |\n| `state_incr(key, delta=1)` | Atomically increment counter, return new value |\n| `state_decr(key, delta=1)` | Atomically decrement counter |\n| `state_delete(key)` | Delete key from ETS |\n| `state_get_multi(keys)` | Batch get multiple keys |\n| `state_keys(prefix=None)` | Get all keys, optionally by prefix |\n| `rpc_call(node, module, function, args, timeout_ms)` | Call function on remote node |\n| `rpc_cast(node, module, function, args)` | Async call (fire and forget) |\n| `nodes()` | Get list of connected Erlang nodes |\n| `node()` | Get this node's name |\n| `publish(topic, message)` | Publish to pub/sub topic |\n| `call(name, *args)` | Call registered Erlang function |\n| `cast(name, *args)` | Async call to registered function |","ref":"readme.html#python-hornbeam_erlang-module"},{"type":"extras","title":"Python hornbeam_ml module - Hornbeam","doc":"| Function | Description |\n|----------|-------------|\n| `cached_inference(fn, input, cache_key=None, cache_prefix=\"ml\")` | Run inference with ETS caching |\n| `cache_stats()` | Get cache hit/miss statistics |","ref":"readme.html#python-hornbeam_ml-module"},{"type":"extras","title":"Development - Hornbeam","doc":"```bash\n# Compile\nrebar3 compile\n\n# Run tests\nrebar3 ct\n\n# Start shell\nrebar3 shell\n```","ref":"readme.html#development"},{"type":"extras","title":"License - Hornbeam","doc":"Apache License 2.0","ref":"readme.html#license"},{"type":"extras","title":"Changelog","doc":"# Changelog\n\nAll notable changes to this project will be documented in this file.\n\nThe format is based on [Keep a Changelog](https://keepachangelog.com/en/1.0.0/),\nand this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html).\n\n## [1.0.0] - 2026-02-15","ref":"changelog.html"},{"type":"extras","title":"Added - Changelog","doc":"- **WSGI Support**: Full PEP 3333 compliance for running Python WSGI applications\n  - Complete environ dict with all required and recommended variables\n  - `wsgi.file_wrapper` for efficient file serving\n  - `wsgi.early_hints` for 103 Early Hints responses\n  - `wsgi.errors` routing to Erlang logging\n\n- **ASGI Support**: Full ASGI 3.0 protocol implementation\n  - HTTP scope with streaming responses\n  - WebSocket scope with RFC 6455 support\n  - Lifespan protocol for startup/shutdown events\n  - Informational responses (1xx)\n\n- **HTTP Features** (via Cowboy)\n  - HTTP/1.1 with keep-alive and chunked encoding\n  - HTTP/2 with multiplexing\n  - TLS/SSL support\n  - WebSocket with binary and text frames\n\n- **Erlang Integration**\n  - Shared state via ETS (`hornbeam_state` module)\n  - Distributed RPC to remote nodes (`hornbeam_dist` module)\n  - Pub/Sub messaging via pg (`hornbeam_pubsub` module)\n  - Registered Erlang functions callable from Python (`hornbeam_callbacks` module)\n\n- **Hooks System**\n  - `on_request` - Modify requests before handling\n  - `on_response` - Modify responses before sending\n  - `on_error` - Custom error handling\n  - `on_worker_start` / `on_worker_exit` - Worker lifecycle hooks\n\n- **ML Integration**\n  - `hornbeam_ml` Python module for cached inference\n  - ETS-backed caching with hit/miss statistics\n  - Support for distributed ML across Erlang cluster\n\n- **Python Modules**\n  - `hornbeam_erlang` - State, RPC, Pub/Sub, and callback APIs\n  - `hornbeam_ml` - ML caching helpers\n  - `hornbeam_wsgi_runner` - WSGI request handling\n  - `hornbeam_asgi_runner` - ASGI request handling\n  - `hornbeam_websocket_runner` - WebSocket session handling\n  - `hornbeam_lifespan_runner` - Lifespan protocol handling\n\n- **Configuration**\n  - Server binding, SSL/TLS options\n  - Worker pool sizing and timeouts\n  - ASGI lifespan control\n  - WebSocket timeout and frame size limits\n  - Python path and virtual environment support\n\n- **Documentation**\n  - Getting started guide\n  - WSGI, ASGI, WebSocket guides\n  - Erlang integration guide\n  - ML integration guide\n  - Flask, FastAPI, WebSocket chat examples\n  - Embedding service and distributed ML examples\n  - Configuration reference\n\n- **Website**\n  - https://hornbeam.dev\n  - Product pages for Hornbeam and Erlang Python\n  - Integrated documentation with product switcher","ref":"changelog.html#added"},{"type":"extras","title":"Dependencies - Changelog","doc":"- Erlang/OTP 27+\n- Python 3.12+ (3.13+ recommended for free-threading)\n- Cowboy 2.12.0\n- erlang_python 1.2.0\n\n[1.0.0]: https://github.com/benoitc/hornbeam/releases/tag/1.0.0","ref":"changelog.html#dependencies"},{"type":"extras","title":"getting-started","doc":"---\ntitle: Getting Started\ndescription: Install and run your first Hornbeam application\norder: 1\n---\n\n# Getting Started\n\nThis guide will help you install Hornbeam and run your first Python web application on the BEAM.","ref":"getting-started.html"},{"type":"extras","title":"Requirements - getting-started","doc":"- Erlang/OTP 27+\n- Python 3.12+ (3.13+ recommended for free-threading)\n- C compiler (gcc or clang)","ref":"getting-started.html#requirements"},{"type":"extras","title":"Installation - getting-started","doc":"Add Hornbeam to your `rebar.config`:\n\n```erlang\n{deps, [\n    {hornbeam, {git, \"https://github.com/benoitc/hornbeam.git\", {branch, \"main\"}}}\n]}.\n```\n\nCompile:\n\n```bash\nrebar3 compile\n```","ref":"getting-started.html#installation"},{"type":"extras","title":"Quick Start - getting-started","doc":"","ref":"getting-started.html#quick-start"},{"type":"extras","title":"1. Create a Python Application - getting-started","doc":"Create a simple WSGI application:\n\n```python\n# myapp.py\ndef application(environ, start_response):\n    start_response('200 OK', [('Content-Type', 'text/plain')])\n    return [b'Hello from Hornbeam!']\n```","ref":"getting-started.html#1-create-a-python-application"},{"type":"extras","title":"2. Start Hornbeam - getting-started","doc":"```erlang\n%% Start the shell\nrebar3 shell\n\n%% Start hornbeam with your app\nhornbeam:start(\"myapp:application\").\n```","ref":"getting-started.html#2-start-hornbeam"},{"type":"extras","title":"3. Test It - getting-started","doc":"```bash\ncurl http://localhost:8000\n# Hello from Hornbeam!\n```","ref":"getting-started.html#3-test-it"},{"type":"extras","title":"Running ASGI Applications - getting-started","doc":"For async applications (FastAPI, Starlette):\n\n```python\n# myapp.py\nasync def application(scope, receive, send):\n    await send({\n        'type': 'http.response.start',\n        'status': 200,\n        'headers': [[b'content-type', b'text/plain']],\n    })\n    await send({\n        'type': 'http.response.body',\n        'body': b'Hello from ASGI!',\n    })\n```\n\n```erlang\nhornbeam:start(\"myapp:application\", #{worker_class => asgi}).\n```","ref":"getting-started.html#running-asgi-applications"},{"type":"extras","title":"Running Existing Frameworks - getting-started","doc":"","ref":"getting-started.html#running-existing-frameworks"},{"type":"extras","title":"Flask - getting-started","doc":"```python\n# app.py\nfrom flask import Flask\napp = Flask(__name__)\n\n@app.route('/')\ndef hello():\n    return 'Hello from Flask on Hornbeam!'\n\n# WSGI application\napplication = app\n```\n\n```erlang\nhornbeam:start(\"app:application\").\n```","ref":"getting-started.html#flask"},{"type":"extras","title":"FastAPI - getting-started","doc":"```python\n# app.py\nfrom fastapi import FastAPI\napp = FastAPI()\n\n@app.get(\"/\")\nasync def root():\n    return {\"message\": \"Hello from FastAPI on Hornbeam!\"}\n```\n\n```erlang\nhornbeam:start(\"app:app\", #{worker_class => asgi}).\n```","ref":"getting-started.html#fastapi"},{"type":"extras","title":"Django - getting-started","doc":"```python\n# myproject/wsgi.py (Django generates this)\nfrom django.core.wsgi import get_wsgi_application\napplication = get_wsgi_application()\n```\n\n```erlang\nhornbeam:start(\"myproject.wsgi:application\", #{\n    pythonpath => [\".\", \"myproject\"]\n}).\n```","ref":"getting-started.html#django"},{"type":"extras","title":"Configuration - getting-started","doc":"Basic configuration options:\n\n```erlang\nhornbeam:start(\"myapp:application\", #{\n    %% Server binding\n    bind => \"0.0.0.0:8000\",\n\n    %% Protocol: wsgi or asgi\n    worker_class => wsgi,\n\n    %% Number of Python workers\n    workers => 4,\n\n    %% Request timeout (ms)\n    timeout => 30000,\n\n    %% Python module search paths\n    pythonpath => [\".\", \"src\"]\n}).\n```\n\nSee [Configuration Reference](./reference/configuration) for all options.","ref":"getting-started.html#configuration"},{"type":"extras","title":"Using Erlang Features - getting-started","doc":"The real power of Hornbeam is accessing Erlang from Python:","ref":"getting-started.html#using-erlang-features"},{"type":"extras","title":"Shared State (ETS) - getting-started","doc":"```python\nfrom hornbeam_erlang import state_get, state_set, state_incr\n\ndef application(environ, start_response):\n    # Atomic counter - safe with millions of concurrent requests\n    views = state_incr('page_views')\n\n    # Cache expensive computations\n    data = state_get('cached_data')\n    if data is None:\n        data = expensive_computation()\n        state_set('cached_data', data)\n\n    start_response('200 OK', [('Content-Type', 'text/plain')])\n    return [f'Views: {views}'.encode()]\n```","ref":"getting-started.html#shared-state-ets"},{"type":"extras","title":"Distributed RPC - getting-started","doc":"```python\nfrom hornbeam_erlang import rpc_call, nodes\n\ndef application(environ, start_response):\n    # Call function on remote node\n    result = rpc_call('ml@gpu-server', 'model', 'predict', [data])\n\n    start_response('200 OK', [('Content-Type', 'application/json')])\n    return [json.dumps(result).encode()]\n```\n\nSee the [Erlang Integration Guide](./guides/erlang-integration) for more.","ref":"getting-started.html#distributed-rpc"},{"type":"extras","title":"Next Steps - getting-started","doc":"- [WSGI Guide](./guides/wsgi) - Full WSGI protocol details\n- [ASGI Guide](./guides/asgi) - Async apps and WebSocket\n- [Erlang Integration](./guides/erlang-integration) - ETS, RPC, Pub/Sub\n- [Examples](./examples/flask-app) - Complete working examples","ref":"getting-started.html#next-steps"},{"type":"extras","title":"wsgi","doc":"---\ntitle: WSGI Guide\ndescription: Running WSGI applications with Hornbeam\norder: 10\n---\n\n# WSGI Guide\n\nHornbeam provides full WSGI (PEP 3333) support, allowing you to run Flask, Django, Bottle, and any standard WSGI application.","ref":"wsgi.html"},{"type":"extras","title":"Basic WSGI Application - wsgi","doc":"```python\n# app.py\ndef application(environ, start_response):\n    path = environ.get('PATH_INFO', '/')\n\n    start_response('200 OK', [\n        ('Content-Type', 'text/plain'),\n    ])\n    return [f'You requested: {path}'.encode()]\n```\n\n```erlang\nhornbeam:start(\"app:application\").\n```","ref":"wsgi.html#basic-wsgi-application"},{"type":"extras","title":"WSGI Environ - wsgi","doc":"Hornbeam provides all standard WSGI environ variables:\n\n| Variable | Description |\n|----------|-------------|\n| `REQUEST_METHOD` | HTTP method (GET, POST, etc.) |\n| `SCRIPT_NAME` | URL path prefix |\n| `PATH_INFO` | URL path |\n| `QUERY_STRING` | Query string after `?` |\n| `CONTENT_TYPE` | Request content type |\n| `CONTENT_LENGTH` | Request body length |\n| `SERVER_NAME` | Server hostname |\n| `SERVER_PORT` | Server port |\n| `SERVER_PROTOCOL` | HTTP/1.1 or HTTP/2 |\n| `HTTP_*` | HTTP headers |","ref":"wsgi.html#wsgi-environ"},{"type":"extras","title":"WSGI Extensions - wsgi","doc":"| Variable | Description |\n|----------|-------------|\n| `wsgi.input` | Request body stream |\n| `wsgi.errors` | Error output stream |\n| `wsgi.url_scheme` | `http` or `https` |\n| `wsgi.file_wrapper` | Efficient file serving |\n| `wsgi.early_hints` | Send 103 Early Hints |","ref":"wsgi.html#wsgi-extensions"},{"type":"extras","title":"Running Flask - wsgi","doc":"```python\n# app.py\nfrom flask import Flask, jsonify\nfrom hornbeam_erlang import state_get, state_set, state_incr\n\napp = Flask(__name__)\n\n@app.route('/')\ndef index():\n    # Use ETS for atomic page view counter\n    views = state_incr('page_views')\n    return f'Page views: {views}'\n\n@app.route('/api/data')\ndef get_data():\n    # Cache expensive operations in ETS\n    cached = state_get('api_cache')\n    if cached is None:\n        cached = expensive_computation()\n        state_set('api_cache', cached, ttl=300)  # 5 min TTL\n    return jsonify(cached)\n\n# WSGI application callable\napplication = app\n```\n\n```erlang\nhornbeam:start(\"app:application\", #{\n    workers => 8,\n    timeout => 30000\n}).\n```","ref":"wsgi.html#running-flask"},{"type":"extras","title":"Running Django - wsgi","doc":"","ref":"wsgi.html#running-django"},{"type":"extras","title":"1. Create Django Project - wsgi","doc":"```bash\ndjango-admin startproject myproject\ncd myproject\npython manage.py migrate\n```","ref":"wsgi.html#1-create-django-project"},{"type":"extras","title":"2. Configure Settings - wsgi","doc":"```python\n# myproject/settings.py\nALLOWED_HOSTS = ['*']  # Configure appropriately for production\n```","ref":"wsgi.html#2-configure-settings"},{"type":"extras","title":"3. Start with Hornbeam - wsgi","doc":"```erlang\nhornbeam:start(\"myproject.wsgi:application\", #{\n    pythonpath => [\".\", \"myproject\"],\n    workers => 4\n}).\n```","ref":"wsgi.html#3-start-with-hornbeam"},{"type":"extras","title":"Running Bottle - wsgi","doc":"```python\n# app.py\nfrom bottle import Bottle, response\n\napp = Bottle()\n\n@app.route('/')\ndef index():\n    return 'Hello from Bottle!'\n\n@app.route('/json')\ndef json_endpoint():\n    response.content_type = 'application/json'\n    return '{\"status\": \"ok\"}'\n\n# WSGI application\napplication = app\n```","ref":"wsgi.html#running-bottle"},{"type":"extras","title":"File Uploads - wsgi","doc":"Handle file uploads efficiently:\n\n```python\ndef application(environ, start_response):\n    if environ['REQUEST_METHOD'] == 'POST':\n        content_length = int(environ.get('CONTENT_LENGTH', 0))\n        body = environ['wsgi.input'].read(content_length)\n\n        # Process uploaded file\n        # ...\n\n        start_response('200 OK', [('Content-Type', 'text/plain')])\n        return [b'Upload received']\n\n    start_response('405 Method Not Allowed', [])\n    return [b'POST only']\n```","ref":"wsgi.html#file-uploads"},{"type":"extras","title":"File Serving - wsgi","doc":"Use `wsgi.file_wrapper` for efficient static file serving:\n\n```python\nimport os\n\ndef application(environ, start_response):\n    path = environ.get('PATH_INFO', '/')[1:]  # Remove leading /\n    filepath = os.path.join('static', path)\n\n    if os.path.isfile(filepath):\n        file_wrapper = environ.get('wsgi.file_wrapper')\n\n        start_response('200 OK', [\n            ('Content-Type', 'application/octet-stream'),\n            ('Content-Length', str(os.path.getsize(filepath))),\n        ])\n\n        f = open(filepath, 'rb')\n        if file_wrapper:\n            return file_wrapper(f)\n        else:\n            return iter(lambda: f.read(8192), b'')\n\n    start_response('404 Not Found', [])\n    return [b'Not found']\n```","ref":"wsgi.html#file-serving"},{"type":"extras","title":"Early Hints (103) - wsgi","doc":"Send preload hints before the response:\n\n```python\ndef application(environ, start_response):\n    # Send early hints for resource preloading\n    early_hints = environ.get('wsgi.early_hints')\n    if early_hints:\n        early_hints([\n            ('Link', ' ; rel=preload; as=style'),\n            ('Link', ' ; rel=preload; as=script'),\n        ])\n\n    # Continue with normal response\n    start_response('200 OK', [('Content-Type', 'text/html')])\n    return [b' ... ']\n```","ref":"wsgi.html#early-hints-103"},{"type":"extras","title":"Error Handling - wsgi","doc":"```python\ndef application(environ, start_response):\n    try:\n        # Your application logic\n        result = process_request(environ)\n        start_response('200 OK', [('Content-Type', 'text/plain')])\n        return [result]\n    except ValueError as e:\n        start_response('400 Bad Request', [('Content-Type', 'text/plain')])\n        return [str(e).encode()]\n    except Exception as e:\n        # Log to wsgi.errors\n        environ['wsgi.errors'].write(f'Error: {e}\\n')\n        start_response('500 Internal Server Error', [('Content-Type', 'text/plain')])\n        return [b'Internal error']\n```","ref":"wsgi.html#error-handling"},{"type":"extras","title":"Middleware - wsgi","doc":"Standard WSGI middleware works with Hornbeam:\n\n```python\nclass LoggingMiddleware:\n    def __init__(self, app):\n        self.app = app\n\n    def __call__(self, environ, start_response):\n        path = environ.get('PATH_INFO', '/')\n        method = environ.get('REQUEST_METHOD')\n        print(f'{method} {path}')\n        return self.app(environ, start_response)\n\n# Wrap your application\nfrom myapp import app\napplication = LoggingMiddleware(app)\n```","ref":"wsgi.html#middleware"},{"type":"extras","title":"Configuration Options - wsgi","doc":"```erlang\nhornbeam:start(\"app:application\", #{\n    %% Protocol\n    worker_class => wsgi,\n\n    %% Workers\n    workers => 4,              % Number of Python workers\n    timeout => 30000,          % Request timeout (ms)\n    max_requests => 1000,      % Recycle workers after N requests\n\n    %% HTTP\n    keepalive => 2,            % Keep-alive timeout (seconds)\n\n    %% Python\n    pythonpath => [\".\", \"src\"]\n}).\n```","ref":"wsgi.html#configuration-options"},{"type":"extras","title":"Performance Tips - wsgi","doc":"1. **Use ETS for caching** - Faster than Redis for local state\n2. **Preload models** - Load ML models at startup with `preload_app => true`\n3. **Batch operations** - Use `state_get_multi` for multiple keys\n4. **Monitor workers** - Check pool stats with `hornbeam:pool_stats()`","ref":"wsgi.html#performance-tips"},{"type":"extras","title":"Next Steps - wsgi","doc":"- [ASGI Guide](./asgi) - For async applications\n- [Erlang Integration](./erlang-integration) - ETS, RPC, Pub/Sub\n- [Flask Example](../examples/flask-app) - Complete Flask application","ref":"wsgi.html#next-steps"},{"type":"extras","title":"asgi","doc":"---\ntitle: ASGI Guide\ndescription: Running async Python applications with Hornbeam\norder: 11\n---\n\n# ASGI Guide\n\nHornbeam provides full ASGI 3.0 support for async Python applications like FastAPI, Starlette, Quart, and custom async apps.","ref":"asgi.html"},{"type":"extras","title":"Basic ASGI Application - asgi","doc":"```python\n# app.py\nasync def application(scope, receive, send):\n    if scope['type'] == 'http':\n        await send({\n            'type': 'http.response.start',\n            'status': 200,\n            'headers': [[b'content-type', b'text/plain']],\n        })\n        await send({\n            'type': 'http.response.body',\n            'body': b'Hello from ASGI!',\n        })\n```\n\n```erlang\nhornbeam:start(\"app:application\", #{worker_class => asgi}).\n```","ref":"asgi.html#basic-asgi-application"},{"type":"extras","title":"ASGI Scope - asgi","doc":"The scope dict contains request information:","ref":"asgi.html#asgi-scope"},{"type":"extras","title":"HTTP Scope - asgi","doc":"| Key | Type | Description |\n|-----|------|-------------|\n| `type` | str | `\"http\"` |\n| `asgi` | dict | `{\"version\": \"3.0\"}` |\n| `http_version` | str | `\"1.1\"` or `\"2\"` |\n| `method` | str | HTTP method |\n| `scheme` | str | `\"http\"` or `\"https\"` |\n| `path` | str | URL path |\n| `query_string` | bytes | Query string |\n| `root_path` | str | ASGI root path |\n| `headers` | list | `[[name, value], ...]` |\n| `server` | tuple | `(host, port)` |\n| `client` | tuple | `(host, port)` or None |","ref":"asgi.html#http-scope"},{"type":"extras","title":"WebSocket Scope - asgi","doc":"| Key | Type | Description |\n|-----|------|-------------|\n| `type` | str | `\"websocket\"` |\n| `path` | str | URL path |\n| `query_string` | bytes | Query string |\n| `headers` | list | Request headers |\n| `subprotocols` | list | Requested subprotocols |","ref":"asgi.html#websocket-scope"},{"type":"extras","title":"Running FastAPI - asgi","doc":"```python\n# app.py\nfrom fastapi import FastAPI\nfrom hornbeam_erlang import state_get, state_set, state_incr\n\napp = FastAPI()\n\n@app.get(\"/\")\nasync def root():\n    views = state_incr('api_views')\n    return {\"views\": views}\n\n@app.get(\"/items/{item_id}\")\nasync def get_item(item_id: int):\n    # Check ETS cache first\n    cached = state_get(f'item:{item_id}')\n    if cached:\n        return cached\n\n    # Fetch and cache\n    item = await fetch_item(item_id)\n    state_set(f'item:{item_id}', item)\n    return item\n\n@app.post(\"/items/\")\nasync def create_item(item: dict):\n    item_id = state_incr('item_id_seq')\n    item['id'] = item_id\n    state_set(f'item:{item_id}', item)\n    return item\n```\n\n```erlang\nhornbeam:start(\"app:app\", #{\n    worker_class => asgi,\n    lifespan => on  % Enable startup/shutdown events\n}).\n```","ref":"asgi.html#running-fastapi"},{"type":"extras","title":"Running Starlette - asgi","doc":"```python\n# app.py\nfrom starlette.applications import Starlette\nfrom starlette.responses import JSONResponse\nfrom starlette.routing import Route\n\nasync def homepage(request):\n    return JSONResponse({'hello': 'world'})\n\nasync def user(request):\n    user_id = request.path_params['user_id']\n    return JSONResponse({'user_id': user_id})\n\napp = Starlette(routes=[\n    Route('/', homepage),\n    Route('/user/{user_id}', user),\n])\n```","ref":"asgi.html#running-starlette"},{"type":"extras","title":"Lifespan Protocol - asgi","doc":"Handle application startup and shutdown:\n\n```python\n# app.py\nfrom contextlib import asynccontextmanager\nfrom fastapi import FastAPI\n\nml_model = None\n\n@asynccontextmanager\nasync def lifespan(app):\n    # Startup: Load ML model\n    global ml_model\n    ml_model = load_model('model.pkl')\n    print(\"Model loaded!\")\n\n    yield  # Application runs here\n\n    # Shutdown: Cleanup\n    ml_model = None\n    print(\"Cleanup complete!\")\n\napp = FastAPI(lifespan=lifespan)\n\n@app.get(\"/predict\")\nasync def predict(text: str):\n    return {\"result\": ml_model.predict(text)}\n```\n\n```erlang\nhornbeam:start(\"app:app\", #{\n    worker_class => asgi,\n    lifespan => on  % Required for lifespan events\n}).\n```","ref":"asgi.html#lifespan-protocol"},{"type":"extras","title":"Lifespan Options - asgi","doc":"| Value | Description |\n|-------|-------------|\n| `auto` | Detect if app supports lifespan (default) |\n| `on` | Require lifespan, fail if unsupported |\n| `off` | Disable lifespan |","ref":"asgi.html#lifespan-options"},{"type":"extras","title":"Streaming Responses - asgi","doc":"```python\nasync def application(scope, receive, send):\n    if scope['type'] == 'http':\n        await send({\n            'type': 'http.response.start',\n            'status': 200,\n            'headers': [[b'content-type', b'text/plain']],\n        })\n\n        # Stream data in chunks\n        for i in range(10):\n            await send({\n                'type': 'http.response.body',\n                'body': f'Chunk {i}\\n'.encode(),\n                'more_body': True,\n            })\n            await asyncio.sleep(0.1)\n\n        # Final chunk\n        await send({\n            'type': 'http.response.body',\n            'body': b'Done!\\n',\n            'more_body': False,\n        })\n```","ref":"asgi.html#streaming-responses"},{"type":"extras","title":"Server-Sent Events (SSE) - asgi","doc":"```python\nfrom fastapi import FastAPI\nfrom fastapi.responses import StreamingResponse\nimport asyncio\n\napp = FastAPI()\n\nasync def event_generator():\n    for i in range(10):\n        yield f\"data: Event {i}\\n\\n\"\n        await asyncio.sleep(1)\n\n@app.get(\"/events\")\nasync def sse():\n    return StreamingResponse(\n        event_generator(),\n        media_type=\"text/event-stream\"\n    )\n```","ref":"asgi.html#server-sent-events-sse"},{"type":"extras","title":"Request Body - asgi","doc":"Read request body asynchronously:\n\n```python\nasync def application(scope, receive, send):\n    if scope['type'] == 'http':\n        # Read body\n        body = b''\n        while True:\n            message = await receive()\n            body += message.get('body', b'')\n            if not message.get('more_body', False):\n                break\n\n        # Process body\n        result = process(body)\n\n        await send({\n            'type': 'http.response.start',\n            'status': 200,\n            'headers': [[b'content-type', b'application/json']],\n        })\n        await send({\n            'type': 'http.response.body',\n            'body': json.dumps(result).encode(),\n        })\n```","ref":"asgi.html#request-body"},{"type":"extras","title":"Early Hints (103) - asgi","doc":"Send preload hints:\n\n```python\nasync def application(scope, receive, send):\n    if scope['type'] == 'http':\n        # Send 103 Early Hints\n        await send({\n            'type': 'http.response.start',\n            'status': 103,\n            'headers': [\n                [b'link', b' ; rel=preload; as=style'],\n            ],\n        })\n\n        # Then send actual response\n        await send({\n            'type': 'http.response.start',\n            'status': 200,\n            'headers': [[b'content-type', b'text/html']],\n        })\n        await send({\n            'type': 'http.response.body',\n            'body': b' ... ',\n        })\n```","ref":"asgi.html#early-hints-103"},{"type":"extras","title":"Middleware - asgi","doc":"ASGI middleware pattern:\n\n```python\nclass TimingMiddleware:\n    def __init__(self, app):\n        self.app = app\n\n    async def __call__(self, scope, receive, send):\n        if scope['type'] == 'http':\n            start = time.time()\n\n            async def send_wrapper(message):\n                if message['type'] == 'http.response.start':\n                    elapsed = time.time() - start\n                    headers = list(message.get('headers', []))\n                    headers.append([b'x-response-time', f'{elapsed:.3f}'.encode()])\n                    message = {**message, 'headers': headers}\n                await send(message)\n\n            await self.app(scope, receive, send_wrapper)\n        else:\n            await self.app(scope, receive, send)\n\n# Usage\nfrom myapp import app\napplication = TimingMiddleware(app)\n```","ref":"asgi.html#middleware"},{"type":"extras","title":"Configuration - asgi","doc":"```erlang\nhornbeam:start(\"app:app\", #{\n    %% Protocol\n    worker_class => asgi,\n    lifespan => auto,\n    root_path => \"\",\n\n    %% Workers\n    workers => 4,\n    timeout => 30000,\n\n    %% HTTP\n    http_version => ['HTTP/1.1', 'HTTP/2']\n}).\n```","ref":"asgi.html#configuration"},{"type":"extras","title":"Error Handling - asgi","doc":"```python\nasync def application(scope, receive, send):\n    try:\n        await handle_request(scope, receive, send)\n    except ValueError as e:\n        await send({\n            'type': 'http.response.start',\n            'status': 400,\n            'headers': [[b'content-type', b'text/plain']],\n        })\n        await send({\n            'type': 'http.response.body',\n            'body': str(e).encode(),\n        })\n    except Exception as e:\n        await send({\n            'type': 'http.response.start',\n            'status': 500,\n            'headers': [[b'content-type', b'text/plain']],\n        })\n        await send({\n            'type': 'http.response.body',\n            'body': b'Internal Server Error',\n        })\n```","ref":"asgi.html#error-handling"},{"type":"extras","title":"Next Steps - asgi","doc":"- [WebSocket Guide](./websocket) - Real-time communication\n- [Erlang Integration](./erlang-integration) - ETS, RPC, Pub/Sub\n- [FastAPI Example](../examples/fastapi-app) - Complete FastAPI application","ref":"asgi.html#next-steps"},{"type":"extras","title":"websocket","doc":"---\ntitle: WebSocket Guide\ndescription: Real-time bidirectional communication with WebSocket\norder: 12\n---\n\n# WebSocket Guide\n\nHornbeam provides full WebSocket support via ASGI, enabling real-time bidirectional communication for chat apps, live updates, multiplayer games, and more.","ref":"websocket.html"},{"type":"extras","title":"Basic WebSocket Handler - websocket","doc":"```python\n# app.py\nasync def application(scope, receive, send):\n    if scope['type'] == 'websocket':\n        # Accept the connection\n        await send({'type': 'websocket.accept'})\n\n        while True:\n            message = await receive()\n\n            if message['type'] == 'websocket.disconnect':\n                break\n\n            if message['type'] == 'websocket.receive':\n                # Echo the message back\n                text = message.get('text', '')\n                await send({\n                    'type': 'websocket.send',\n                    'text': f'Echo: {text}'\n                })\n```\n\n```erlang\nhornbeam:start(\"app:application\", #{worker_class => asgi}).\n```","ref":"websocket.html#basic-websocket-handler"},{"type":"extras","title":"WebSocket Scope - websocket","doc":"| Key | Type | Description |\n|-----|------|-------------|\n| `type` | str | `\"websocket\"` |\n| `path` | str | URL path |\n| `query_string` | bytes | Query string |\n| `headers` | list | Request headers |\n| `subprotocols` | list | Requested subprotocols |\n| `client` | tuple | `(host, port)` |\n| `server` | tuple | `(host, port)` |","ref":"websocket.html#websocket-scope"},{"type":"extras","title":"Message Types - websocket","doc":"","ref":"websocket.html#message-types"},{"type":"extras","title":"Receive Messages - websocket","doc":"| Type | Fields | Description |\n|------|--------|-------------|\n| `websocket.connect` | - | Client requesting connection |\n| `websocket.receive` | `text` or `bytes` | Message from client |\n| `websocket.disconnect` | `code` | Client disconnected |","ref":"websocket.html#receive-messages"},{"type":"extras","title":"Send Messages - websocket","doc":"| Type | Fields | Description |\n|------|--------|-------------|\n| `websocket.accept` | `subprotocol` | Accept connection |\n| `websocket.send` | `text` or `bytes` | Send to client |\n| `websocket.close` | `code`, `reason` | Close connection |","ref":"websocket.html#send-messages"},{"type":"extras","title":"Chat Room with Pub/Sub - websocket","doc":"Use Hornbeam's Erlang pub/sub for multi-user chat:\n\n```python\n# chat.py\nfrom hornbeam_erlang import publish, subscribe, unsubscribe\nimport asyncio\nimport json\n\nasync def application(scope, receive, send):\n    if scope['type'] == 'websocket':\n        await send({'type': 'websocket.accept'})\n\n        # Get room from path: /chat/room-name\n        path = scope.get('path', '/chat/default')\n        room = path.split('/')[-1] or 'default'\n        topic = f'chat:{room}'\n\n        # Subscribe to room\n        subscribe(topic)\n\n        try:\n            # Create tasks for receiving from client and pub/sub\n            await handle_chat(scope, receive, send, topic)\n        finally:\n            unsubscribe(topic)\n\nasync def handle_chat(scope, receive, send, topic):\n    while True:\n        message = await receive()\n\n        if message['type'] == 'websocket.disconnect':\n            break\n\n        if message['type'] == 'websocket.receive':\n            text = message.get('text', '')\n            data = json.loads(text)\n\n            # Broadcast to all subscribers\n            publish(topic, {\n                'type': 'message',\n                'user': data.get('user', 'anonymous'),\n                'text': data.get('text', '')\n            })\n\n        # Check for pub/sub messages\n        pubsub_msg = check_pubsub()\n        if pubsub_msg:\n            await send({\n                'type': 'websocket.send',\n                'text': json.dumps(pubsub_msg)\n            })\n```","ref":"websocket.html#chat-room-with-pub-sub"},{"type":"extras","title":"Binary Data - websocket","doc":"Send and receive binary data:\n\n```python\nasync def application(scope, receive, send):\n    if scope['type'] == 'websocket':\n        await send({'type': 'websocket.accept'})\n\n        while True:\n            message = await receive()\n\n            if message['type'] == 'websocket.disconnect':\n                break\n\n            if message['type'] == 'websocket.receive':\n                # Handle binary data\n                if 'bytes' in message:\n                    data = message['bytes']\n                    # Process binary (e.g., image, protobuf)\n                    result = process_binary(data)\n                    await send({\n                        'type': 'websocket.send',\n                        'bytes': result\n                    })\n                # Handle text\n                elif 'text' in message:\n                    await send({\n                        'type': 'websocket.send',\n                        'text': message['text']\n                    })\n```","ref":"websocket.html#binary-data"},{"type":"extras","title":"Subprotocols - websocket","doc":"Handle WebSocket subprotocols:\n\n```python\nasync def application(scope, receive, send):\n    if scope['type'] == 'websocket':\n        # Check requested subprotocols\n        requested = scope.get('subprotocols', [])\n\n        # Choose one we support\n        if 'graphql-ws' in requested:\n            subprotocol = 'graphql-ws'\n        elif 'json' in requested:\n            subprotocol = 'json'\n        else:\n            subprotocol = None\n\n        await send({\n            'type': 'websocket.accept',\n            'subprotocol': subprotocol\n        })\n\n        # Handle based on protocol\n        if subprotocol == 'graphql-ws':\n            await handle_graphql(scope, receive, send)\n        else:\n            await handle_generic(scope, receive, send)\n```","ref":"websocket.html#subprotocols"},{"type":"extras","title":"Connection with ETS State - websocket","doc":"Track connected clients using ETS:\n\n```python\nfrom hornbeam_erlang import state_set, state_get, state_delete, state_incr\nimport uuid\n\nasync def application(scope, receive, send):\n    if scope['type'] == 'websocket':\n        # Generate unique client ID\n        client_id = str(uuid.uuid4())\n\n        await send({'type': 'websocket.accept'})\n\n        # Register client\n        state_set(f'ws:client:{client_id}', {\n            'connected_at': time.time(),\n            'path': scope.get('path', '/')\n        })\n        state_incr('ws:active_connections')\n\n        try:\n            await handle_connection(scope, receive, send, client_id)\n        finally:\n            # Cleanup on disconnect\n            state_delete(f'ws:client:{client_id}')\n            state_incr('ws:active_connections', -1)\n\nasync def handle_connection(scope, receive, send, client_id):\n    while True:\n        message = await receive()\n        if message['type'] == 'websocket.disconnect':\n            break\n        # Handle messages...\n```","ref":"websocket.html#connection-with-ets-state"},{"type":"extras","title":"Live Dashboard Example - websocket","doc":"Push real-time updates to dashboard:\n\n```python\nfrom hornbeam_erlang import state_get\nimport asyncio\nimport json\n\nasync def application(scope, receive, send):\n    if scope['type'] == 'websocket':\n        await send({'type': 'websocket.accept'})\n\n        # Start background task to push updates\n        push_task = asyncio.create_task(\n            push_metrics(send)\n        )\n\n        try:\n            while True:\n                message = await receive()\n                if message['type'] == 'websocket.disconnect':\n                    break\n        finally:\n            push_task.cancel()\n\nasync def push_metrics(send):\n    while True:\n        # Get metrics from ETS\n        metrics = {\n            'requests': state_get('metrics:requests') or 0,\n            'errors': state_get('metrics:errors') or 0,\n            'latency_p99': state_get('metrics:latency_p99') or 0,\n        }\n\n        await send({\n            'type': 'websocket.send',\n            'text': json.dumps(metrics)\n        })\n\n        await asyncio.sleep(1)  # Update every second\n```","ref":"websocket.html#live-dashboard-example"},{"type":"extras","title":"Error Handling - websocket","doc":"```python\nasync def application(scope, receive, send):\n    if scope['type'] == 'websocket':\n        try:\n            await send({'type': 'websocket.accept'})\n            await handle_websocket(scope, receive, send)\n        except Exception as e:\n            # Close with error code\n            await send({\n                'type': 'websocket.close',\n                'code': 1011,  # Internal error\n                'reason': 'Internal server error'\n            })\n```","ref":"websocket.html#error-handling"},{"type":"extras","title":"Close Codes - websocket","doc":"| Code | Meaning |\n|------|---------|\n| 1000 | Normal closure |\n| 1001 | Going away |\n| 1002 | Protocol error |\n| 1003 | Unsupported data |\n| 1008 | Policy violation |\n| 1011 | Internal error |","ref":"websocket.html#close-codes"},{"type":"extras","title":"Configuration - websocket","doc":"```erlang\nhornbeam:start(\"app:application\", #{\n    worker_class => asgi,\n\n    %% WebSocket settings\n    websocket_timeout => 60000,        % Idle timeout (ms)\n    websocket_max_frame_size => 16777216  % Max frame size (16MB)\n}).\n```","ref":"websocket.html#configuration"},{"type":"extras","title":"FastAPI WebSocket - websocket","doc":"```python\nfrom fastapi import FastAPI, WebSocket, WebSocketDisconnect\nfrom hornbeam_erlang import state_incr\n\napp = FastAPI()\n\n@app.websocket(\"/ws/{room}\")\nasync def websocket_endpoint(websocket: WebSocket, room: str):\n    await websocket.accept()\n    state_incr(f'room:{room}:users')\n\n    try:\n        while True:\n            data = await websocket.receive_text()\n            await websocket.send_text(f\"Room {room}: {data}\")\n    except WebSocketDisconnect:\n        state_incr(f'room:{room}:users', -1)\n```","ref":"websocket.html#fastapi-websocket"},{"type":"extras","title":"Starlette WebSocket - websocket","doc":"```python\nfrom starlette.applications import Starlette\nfrom starlette.routing import WebSocketRoute\nfrom starlette.websockets import WebSocket\n\nasync def websocket_handler(websocket: WebSocket):\n    await websocket.accept()\n    while True:\n        text = await websocket.receive_text()\n        await websocket.send_text(f'Echo: {text}')\n\napp = Starlette(routes=[\n    WebSocketRoute('/ws', websocket_handler),\n])\n```","ref":"websocket.html#starlette-websocket"},{"type":"extras","title":"Testing WebSockets - websocket","doc":"","ref":"websocket.html#testing-websockets"},{"type":"extras","title":"Using websocat - websocket","doc":"```bash\nwebsocat ws://localhost:8000/ws\n```","ref":"websocket.html#using-websocat"},{"type":"extras","title":"Using Python - websocket","doc":"```python\nimport asyncio\nimport websockets\n\nasync def test():\n    async with websockets.connect('ws://localhost:8000/ws') as ws:\n        await ws.send('Hello!')\n        response = await ws.recv()\n        print(response)\n\nasyncio.run(test())\n```","ref":"websocket.html#using-python"},{"type":"extras","title":"Next Steps - websocket","doc":"- [Erlang Integration](./erlang-integration) - ETS, RPC, Pub/Sub details\n- [WebSocket Chat Example](../examples/websocket-chat) - Full chat application\n- [Configuration Reference](../reference/configuration) - All WebSocket options","ref":"websocket.html#next-steps"},{"type":"extras","title":"erlang-integration","doc":"---\ntitle: Erlang Integration\ndescription: Using ETS, RPC, Pub/Sub, and callbacks from Python\norder: 13\n---\n\n# Erlang Integration\n\nHornbeam's power comes from accessing Erlang features directly from Python. This guide covers shared state (ETS), distributed RPC, pub/sub messaging, and registered functions.","ref":"erlang-integration.html"},{"type":"extras","title":"Shared State (ETS) - erlang-integration","doc":"ETS (Erlang Term Storage) provides high-performance concurrent key-value storage accessible from Python.","ref":"erlang-integration.html#shared-state-ets"},{"type":"extras","title":"Basic Operations - erlang-integration","doc":"```python\nfrom hornbeam_erlang import (\n    state_get, state_set, state_delete,\n    state_incr, state_decr,\n    state_get_multi, state_keys\n)\n\n# Get and set values\nstate_set('user:123', {'name': 'Alice', 'email': 'alice@example.com'})\nuser = state_get('user:123')  # Returns dict or None\n\n# Delete\nstate_delete('user:123')\n```","ref":"erlang-integration.html#basic-operations"},{"type":"extras","title":"Atomic Counters - erlang-integration","doc":"Perfect for metrics, rate limiting, and sequences:\n\n```python\n# Increment (returns new value)\nviews = state_incr('page_views')           # +1\nviews = state_incr('page_views', 10)       # +10\n\n# Decrement\nremaining = state_decr('quota', 1)\n\n# Use for rate limiting\ndef check_rate_limit(user_id, limit=100):\n    key = f'rate:{user_id}:{minute()}'\n    count = state_incr(key)\n    if count == 1:\n        state_set(f'{key}:ttl', 60)  # Set TTL on first access\n    return count <= limit\n```","ref":"erlang-integration.html#atomic-counters"},{"type":"extras","title":"Batch Operations - erlang-integration","doc":"```python\n# Get multiple keys at once\nusers = state_get_multi(['user:1', 'user:2', 'user:3'])\n# Returns: {'user:1': {...}, 'user:2': {...}, 'user:3': None}\n\n# Find keys by prefix\nuser_keys = state_keys('user:')\n# Returns: ['user:1', 'user:2', 'user:123', ...]\n```","ref":"erlang-integration.html#batch-operations"},{"type":"extras","title":"Use Cases - erlang-integration","doc":"**Caching:**\n```python\ndef get_product(product_id):\n    cached = state_get(f'product:{product_id}')\n    if cached:\n        return cached\n\n    product = fetch_from_database(product_id)\n    state_set(f'product:{product_id}', product)\n    return product\n```\n\n**Session Storage:**\n```python\ndef get_session(session_id):\n    return state_get(f'session:{session_id}')\n\ndef set_session(session_id, data):\n    state_set(f'session:{session_id}', data)\n```\n\n**Real-time Metrics:**\n```python\ndef track_request(path, status, latency):\n    state_incr(f'metrics:requests:{path}')\n    state_incr(f'metrics:status:{status}')\n    # Update latency histogram\n    bucket = latency // 100 * 100  # Round to 100ms buckets\n    state_incr(f'metrics:latency:{bucket}')\n```","ref":"erlang-integration.html#use-cases"},{"type":"extras","title":"Distributed RPC - erlang-integration","doc":"Call functions on remote Erlang nodes in a cluster:\n\n```python\nfrom hornbeam_erlang import rpc_call, rpc_cast, nodes, node\n\n# Get cluster info\ncurrent = node()           # This node's name\nconnected = nodes()        # List of connected nodes\n\n# Synchronous call\nresult = rpc_call(\n    'worker@gpu-server',   # Remote node\n    'ml_model',            # Module\n    'predict',             # Function\n    [input_data],          # Arguments\n    timeout_ms=30000       # Timeout\n)\n\n# Async call (fire and forget)\nrpc_cast('logger@log-server', 'logger', 'log', [\n    'info', 'User logged in', {'user_id': 123}\n])\n```","ref":"erlang-integration.html#distributed-rpc"},{"type":"extras","title":"Distributed ML Inference - erlang-integration","doc":"Spread ML workloads across GPU nodes:\n\n```python\nfrom hornbeam_erlang import rpc_call, nodes\nimport asyncio\n\nasync def distributed_embedding(texts):\n    \"\"\"Distribute embedding computation across GPU nodes.\"\"\"\n    gpu_nodes = [n for n in nodes() if 'gpu' in n]\n\n    if not gpu_nodes:\n        # Fallback to local\n        return local_embed(texts)\n\n    # Split work across nodes\n    chunks = split_list(texts, len(gpu_nodes))\n    results = []\n\n    for node, chunk in zip(gpu_nodes, chunks):\n        result = rpc_call(\n            node,\n            'embedding_service',\n            'encode',\n            [chunk],\n            timeout_ms=60000\n        )\n        results.extend(result)\n\n    return results\n```","ref":"erlang-integration.html#distributed-ml-inference"},{"type":"extras","title":"Error Handling - erlang-integration","doc":"```python\nfrom hornbeam_erlang import rpc_call\n\ntry:\n    result = rpc_call('worker@remote', 'mod', 'func', [args])\nexcept TimeoutError:\n    # Node didn't respond in time\n    result = fallback()\nexcept ConnectionError:\n    # Node not connected\n    result = fallback()\nexcept Exception as e:\n    # Remote function raised an error\n    log_error(f\"RPC failed: {e}\")\n    result = fallback()\n```","ref":"erlang-integration.html#error-handling"},{"type":"extras","title":"Pub/Sub Messaging - erlang-integration","doc":"pg-based publish/subscribe for real-time messaging:\n\n```python\nfrom hornbeam_erlang import publish, subscribe, unsubscribe\n\n# Subscribe current process to topic\nsubscribe('notifications')\nsubscribe('user:123:events')\n\n# Publish message (returns subscriber count)\ncount = publish('notifications', {\n    'type': 'alert',\n    'message': 'Server restart in 5 minutes'\n})\n\n# Unsubscribe\nunsubscribe('notifications')\n```","ref":"erlang-integration.html#pub-sub-messaging"},{"type":"extras","title":"WebSocket with Pub/Sub - erlang-integration","doc":"```python\nfrom hornbeam_erlang import publish, subscribe, receive_pubsub\n\nasync def websocket_handler(scope, receive, send):\n    await send({'type': 'websocket.accept'})\n\n    user_id = get_user_from_scope(scope)\n    subscribe(f'user:{user_id}')\n\n    try:\n        while True:\n            # Check for client messages\n            message = await receive()\n            if message['type'] == 'websocket.disconnect':\n                break\n\n            # Check for pub/sub messages\n            pubsub_msg = receive_pubsub(timeout=0)\n            if pubsub_msg:\n                await send({\n                    'type': 'websocket.send',\n                    'text': json.dumps(pubsub_msg)\n                })\n    finally:\n        unsubscribe(f'user:{user_id}')\n```","ref":"erlang-integration.html#websocket-with-pub-sub"},{"type":"extras","title":"Broadcasting Updates - erlang-integration","doc":"```python\ndef update_product(product_id, data):\n    # Update in ETS\n    state_set(f'product:{product_id}', data)\n\n    # Notify all subscribers\n    publish(f'product:{product_id}', {\n        'type': 'updated',\n        'product_id': product_id,\n        'data': data\n    })\n```","ref":"erlang-integration.html#broadcasting-updates"},{"type":"extras","title":"Registered Functions - erlang-integration","doc":"Call Erlang functions from Python:","ref":"erlang-integration.html#registered-functions"},{"type":"extras","title":"Register in Erlang - erlang-integration","doc":"```erlang\n%% Register a simple function\nhornbeam:register_function(add, fun([A, B]) -> A + B end).\n\n%% Register a module function\nhornbeam:register_function(get_user, user_db, get).\n\n%% Register with validation\nhornbeam:register_function(validate_token, fun([Token]) ->\n    case auth:verify(Token) of\n        {ok, UserId} -> {ok, UserId};\n        error -> {error, invalid_token}\n    end\nend).\n```","ref":"erlang-integration.html#register-in-erlang"},{"type":"extras","title":"Call from Python - erlang-integration","doc":"```python\nfrom hornbeam_erlang import call, cast\n\n# Synchronous call\nresult = call('add', 1, 2)  # Returns 3\n\n# Get user from Erlang\nuser = call('get_user', user_id)\n\n# Validate token\ntry:\n    user_id = call('validate_token', token)\nexcept Exception as e:\n    return {'error': 'Invalid token'}\n\n# Async call (fire and forget)\ncast('log_event', 'user_login', {'user_id': 123})\n```","ref":"erlang-integration.html#call-from-python"},{"type":"extras","title":"Use Cases - erlang-integration","doc":"**Authentication:**\n```erlang\n%% Erlang side\nhornbeam:register_function(verify_session, fun([SessionId]) ->\n    case session_store:get(SessionId) of\n        {ok, Session} -> {ok, Session};\n        not_found -> {error, not_found}\n    end\nend).\n```\n\n```python\n# Python side\ndef get_current_user(request):\n    session_id = request.cookies.get('session_id')\n    if not session_id:\n        return None\n\n    try:\n        session = call('verify_session', session_id)\n        return session.get('user')\n    except:\n        return None\n```\n\n**Feature Flags:**\n```erlang\n%% Erlang side\nhornbeam:register_function(feature_enabled, fun([Feature, UserId]) ->\n    feature_flags:is_enabled(Feature, UserId)\nend).\n```\n\n```python\n# Python side\ndef get_features(user_id):\n    return {\n        'new_ui': call('feature_enabled', 'new_ui', user_id),\n        'beta': call('feature_enabled', 'beta', user_id),\n    }\n```","ref":"erlang-integration.html#use-cases-1"},{"type":"extras","title":"Hooks - erlang-integration","doc":"Execute Erlang code at key points in request lifecycle:\n\n```erlang\n%% Configure hooks\nhornbeam:start(\"app:application\", #{\n    hooks => #{\n        on_request => fun(Request) ->\n            %% Log, authenticate, modify request\n            Request\n        end,\n        on_response => fun(Response) ->\n            %% Modify response, log metrics\n            Response\n        end,\n        on_error => fun(Error, Request) ->\n            %% Error handling, alerting\n            error_logger:error_msg(\"Error: ~p~n\", [Error]),\n            {500, \"Internal Error\"}\n        end\n    }\n}).\n```","ref":"erlang-integration.html#hooks"},{"type":"extras","title":"Best Practices - erlang-integration","doc":"","ref":"erlang-integration.html#best-practices"},{"type":"extras","title":"1. Use ETS for Hot Data - erlang-integration","doc":"```python\n# Good: Frequently accessed, changes often\nstate_set('rate_limit:user:123', count)\nstate_set('session:abc123', session_data)\n\n# Bad: Large, rarely accessed (use database)\nstate_set('user:history', huge_list)\n```","ref":"erlang-integration.html#1-use-ets-for-hot-data"},{"type":"extras","title":"2. Atomic Operations - erlang-integration","doc":"```python\n# Good: Atomic increment\ncount = state_incr('counter')\n\n# Bad: Race condition\ncount = state_get('counter') or 0\nstate_set('counter', count + 1)\n```","ref":"erlang-integration.html#2-atomic-operations"},{"type":"extras","title":"3. Key Naming Convention - erlang-integration","doc":"```python\n# Use structured keys\nstate_set('user:123:profile', profile)\nstate_set('cache:product:456', product)\nstate_set('metric:requests:total', count)\n```","ref":"erlang-integration.html#3-key-naming-convention"},{"type":"extras","title":"4. Handle Missing Keys - erlang-integration","doc":"```python\n# state_get returns None for missing keys\nvalue = state_get('maybe_exists')\nif value is None:\n    value = compute_default()\n    state_set('maybe_exists', value)\n```","ref":"erlang-integration.html#4-handle-missing-keys"},{"type":"extras","title":"Next Steps - erlang-integration","doc":"- [ML Integration](./ml-integration) - Caching ML inference\n- [Distributed ML Example](../examples/distributed-ml) - Cluster inference\n- [Erlang API Reference](../reference/erlang-api) - All Erlang modules","ref":"erlang-integration.html#next-steps"},{"type":"extras","title":"ml-integration","doc":"---\ntitle: ML Integration\ndescription: Caching, distributed inference, and AI patterns\norder: 14\n---\n\n# ML Integration\n\nHornbeam makes Python ML/AI workloads production-ready with ETS caching, distributed inference, and Erlang's fault tolerance.","ref":"ml-integration.html"},{"type":"extras","title":"Cached Inference - ml-integration","doc":"Use ETS to cache ML inference results:\n\n```python\nfrom hornbeam_ml import cached_inference, cache_stats\n\n# Load model at startup\nfrom sentence_transformers import SentenceTransformer\nmodel = SentenceTransformer('all-MiniLM-L6-v2')\n\ndef get_embedding(text):\n    # Automatically cached by input hash\n    return cached_inference(model.encode, text)\n\n# Check cache performance\nstats = cache_stats()\n# {'hits': 1000, 'misses': 100, 'hit_rate': 0.91}\n```","ref":"ml-integration.html#cached-inference"},{"type":"extras","title":"Custom Cache Keys - ml-integration","doc":"```python\nfrom hornbeam_ml import cached_inference\n\ndef get_embedding(text, model_name='default'):\n    # Custom cache key includes model name\n    return cached_inference(\n        model.encode,\n        text,\n        cache_key=f'{model_name}:{hash(text)}',\n        cache_prefix='embedding'\n    )\n```","ref":"ml-integration.html#custom-cache-keys"},{"type":"extras","title":"Cache with TTL - ml-integration","doc":"```python\nfrom hornbeam_erlang import state_get, state_set\nimport hashlib\nimport time\n\ndef cached_with_ttl(fn, input, ttl_seconds=3600):\n    \"\"\"Cache result with time-to-live.\"\"\"\n    key = f'cache:{hashlib.md5(str(input).encode()).hexdigest()}'\n\n    cached = state_get(key)\n    if cached:\n        if time.time() - cached['timestamp'] < ttl_seconds:\n            return cached['result']\n\n    result = fn(input)\n    state_set(key, {\n        'result': result,\n        'timestamp': time.time()\n    })\n    return result\n```","ref":"ml-integration.html#cache-with-ttl"},{"type":"extras","title":"Embedding Service - ml-integration","doc":"Complete embedding service with caching:\n\n```python\n# embedding_service.py\nfrom fastapi import FastAPI\nfrom pydantic import BaseModel\nfrom hornbeam_ml import cached_inference, cache_stats\nfrom hornbeam_erlang import state_incr\nfrom sentence_transformers import SentenceTransformer\nimport numpy as np\n\napp = FastAPI()\nmodel = None\n\n@app.on_event(\"startup\")\nasync def load_model():\n    global model\n    model = SentenceTransformer('all-MiniLM-L6-v2')\n\nclass EmbedRequest(BaseModel):\n    texts: list[str]\n\nclass EmbedResponse(BaseModel):\n    embeddings: list[list[float]]\n    cache_hits: int\n    cache_misses: int\n\n@app.post(\"/embed\", response_model=EmbedResponse)\nasync def embed(request: EmbedRequest):\n    state_incr('metrics:embed_requests')\n\n    embeddings = []\n    for text in request.texts:\n        emb = cached_inference(model.encode, text)\n        embeddings.append(emb.tolist())\n\n    stats = cache_stats()\n    return EmbedResponse(\n        embeddings=embeddings,\n        cache_hits=stats['hits'],\n        cache_misses=stats['misses']\n    )\n\n@app.get(\"/stats\")\nasync def get_stats():\n    return cache_stats()\n```\n\n```erlang\nhornbeam:start(\"embedding_service:app\", #{\n    worker_class => asgi,\n    lifespan => on,\n    workers => 4\n}).\n```","ref":"ml-integration.html#embedding-service"},{"type":"extras","title":"Semantic Search - ml-integration","doc":"Build semantic search with ETS-cached embeddings:\n\n```python\nfrom hornbeam_erlang import state_get, state_set, state_keys\nfrom hornbeam_ml import cached_inference\nimport numpy as np\n\ndef index_document(doc_id, text):\n    \"\"\"Index a document for semantic search.\"\"\"\n    embedding = cached_inference(model.encode, text)\n    state_set(f'doc:{doc_id}', {\n        'text': text,\n        'embedding': embedding.tolist()\n    })\n\ndef search(query, top_k=10):\n    \"\"\"Search documents by semantic similarity.\"\"\"\n    query_emb = cached_inference(model.encode, query)\n    query_emb = np.array(query_emb)\n\n    # Get all documents\n    doc_keys = state_keys('doc:')\n    results = []\n\n    for key in doc_keys:\n        doc = state_get(key)\n        if doc:\n            doc_emb = np.array(doc['embedding'])\n            similarity = cosine_similarity(query_emb, doc_emb)\n            results.append({\n                'id': key.replace('doc:', ''),\n                'text': doc['text'],\n                'score': float(similarity)\n            })\n\n    # Sort by similarity\n    results.sort(key=lambda x: x['score'], reverse=True)\n    return results[:top_k]\n\ndef cosine_similarity(a, b):\n    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n```","ref":"ml-integration.html#semantic-search"},{"type":"extras","title":"Distributed Inference - ml-integration","doc":"Spread ML workloads across a cluster:\n\n```python\nfrom hornbeam_erlang import rpc_call, nodes\nimport asyncio\n\nclass DistributedInference:\n    def __init__(self, node_filter='gpu'):\n        self.node_filter = node_filter\n\n    def get_worker_nodes(self):\n        \"\"\"Get available GPU nodes.\"\"\"\n        return [n for n in nodes() if self.node_filter in n]\n\n    async def predict_batch(self, inputs):\n        \"\"\"Distribute predictions across nodes.\"\"\"\n        worker_nodes = self.get_worker_nodes()\n\n        if not worker_nodes:\n            # No GPU nodes, run locally\n            return self.local_predict(inputs)\n\n        # Split inputs across workers\n        chunks = self.split_inputs(inputs, len(worker_nodes))\n        results = []\n\n        for node, chunk in zip(worker_nodes, chunks):\n            try:\n                result = rpc_call(\n                    node,\n                    'ml_worker',\n                    'predict',\n                    [chunk],\n                    timeout_ms=60000\n                )\n                results.extend(result)\n            except Exception as e:\n                # Fallback to local on failure\n                results.extend(self.local_predict(chunk))\n\n        return results\n\n    def split_inputs(self, inputs, n):\n        \"\"\"Split list into n roughly equal chunks.\"\"\"\n        k, m = divmod(len(inputs), n)\n        return [inputs[i*k+min(i,m):(i+1)*k+min(i+1,m)] for i in range(n)]\n\n    def local_predict(self, inputs):\n        return model.predict(inputs)\n```","ref":"ml-integration.html#distributed-inference"},{"type":"extras","title":"GPU Node Setup - ml-integration","doc":"On each GPU node:\n\n```erlang\n%% gpu_node.erl\n-module(ml_worker).\n-export([predict/1, encode/1]).\n\npredict(Inputs) ->\n    py:call('model', 'predict', [Inputs]).\n\nencode(Texts) ->\n    py:call('model', 'encode', [Texts]).\n```","ref":"ml-integration.html#gpu-node-setup"},{"type":"extras","title":"LLM Integration - ml-integration","doc":"Integrate with LLM APIs:\n\n```python\nfrom hornbeam_erlang import state_get, state_set\nimport hashlib\nimport openai\n\ndef cached_llm_call(prompt, model=\"gpt-4\", temperature=0):\n    \"\"\"Cache LLM responses for identical prompts.\"\"\"\n    # Only cache deterministic (temp=0) responses\n    if temperature == 0:\n        cache_key = f'llm:{model}:{hashlib.md5(prompt.encode()).hexdigest()}'\n        cached = state_get(cache_key)\n        if cached:\n            return cached\n\n    response = openai.chat.completions.create(\n        model=model,\n        messages=[{\"role\": \"user\", \"content\": prompt}],\n        temperature=temperature\n    )\n\n    result = response.choices[0].message.content\n\n    if temperature == 0:\n        state_set(cache_key, result)\n\n    return result\n```","ref":"ml-integration.html#llm-integration"},{"type":"extras","title":"Streaming LLM Responses - ml-integration","doc":"```python\nfrom fastapi import FastAPI\nfrom fastapi.responses import StreamingResponse\nimport openai\n\napp = FastAPI()\n\nasync def stream_llm(prompt):\n    stream = openai.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": prompt}],\n        stream=True\n    )\n\n    for chunk in stream:\n        if chunk.choices[0].delta.content:\n            yield f\"data: {chunk.choices[0].delta.content}\\n\\n\"\n\n    yield \"data: [DONE]\\n\\n\"\n\n@app.post(\"/chat/stream\")\nasync def chat_stream(prompt: str):\n    return StreamingResponse(\n        stream_llm(prompt),\n        media_type=\"text/event-stream\"\n    )\n```","ref":"ml-integration.html#streaming-llm-responses"},{"type":"extras","title":"RAG (Retrieval-Augmented Generation) - ml-integration","doc":"Combine semantic search with LLM:\n\n```python\nfrom hornbeam_ml import cached_inference\nfrom hornbeam_erlang import state_get, state_keys\nimport openai\n\ndef rag_query(question, top_k=5):\n    \"\"\"Answer question using retrieved context.\"\"\"\n\n    # 1. Retrieve relevant documents\n    docs = search(question, top_k=top_k)\n\n    # 2. Build context\n    context = \"\\n\\n\".join([\n        f\"Document {i+1}:\\n{doc['text']}\"\n        for i, doc in enumerate(docs)\n    ])\n\n    # 3. Generate answer with LLM\n    prompt = f\"\"\"Answer the question based on the following context.\n\nContext:\n{context}\n\nQuestion: {question}\n\nAnswer:\"\"\"\n\n    response = openai.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n    )\n\n    return {\n        'answer': response.choices[0].message.content,\n        'sources': [doc['id'] for doc in docs]\n    }\n```","ref":"ml-integration.html#rag-retrieval-augmented-generation"},{"type":"extras","title":"Model Loading with Lifespan - ml-integration","doc":"Load models at startup, not per-request:\n\n```python\nfrom contextlib import asynccontextmanager\nfrom fastapi import FastAPI\n\nmodels = {}\n\n@asynccontextmanager\nasync def lifespan(app):\n    # Startup: Load all models\n    models['embedding'] = SentenceTransformer('all-MiniLM-L6-v2')\n    models['classifier'] = load_classifier('model.pkl')\n    print(\"Models loaded!\")\n\n    yield\n\n    # Shutdown: Cleanup\n    models.clear()\n\napp = FastAPI(lifespan=lifespan)\n\n@app.post(\"/embed\")\nasync def embed(text: str):\n    return models['embedding'].encode(text).tolist()\n```","ref":"ml-integration.html#model-loading-with-lifespan"},{"type":"extras","title":"Batch Processing - ml-integration","doc":"Efficient batch processing with BEAM parallelism:\n\n```python\nfrom hornbeam_erlang import rpc_call\nimport concurrent.futures\n\ndef process_batch(items, batch_size=100):\n    \"\"\"Process items in parallel batches.\"\"\"\n    results = []\n\n    # Split into batches\n    batches = [items[i:i+batch_size] for i in range(0, len(items), batch_size)]\n\n    # Process batches in parallel using Erlang processes\n    with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:\n        futures = [\n            executor.submit(process_single_batch, batch)\n            for batch in batches\n        ]\n\n        for future in concurrent.futures.as_completed(futures):\n            results.extend(future.result())\n\n    return results\n\ndef process_single_batch(batch):\n    # This runs in a separate BEAM process\n    embeddings = model.encode(batch)\n    return embeddings.tolist()\n```","ref":"ml-integration.html#batch-processing"},{"type":"extras","title":"Monitoring ML Performance - ml-integration","doc":"```python\nfrom hornbeam_erlang import state_incr, state_get\nimport time\n\nclass MLMetrics:\n    @staticmethod\n    def track_inference(model_name, latency_ms):\n        state_incr(f'ml:{model_name}:calls')\n        state_incr(f'ml:{model_name}:latency_total', int(latency_ms))\n\n        # Track latency buckets\n        bucket = (latency_ms // 100) * 100\n        state_incr(f'ml:{model_name}:latency:{bucket}')\n\n    @staticmethod\n    def get_stats(model_name):\n        calls = state_get(f'ml:{model_name}:calls') or 0\n        latency_total = state_get(f'ml:{model_name}:latency_total') or 0\n\n        return {\n            'calls': calls,\n            'avg_latency_ms': latency_total / calls if calls > 0 else 0\n        }\n\n# Usage\ndef timed_inference(model, input):\n    start = time.time()\n    result = model.predict(input)\n    latency = (time.time() - start) * 1000\n    MLMetrics.track_inference('classifier', latency)\n    return result\n```","ref":"ml-integration.html#monitoring-ml-performance"},{"type":"extras","title":"Next Steps - ml-integration","doc":"- [Embedding Service Example](../examples/embedding-service) - Complete service\n- [Distributed ML Example](../examples/distributed-ml) - Cluster setup\n- [Erlang Python AI Guide](https://hexdocs.pm/erlang_python/ai-integration.html) - Low-level AI integration","ref":"ml-integration.html#next-steps"},{"type":"extras","title":"flask-app","doc":"---\ntitle: Flask Application\ndescription: Running Flask on Hornbeam with ETS caching\norder: 20\n---\n\n# Flask Application Example\n\nThis example shows a Flask application running on Hornbeam with ETS-backed caching and metrics.","ref":"flask-app.html"},{"type":"extras","title":"Project Structure - flask-app","doc":"```\nflask_demo/\n app.py           # Flask application\n models.py        # Data models\n requirements.txt # Python dependencies\n```","ref":"flask-app.html#project-structure"},{"type":"extras","title":"Application Code - flask-app","doc":"```python\n# app.py\nfrom flask import Flask, jsonify, request\nfrom hornbeam_erlang import state_get, state_set, state_incr, state_delete\n\napp = Flask(__name__)\n\n# ============================================================\n# Middleware: Track request metrics\n# ============================================================\n\n@app.before_request\ndef track_request():\n    state_incr('metrics:requests:total')\n    state_incr(f'metrics:requests:{request.endpoint or \"unknown\"}')\n\n# ============================================================\n# Routes\n# ============================================================\n\n@app.route('/')\ndef index():\n    views = state_incr('page:home:views')\n    return jsonify({\n        'message': 'Welcome to Flask on Hornbeam!',\n        'views': views\n    })\n\n@app.route('/api/items', methods=['GET'])\ndef list_items():\n    \"\"\"List all items with caching.\"\"\"\n    # Check cache first\n    cached = state_get('cache:items:list')\n    if cached:\n        return jsonify({'items': cached, 'cached': True})\n\n    # Simulate database query\n    items = fetch_items_from_db()\n\n    # Cache for 60 seconds\n    state_set('cache:items:list', items)\n\n    return jsonify({'items': items, 'cached': False})\n\n@app.route('/api/items/ ', methods=['GET'])\ndef get_item(item_id):\n    \"\"\"Get single item with caching.\"\"\"\n    cache_key = f'cache:item:{item_id}'\n\n    cached = state_get(cache_key)\n    if cached:\n        return jsonify({'item': cached, 'cached': True})\n\n    item = fetch_item_from_db(item_id)\n    if not item:\n        return jsonify({'error': 'Not found'}), 404\n\n    state_set(cache_key, item)\n    return jsonify({'item': item, 'cached': False})\n\n@app.route('/api/items', methods=['POST'])\ndef create_item():\n    \"\"\"Create item and invalidate cache.\"\"\"\n    data = request.get_json()\n\n    # Generate ID using atomic counter\n    item_id = state_incr('items:id_seq')\n\n    item = {\n        'id': item_id,\n        'name': data.get('name'),\n        'description': data.get('description')\n    }\n\n    # Store in ETS (simulating database)\n    state_set(f'item:{item_id}', item)\n\n    # Invalidate list cache\n    state_delete('cache:items:list')\n\n    return jsonify({'item': item}), 201\n\n@app.route('/api/items/ ', methods=['DELETE'])\ndef delete_item(item_id):\n    \"\"\"Delete item and invalidate caches.\"\"\"\n    state_delete(f'item:{item_id}')\n    state_delete(f'cache:item:{item_id}')\n    state_delete('cache:items:list')\n\n    return '', 204\n\n# ============================================================\n# Metrics endpoint\n# ============================================================\n\n@app.route('/metrics')\ndef metrics():\n    \"\"\"Return application metrics.\"\"\"\n    return jsonify({\n        'requests': {\n            'total': state_get('metrics:requests:total') or 0,\n        },\n        'items': {\n            'count': len(list_all_items()),\n            'id_seq': state_get('items:id_seq') or 0\n        },\n        'cache': {\n            'items_list_cached': state_get('cache:items:list') is not None\n        }\n    })\n\n# ============================================================\n# Rate limiting example\n# ============================================================\n\n@app.route('/api/limited')\ndef rate_limited_endpoint():\n    \"\"\"Endpoint with rate limiting.\"\"\"\n    # Get client IP\n    client_ip = request.remote_addr\n\n    # Rate limit key (per minute)\n    from datetime import datetime\n    minute = datetime.now().strftime('%Y%m%d%H%M')\n    limit_key = f'ratelimit:{client_ip}:{minute}'\n\n    # Check and increment\n    count = state_incr(limit_key)\n\n    if count > 100:  # 100 requests per minute\n        return jsonify({'error': 'Rate limit exceeded'}), 429\n\n    return jsonify({'remaining': 100 - count})\n\n# ============================================================\n# Helper functions\n# ============================================================\n\ndef fetch_items_from_db():\n    \"\"\"Simulate database fetch.\"\"\"\n    return list_all_items()\n\ndef fetch_item_from_db(item_id):\n    \"\"\"Simulate database fetch.\"\"\"\n    return state_get(f'item:{item_id}')\n\ndef list_all_items():\n    \"\"\"List all items from ETS.\"\"\"\n    from hornbeam_erlang import state_keys\n    keys = state_keys('item:')\n    items = []\n    for key in keys:\n        if not key.startswith('items:'):  # Skip metadata keys\n            item = state_get(key)\n            if item:\n                items.append(item)\n    return items\n\n# ============================================================\n# WSGI application\n# ============================================================\n\napplication = app\n\n# For gunicorn compatibility\nif __name__ == '__main__':\n    app.run(debug=True)\n```","ref":"flask-app.html#application-code"},{"type":"extras","title":"Running with Hornbeam - flask-app","doc":"```erlang\n%% Start Erlang shell\nrebar3 shell\n\n%% Start Hornbeam\nhornbeam:start(\"app:application\", #{\n    pythonpath => [\"flask_demo\"],\n    workers => 4,\n    timeout => 30000\n}).\n```","ref":"flask-app.html#running-with-hornbeam"},{"type":"extras","title":"Running with Gunicorn (for comparison) - flask-app","doc":"```bash\ncd flask_demo\npip install gunicorn\ngunicorn app:application\n```","ref":"flask-app.html#running-with-gunicorn-for-comparison"},{"type":"extras","title":"Testing - flask-app","doc":"```bash\n# Create items\ncurl -X POST http://localhost:8000/api/items \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"name\": \"Item 1\", \"description\": \"First item\"}'\n\ncurl -X POST http://localhost:8000/api/items \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"name\": \"Item 2\", \"description\": \"Second item\"}'\n\n# List items (first call - not cached)\ncurl http://localhost:8000/api/items\n# {\"items\": [...], \"cached\": false}\n\n# List items (second call - cached)\ncurl http://localhost:8000/api/items\n# {\"items\": [...], \"cached\": true}\n\n# Get single item\ncurl http://localhost:8000/api/items/1\n\n# Check metrics\ncurl http://localhost:8000/metrics\n\n# Test rate limiting\nfor i in {1..105}; do curl http://localhost:8000/api/limited; done\n```","ref":"flask-app.html#testing"},{"type":"extras","title":"Requirements - flask-app","doc":"```\n# requirements.txt\nflask>=2.0\n```","ref":"flask-app.html#requirements"},{"type":"extras","title":"Key Features Demonstrated - flask-app","doc":"1. **Atomic counters** for IDs and metrics\n2. **ETS caching** with cache invalidation\n3. **Rate limiting** using ETS\n4. **Request tracking** for metrics\n5. **Gunicorn compatibility** (fallback when not on Hornbeam)","ref":"flask-app.html#key-features-demonstrated"},{"type":"extras","title":"Making It Gunicorn-Compatible - flask-app","doc":"For code that works on both Hornbeam and gunicorn:\n\n```python\n# Check if running on Hornbeam\ntry:\n    from hornbeam_erlang import state_get, state_set, state_incr\n    ON_HORNBEAM = True\nexcept ImportError:\n    ON_HORNBEAM = False\n\n    # Fallback implementations\n    _cache = {}\n\n    def state_get(key):\n        return _cache.get(key)\n\n    def state_set(key, value):\n        _cache[key] = value\n\n    def state_incr(key, delta=1):\n        _cache[key] = _cache.get(key, 0) + delta\n        return _cache[key]\n```","ref":"flask-app.html#making-it-gunicorn-compatible"},{"type":"extras","title":"Next Steps - flask-app","doc":"- [FastAPI Example](./fastapi-app) - Async application\n- [Erlang Integration Guide](../guides/erlang-integration) - More ETS patterns","ref":"flask-app.html#next-steps"},{"type":"extras","title":"fastapi-app","doc":"---\ntitle: FastAPI Application\ndescription: Running FastAPI on Hornbeam with async features\norder: 21\n---\n\n# FastAPI Application Example\n\nThis example demonstrates a FastAPI application on Hornbeam with async endpoints, WebSocket support, and ML integration.","ref":"fastapi-app.html"},{"type":"extras","title":"Project Structure - fastapi-app","doc":"```\nfastapi_demo/\n app.py           # FastAPI application\n models.py        # Pydantic models\n ml.py            # ML model loading\n requirements.txt\n```","ref":"fastapi-app.html#project-structure"},{"type":"extras","title":"Application Code - fastapi-app","doc":"```python\n# app.py\nfrom contextlib import asynccontextmanager\nfrom fastapi import FastAPI, WebSocket, WebSocketDisconnect, HTTPException\nfrom fastapi.responses import StreamingResponse\nfrom pydantic import BaseModel\nfrom hornbeam_erlang import (\n    state_get, state_set, state_incr, state_delete,\n    state_keys, publish\n)\nfrom hornbeam_ml import cached_inference\nimport asyncio\nimport json\n\n# ============================================================\n# Models\n# ============================================================\n\nclass Item(BaseModel):\n    name: str\n    description: str | None = None\n    price: float\n\nclass ItemResponse(BaseModel):\n    id: int\n    name: str\n    description: str | None\n    price: float\n\nclass EmbedRequest(BaseModel):\n    text: str\n\nclass SearchRequest(BaseModel):\n    query: str\n    top_k: int = 5\n\n# ============================================================\n# Lifespan: Load ML models at startup\n# ============================================================\n\nml_model = None\n\n@asynccontextmanager\nasync def lifespan(app):\n    global ml_model\n    # Startup\n    print(\"Loading ML model...\")\n    from sentence_transformers import SentenceTransformer\n    ml_model = SentenceTransformer('all-MiniLM-L6-v2')\n    print(\"Model loaded!\")\n\n    yield\n\n    # Shutdown\n    ml_model = None\n    print(\"Cleanup complete\")\n\napp = FastAPI(\n    title=\"FastAPI on Hornbeam\",\n    lifespan=lifespan\n)\n\n# ============================================================\n# Middleware\n# ============================================================\n\n@app.middleware(\"http\")\nasync def track_requests(request, call_next):\n    state_incr('metrics:requests')\n    response = await call_next(request)\n    state_incr(f'metrics:status:{response.status_code}')\n    return response\n\n# ============================================================\n# CRUD Endpoints\n# ============================================================\n\n@app.post(\"/items/\", response_model=ItemResponse)\nasync def create_item(item: Item):\n    item_id = state_incr('items:seq')\n\n    item_data = {\n        'id': item_id,\n        **item.model_dump()\n    }\n    state_set(f'item:{item_id}', item_data)\n\n    # Notify subscribers\n    publish('items:created', item_data)\n\n    return item_data\n\n@app.get(\"/items/\", response_model=list[ItemResponse])\nasync def list_items():\n    keys = state_keys('item:')\n    items = []\n    for key in keys:\n        if key.startswith('item:') and ':' not in key[5:]:\n            item = state_get(key)\n            if item:\n                items.append(item)\n    return items\n\n@app.get(\"/items/{item_id}\", response_model=ItemResponse)\nasync def get_item(item_id: int):\n    item = state_get(f'item:{item_id}')\n    if not item:\n        raise HTTPException(status_code=404, detail=\"Item not found\")\n    return item\n\n@app.delete(\"/items/{item_id}\")\nasync def delete_item(item_id: int):\n    if not state_get(f'item:{item_id}'):\n        raise HTTPException(status_code=404, detail=\"Item not found\")\n\n    state_delete(f'item:{item_id}')\n    publish('items:deleted', {'id': item_id})\n    return {\"deleted\": item_id}\n\n# ============================================================\n# ML Endpoints\n# ============================================================\n\n@app.post(\"/embed\")\nasync def embed_text(request: EmbedRequest):\n    \"\"\"Generate embedding with caching.\"\"\"\n    embedding = cached_inference(ml_model.encode, request.text)\n    return {\n        'text': request.text,\n        'embedding': embedding.tolist(),\n        'dimensions': len(embedding)\n    }\n\n@app.post(\"/search\")\nasync def semantic_search(request: SearchRequest):\n    \"\"\"Search items by semantic similarity.\"\"\"\n    import numpy as np\n\n    query_embedding = cached_inference(ml_model.encode, request.query)\n    query_embedding = np.array(query_embedding)\n\n    # Get all items with embeddings\n    results = []\n    for key in state_keys('item:'):\n        if ':emb' in key:\n            continue\n\n        item = state_get(key)\n        if not item:\n            continue\n\n        # Get or compute embedding\n        emb_key = f'{key}:emb'\n        item_emb = state_get(emb_key)\n\n        if not item_emb:\n            text = f\"{item['name']} {item.get('description', '')}\"\n            item_emb = cached_inference(ml_model.encode, text).tolist()\n            state_set(emb_key, item_emb)\n\n        # Compute similarity\n        item_emb = np.array(item_emb)\n        similarity = np.dot(query_embedding, item_emb) / (\n            np.linalg.norm(query_embedding) * np.linalg.norm(item_emb)\n        )\n\n        results.append({\n            **item,\n            'score': float(similarity)\n        })\n\n    # Sort by similarity\n    results.sort(key=lambda x: x['score'], reverse=True)\n    return results[:request.top_k]\n\n# ============================================================\n# Streaming\n# ============================================================\n\n@app.get(\"/stream\")\nasync def stream_data():\n    \"\"\"Stream data using Server-Sent Events.\"\"\"\n    async def generate():\n        for i in range(10):\n            data = {\n                'count': i,\n                'requests': state_get('metrics:requests') or 0\n            }\n            yield f\"data: {json.dumps(data)}\\n\\n\"\n            await asyncio.sleep(1)\n        yield \"data: {\\\"done\\\": true}\\n\\n\"\n\n    return StreamingResponse(\n        generate(),\n        media_type=\"text/event-stream\"\n    )\n\n# ============================================================\n# WebSocket\n# ============================================================\n\nclass ConnectionManager:\n    def __init__(self):\n        self.active_connections: list[WebSocket] = []\n\n    async def connect(self, websocket: WebSocket):\n        await websocket.accept()\n        self.active_connections.append(websocket)\n        state_incr('ws:connections')\n\n    def disconnect(self, websocket: WebSocket):\n        self.active_connections.remove(websocket)\n        state_incr('ws:connections', -1)\n\n    async def broadcast(self, message: str):\n        for connection in self.active_connections:\n            await connection.send_text(message)\n\nmanager = ConnectionManager()\n\n@app.websocket(\"/ws\")\nasync def websocket_endpoint(websocket: WebSocket):\n    await manager.connect(websocket)\n    try:\n        while True:\n            data = await websocket.receive_text()\n            message = json.loads(data)\n\n            # Echo back with metadata\n            response = {\n                'received': message,\n                'connections': len(manager.active_connections),\n                'timestamp': asyncio.get_event_loop().time()\n            }\n            await websocket.send_text(json.dumps(response))\n\n    except WebSocketDisconnect:\n        manager.disconnect(websocket)\n\n@app.websocket(\"/ws/broadcast\")\nasync def broadcast_endpoint(websocket: WebSocket):\n    \"\"\"WebSocket that broadcasts to all connections.\"\"\"\n    await manager.connect(websocket)\n    try:\n        while True:\n            data = await websocket.receive_text()\n            # Broadcast to all connected clients\n            await manager.broadcast(data)\n    except WebSocketDisconnect:\n        manager.disconnect(websocket)\n\n# ============================================================\n# Metrics\n# ============================================================\n\n@app.get(\"/metrics\")\nasync def get_metrics():\n    from hornbeam_ml import cache_stats\n\n    return {\n        'requests': {\n            'total': state_get('metrics:requests') or 0,\n            'status_200': state_get('metrics:status:200') or 0,\n            'status_404': state_get('metrics:status:404') or 0,\n        },\n        'items': {\n            'count': len([k for k in state_keys('item:') if ':' not in k[5:]]),\n        },\n        'websocket': {\n            'connections': state_get('ws:connections') or 0,\n        },\n        'ml_cache': cache_stats()\n    }\n\n# ============================================================\n# Health check\n# ============================================================\n\n@app.get(\"/health\")\nasync def health():\n    return {\n        'status': 'healthy',\n        'model_loaded': ml_model is not None\n    }\n```","ref":"fastapi-app.html#application-code"},{"type":"extras","title":"Running with Hornbeam - fastapi-app","doc":"```erlang\nrebar3 shell\n\nhornbeam:start(\"app:app\", #{\n    worker_class => asgi,\n    lifespan => on,\n    pythonpath => [\"fastapi_demo\"],\n    workers => 4\n}).\n```","ref":"fastapi-app.html#running-with-hornbeam"},{"type":"extras","title":"Testing - fastapi-app","doc":"```bash\n# Health check\ncurl http://localhost:8000/health\n\n# Create items\ncurl -X POST http://localhost:8000/items/ \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"name\": \"Laptop\", \"description\": \"High-performance laptop\", \"price\": 999.99}'\n\ncurl -X POST http://localhost:8000/items/ \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"name\": \"Phone\", \"description\": \"Smartphone with great camera\", \"price\": 699.99}'\n\n# List items\ncurl http://localhost:8000/items/\n\n# Generate embedding\ncurl -X POST http://localhost:8000/embed \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"text\": \"high quality electronics\"}'\n\n# Semantic search\ncurl -X POST http://localhost:8000/search \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"query\": \"computing device\", \"top_k\": 2}'\n\n# Stream events\ncurl http://localhost:8000/stream\n\n# Metrics\ncurl http://localhost:8000/metrics\n```","ref":"fastapi-app.html#testing"},{"type":"extras","title":"WebSocket Testing - fastapi-app","doc":"```python\nimport asyncio\nimport websockets\n\nasync def test_ws():\n    async with websockets.connect('ws://localhost:8000/ws') as ws:\n        await ws.send('{\"message\": \"Hello!\"}')\n        response = await ws.recv()\n        print(response)\n\nasyncio.run(test_ws())\n```","ref":"fastapi-app.html#websocket-testing"},{"type":"extras","title":"Requirements - fastapi-app","doc":"```\n# requirements.txt\nfastapi>=0.100\nuvicorn>=0.20\nsentence-transformers>=2.0\nnumpy>=1.20\n```","ref":"fastapi-app.html#requirements"},{"type":"extras","title":"Key Features Demonstrated - fastapi-app","doc":"1. **Lifespan events** for ML model loading\n2. **ETS caching** for embeddings\n3. **Semantic search** with cached vectors\n4. **WebSocket** with connection management\n5. **Server-Sent Events** streaming\n6. **Metrics** endpoint with cache stats\n7. **Pub/Sub** for item notifications","ref":"fastapi-app.html#key-features-demonstrated"},{"type":"extras","title":"Next Steps - fastapi-app","doc":"- [WebSocket Chat Example](./websocket-chat) - Full chat application\n- [Embedding Service Example](./embedding-service) - Production ML service","ref":"fastapi-app.html#next-steps"},{"type":"extras","title":"websocket-chat","doc":"---\ntitle: WebSocket Chat\ndescription: Real-time chat with WebSocket and Pub/Sub\norder: 22\n---\n\n# WebSocket Chat Example\n\nA real-time chat application using WebSocket, ETS for user state, and Erlang Pub/Sub for message broadcasting.","ref":"websocket-chat.html"},{"type":"extras","title":"Project Structure - websocket-chat","doc":"```\nchat_demo/\n app.py           # ASGI application\n static/\n    index.html   # Chat UI\n requirements.txt\n```","ref":"websocket-chat.html#project-structure"},{"type":"extras","title":"Server Code - websocket-chat","doc":"```python\n# app.py\nimport asyncio\nimport json\nimport uuid\nfrom datetime import datetime\nfrom hornbeam_erlang import (\n    state_get, state_set, state_delete, state_incr,\n    state_keys, publish, subscribe, unsubscribe\n)\n\nclass ChatRoom:\n    def __init__(self, room_id):\n        self.room_id = room_id\n        self.topic = f'chat:{room_id}'\n\n    def add_user(self, user_id, username):\n        state_set(f'room:{self.room_id}:user:{user_id}', {\n            'username': username,\n            'joined_at': datetime.now().isoformat()\n        })\n        state_incr(f'room:{self.room_id}:user_count')\n\n    def remove_user(self, user_id):\n        state_delete(f'room:{self.room_id}:user:{user_id}')\n        state_incr(f'room:{self.room_id}:user_count', -1)\n\n    def get_users(self):\n        keys = state_keys(f'room:{self.room_id}:user:')\n        users = []\n        for key in keys:\n            user = state_get(key)\n            if user:\n                users.append(user)\n        return users\n\n    def add_message(self, user_id, username, text):\n        msg_id = state_incr(f'room:{self.room_id}:msg_seq')\n        message = {\n            'id': msg_id,\n            'user_id': user_id,\n            'username': username,\n            'text': text,\n            'timestamp': datetime.now().isoformat()\n        }\n\n        # Store last 100 messages\n        state_set(f'room:{self.room_id}:msg:{msg_id}', message)\n\n        # Broadcast to all subscribers\n        publish(self.topic, {\n            'type': 'message',\n            'data': message\n        })\n\n        return message\n\n    def get_recent_messages(self, limit=50):\n        seq = state_get(f'room:{self.room_id}:msg_seq') or 0\n        messages = []\n\n        for i in range(max(1, seq - limit + 1), seq + 1):\n            msg = state_get(f'room:{self.room_id}:msg:{i}')\n            if msg:\n                messages.append(msg)\n\n        return messages\n\n\nasync def application(scope, receive, send):\n    if scope['type'] == 'http':\n        await handle_http(scope, receive, send)\n    elif scope['type'] == 'websocket':\n        await handle_websocket(scope, receive, send)\n\n\nasync def handle_http(scope, receive, send):\n    path = scope.get('path', '/')\n\n    if path == '/':\n        # Serve chat UI\n        await send({\n            'type': 'http.response.start',\n            'status': 200,\n            'headers': [[b'content-type', b'text/html']],\n        })\n        await send({\n            'type': 'http.response.body',\n            'body': get_chat_html().encode(),\n        })\n\n    elif path == '/api/rooms':\n        # List rooms\n        rooms = []\n        for key in state_keys('room:'):\n            if ':user_count' in key:\n                room_id = key.split(':')[1]\n                count = state_get(key) or 0\n                rooms.append({'id': room_id, 'users': count})\n\n        await send({\n            'type': 'http.response.start',\n            'status': 200,\n            'headers': [[b'content-type', b'application/json']],\n        })\n        await send({\n            'type': 'http.response.body',\n            'body': json.dumps(rooms).encode(),\n        })\n\n    else:\n        await send({\n            'type': 'http.response.start',\n            'status': 404,\n            'headers': [[b'content-type', b'text/plain']],\n        })\n        await send({\n            'type': 'http.response.body',\n            'body': b'Not Found',\n        })\n\n\nasync def handle_websocket(scope, receive, send):\n    # Parse room from path: /ws/room-name\n    path = scope.get('path', '/ws/general')\n    room_id = path.split('/')[-1] or 'general'\n\n    room = ChatRoom(room_id)\n    user_id = str(uuid.uuid4())\n    username = None\n\n    # Accept connection\n    await send({'type': 'websocket.accept'})\n\n    # Subscribe to room topic\n    subscribe(room.topic)\n\n    try:\n        while True:\n            message = await receive()\n\n            if message['type'] == 'websocket.disconnect':\n                break\n\n            if message['type'] == 'websocket.receive':\n                data = json.loads(message.get('text', '{}'))\n                msg_type = data.get('type')\n\n                if msg_type == 'join':\n                    username = data.get('username', f'user_{user_id[:8]}')\n                    room.add_user(user_id, username)\n\n                    # Send join confirmation\n                    await send({\n                        'type': 'websocket.send',\n                        'text': json.dumps({\n                            'type': 'joined',\n                            'user_id': user_id,\n                            'username': username,\n                            'room': room_id,\n                            'users': room.get_users(),\n                            'recent_messages': room.get_recent_messages()\n                        })\n                    })\n\n                    # Broadcast user joined\n                    publish(room.topic, {\n                        'type': 'user_joined',\n                        'data': {'user_id': user_id, 'username': username}\n                    })\n\n                elif msg_type == 'message' and username:\n                    text = data.get('text', '').strip()\n                    if text:\n                        room.add_message(user_id, username, text)\n\n                elif msg_type == 'typing' and username:\n                    publish(room.topic, {\n                        'type': 'typing',\n                        'data': {'user_id': user_id, 'username': username}\n                    })\n\n            # Check for pub/sub messages\n            # In real implementation, use async pub/sub receiver\n            # This is simplified for demonstration\n\n    finally:\n        if username:\n            room.remove_user(user_id)\n            publish(room.topic, {\n                'type': 'user_left',\n                'data': {'user_id': user_id, 'username': username}\n            })\n        unsubscribe(room.topic)\n\n\ndef get_chat_html():\n    return '''<!DOCTYPE html>\n \n \n     Hornbeam Chat \n     \n        * { box-sizing: border-box; margin: 0; padding: 0; }\n        body { font-family: system-ui; background: #1a1a2e; color: #eee; }\n        .container { max-width: 800px; margin: 0 auto; height: 100vh; display: flex; flex-direction: column; }\n        .header { padding: 1rem; background: #16213e; border-bottom: 1px solid #0f3460; }\n        .header h1 { font-size: 1.5rem; color: #4a7c50; }\n        .messages { flex: 1; overflow-y: auto; padding: 1rem; }\n        .message { margin-bottom: 0.5rem; padding: 0.5rem; background: #16213e; border-radius: 8px; }\n        .message .username { color: #4a7c50; font-weight: bold; }\n        .message .time { color: #666; font-size: 0.8rem; }\n        .message .text { margin-top: 0.25rem; }\n        .input-area { padding: 1rem; background: #16213e; border-top: 1px solid #0f3460; }\n        .input-area form { display: flex; gap: 0.5rem; }\n        .input-area input { flex: 1; padding: 0.75rem; border: none; border-radius: 8px; background: #1a1a2e; color: #eee; }\n        .input-area button { padding: 0.75rem 1.5rem; border: none; border-radius: 8px; background: #4a7c50; color: #fff; cursor: pointer; }\n        .input-area button:hover { background: #3d6a43; }\n        .join-form { padding: 2rem; text-align: center; }\n        .join-form input { width: 200px; margin-right: 0.5rem; }\n        .users { padding: 0.5rem 1rem; background: #0f3460; font-size: 0.9rem; }\n     \n \n \n     \n         \n             Hornbeam Chat \n         \n         Users: - \n          \n         \n             \n                 \n                 Join Chat \n             \n             \n                 \n                 Send \n             \n         \n     \n     \n        let ws;\n        const room = 'general';\n\n        function connect() {\n            ws = new WebSocket(`ws://${location.host}/ws/${room}`);\n\n            ws.onmessage = (event) => {\n                const data = JSON.parse(event.data);\n                handleMessage(data);\n            };\n\n            ws.onclose = () => {\n                setTimeout(connect, 1000);\n            };\n        }\n\n        function join() {\n            const username = document.getElementById('username').value.trim();\n            if (!username) return;\n\n            ws.send(JSON.stringify({ type: 'join', username }));\n            document.getElementById('join-form').style.display = 'none';\n            document.getElementById('message-form').style.display = 'flex';\n        }\n\n        function sendMessage(e) {\n            e.preventDefault();\n            const input = document.getElementById('message');\n            const text = input.value.trim();\n            if (!text) return;\n\n            ws.send(JSON.stringify({ type: 'message', text }));\n            input.value = '';\n        }\n\n        function handleMessage(data) {\n            if (data.type === 'joined') {\n                updateUsers(data.users);\n                data.recent_messages.forEach(addMessage);\n            } else if (data.type === 'message') {\n                addMessage(data.data);\n            } else if (data.type === 'user_joined') {\n                addSystemMessage(`${data.data.username} joined`);\n            } else if (data.type === 'user_left') {\n                addSystemMessage(`${data.data.username} left`);\n            }\n        }\n\n        function addMessage(msg) {\n            const div = document.createElement('div');\n            div.className = 'message';\n            div.innerHTML = `\n                 ${msg.username} \n                 ${new Date(msg.timestamp).toLocaleTimeString()} \n                 ${escapeHtml(msg.text)} \n            `;\n            document.getElementById('messages').appendChild(div);\n            div.scrollIntoView();\n        }\n\n        function addSystemMessage(text) {\n            const div = document.createElement('div');\n            div.className = 'message';\n            div.innerHTML = ` ${text} `;\n            document.getElementById('messages').appendChild(div);\n        }\n\n        function updateUsers(users) {\n            document.getElementById('users').textContent = `Users: ${users.map(u => u.username).join(', ')}`;\n        }\n\n        function escapeHtml(text) {\n            const div = document.createElement('div');\n            div.textContent = text;\n            return div.innerHTML;\n        }\n\n        connect();\n     \n \n '''\n```","ref":"websocket-chat.html#server-code"},{"type":"extras","title":"Running - websocket-chat","doc":"```erlang\nrebar3 shell\n\nhornbeam:start(\"app:application\", #{\n    worker_class => asgi,\n    pythonpath => [\"chat_demo\"],\n    workers => 4,\n    websocket_timeout => 300000  % 5 min timeout\n}).\n```\n\nOpen http://localhost:8000 in multiple browsers to test.","ref":"websocket-chat.html#running"},{"type":"extras","title":"Features - websocket-chat","doc":"1. **Multiple rooms** - Join different rooms via URL path\n2. **User presence** - Track who's online\n3. **Message history** - Load recent messages on join\n4. **ETS storage** - Messages and users in ETS\n5. **Pub/Sub** - Real-time message broadcasting\n6. **Typing indicators** - Show who's typing","ref":"websocket-chat.html#features"},{"type":"extras","title":"Next Steps - websocket-chat","doc":"- [Embedding Service](./embedding-service) - ML with ETS caching\n- [WebSocket Guide](../guides/websocket) - WebSocket details","ref":"websocket-chat.html#next-steps"},{"type":"extras","title":"embedding-service","doc":"---\ntitle: Embedding Service\ndescription: Production ML embedding service with ETS caching\norder: 23\n---\n\n# Embedding Service Example\n\nA production-ready embedding service using sentence-transformers with ETS-backed caching for high-throughput semantic search.","ref":"embedding-service.html"},{"type":"extras","title":"Project Structure - embedding-service","doc":"```\nembedding_service/\n app.py              # FastAPI application\n embeddings.py       # Embedding logic\n search.py           # Search implementation\n config.py           # Configuration\n requirements.txt\n```","ref":"embedding-service.html#project-structure"},{"type":"extras","title":"Application Code - embedding-service","doc":"```python\n# app.py\nfrom contextlib import asynccontextmanager\nfrom fastapi import FastAPI, HTTPException\nfrom pydantic import BaseModel\nfrom hornbeam_erlang import state_get, state_set, state_incr\nfrom hornbeam_ml import cached_inference, cache_stats\nimport numpy as np\nfrom typing import Optional\n\n# ============================================================\n# Configuration\n# ============================================================\n\nclass Config:\n    MODEL_NAME = 'all-MiniLM-L6-v2'\n    CACHE_PREFIX = 'emb'\n    MAX_BATCH_SIZE = 100\n    SIMILARITY_THRESHOLD = 0.5\n\nconfig = Config()\n\n# ============================================================\n# Models\n# ============================================================\n\nclass EmbedRequest(BaseModel):\n    texts: list[str]\n    cache: bool = True\n\nclass EmbedResponse(BaseModel):\n    embeddings: list[list[float]]\n    dimensions: int\n    cached_count: int\n\nclass IndexRequest(BaseModel):\n    id: str\n    text: str\n    metadata: Optional[dict] = None\n\nclass SearchRequest(BaseModel):\n    query: str\n    top_k: int = 10\n    threshold: float = 0.5\n\nclass SearchResult(BaseModel):\n    id: str\n    text: str\n    score: float\n    metadata: Optional[dict]\n\n# ============================================================\n# ML Model\n# ============================================================\n\nmodel = None\n\n@asynccontextmanager\nasync def lifespan(app):\n    global model\n    print(f\"Loading model: {config.MODEL_NAME}\")\n    from sentence_transformers import SentenceTransformer\n    model = SentenceTransformer(config.MODEL_NAME)\n    print(f\"Model loaded. Dimensions: {model.get_sentence_embedding_dimension()}\")\n\n    # Warm up cache\n    _ = model.encode(\"warmup\")\n\n    yield\n\n    model = None\n\napp = FastAPI(\n    title=\"Embedding Service\",\n    description=\"High-performance embedding service with ETS caching\",\n    lifespan=lifespan\n)\n\n# ============================================================\n# Embedding Endpoints\n# ============================================================\n\n@app.post(\"/embed\", response_model=EmbedResponse)\nasync def embed(request: EmbedRequest):\n    \"\"\"Generate embeddings for texts.\"\"\"\n    if len(request.texts) > config.MAX_BATCH_SIZE:\n        raise HTTPException(\n            status_code=400,\n            detail=f\"Maximum batch size is {config.MAX_BATCH_SIZE}\"\n        )\n\n    state_incr('metrics:embed_requests')\n    state_incr('metrics:embed_texts', len(request.texts))\n\n    embeddings = []\n    cached_count = 0\n\n    for text in request.texts:\n        if request.cache:\n            # Use cached inference\n            emb = cached_inference(\n                model.encode,\n                text,\n                cache_prefix=config.CACHE_PREFIX\n            )\n            # Check if it was a cache hit\n            if state_get(f'_cache_hit'):\n                cached_count += 1\n        else:\n            emb = model.encode(text)\n\n        embeddings.append(emb.tolist())\n\n    return EmbedResponse(\n        embeddings=embeddings,\n        dimensions=len(embeddings[0]) if embeddings else 0,\n        cached_count=cached_count\n    )\n\n@app.post(\"/embed/batch\")\nasync def embed_batch(request: EmbedRequest):\n    \"\"\"Batch embed for better throughput (less caching).\"\"\"\n    if len(request.texts) > config.MAX_BATCH_SIZE:\n        raise HTTPException(status_code=400, detail=\"Batch too large\")\n\n    state_incr('metrics:batch_requests')\n\n    # Batch encode is more efficient but doesn't use per-item cache\n    embeddings = model.encode(request.texts)\n\n    return {\n        'embeddings': embeddings.tolist(),\n        'dimensions': embeddings.shape[1]\n    }\n\n# ============================================================\n# Index & Search\n# ============================================================\n\n@app.post(\"/index\")\nasync def index_document(request: IndexRequest):\n    \"\"\"Index a document for search.\"\"\"\n    state_incr('metrics:index_requests')\n\n    # Generate embedding\n    embedding = cached_inference(model.encode, request.text)\n\n    # Store document\n    doc = {\n        'id': request.id,\n        'text': request.text,\n        'embedding': embedding.tolist(),\n        'metadata': request.metadata or {}\n    }\n    state_set(f'doc:{request.id}', doc)\n\n    # Update index stats\n    state_incr('index:doc_count')\n\n    return {'indexed': request.id}\n\n@app.delete(\"/index/{doc_id}\")\nasync def delete_document(doc_id: str):\n    \"\"\"Remove document from index.\"\"\"\n    from hornbeam_erlang import state_delete\n\n    if not state_get(f'doc:{doc_id}'):\n        raise HTTPException(status_code=404, detail=\"Document not found\")\n\n    state_delete(f'doc:{doc_id}')\n    state_incr('index:doc_count', -1)\n\n    return {'deleted': doc_id}\n\n@app.post(\"/search\", response_model=list[SearchResult])\nasync def search(request: SearchRequest):\n    \"\"\"Semantic search across indexed documents.\"\"\"\n    state_incr('metrics:search_requests')\n\n    # Get query embedding\n    query_emb = cached_inference(model.encode, request.query)\n    query_emb = np.array(query_emb)\n\n    # Search all documents\n    from hornbeam_erlang import state_keys\n\n    results = []\n    for key in state_keys('doc:'):\n        doc = state_get(key)\n        if not doc:\n            continue\n\n        doc_emb = np.array(doc['embedding'])\n\n        # Cosine similarity\n        similarity = np.dot(query_emb, doc_emb) / (\n            np.linalg.norm(query_emb) * np.linalg.norm(doc_emb)\n        )\n\n        if similarity >= request.threshold:\n            results.append(SearchResult(\n                id=doc['id'],\n                text=doc['text'],\n                score=float(similarity),\n                metadata=doc.get('metadata')\n            ))\n\n    # Sort by score\n    results.sort(key=lambda x: x.score, reverse=True)\n\n    return results[:request.top_k]\n\n# ============================================================\n# Stats & Health\n# ============================================================\n\n@app.get(\"/stats\")\nasync def get_stats():\n    \"\"\"Get service statistics.\"\"\"\n    ml_cache = cache_stats()\n\n    return {\n        'model': config.MODEL_NAME,\n        'index': {\n            'documents': state_get('index:doc_count') or 0\n        },\n        'requests': {\n            'embed': state_get('metrics:embed_requests') or 0,\n            'batch': state_get('metrics:batch_requests') or 0,\n            'search': state_get('metrics:search_requests') or 0,\n            'index': state_get('metrics:index_requests') or 0\n        },\n        'cache': {\n            'hits': ml_cache.get('hits', 0),\n            'misses': ml_cache.get('misses', 0),\n            'hit_rate': ml_cache.get('hit_rate', 0)\n        }\n    }\n\n@app.get(\"/health\")\nasync def health():\n    return {\n        'status': 'healthy',\n        'model_loaded': model is not None\n    }\n```","ref":"embedding-service.html#application-code"},{"type":"extras","title":"Running - embedding-service","doc":"```erlang\nrebar3 shell\n\nhornbeam:start(\"app:app\", #{\n    worker_class => asgi,\n    lifespan => on,\n    pythonpath => [\"embedding_service\"],\n    workers => 4,\n    timeout => 60000\n}).\n```","ref":"embedding-service.html#running"},{"type":"extras","title":"Usage - embedding-service","doc":"```bash\n# Index documents\ncurl -X POST http://localhost:8000/index \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"id\": \"1\", \"text\": \"Python is a programming language\", \"metadata\": {\"category\": \"tech\"}}'\n\ncurl -X POST http://localhost:8000/index \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"id\": \"2\", \"text\": \"Erlang is great for concurrent systems\"}'\n\ncurl -X POST http://localhost:8000/index \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"id\": \"3\", \"text\": \"Machine learning uses neural networks\"}'\n\n# Search\ncurl -X POST http://localhost:8000/search \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"query\": \"programming languages\", \"top_k\": 5}'\n\n# Direct embedding\ncurl -X POST http://localhost:8000/embed \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"texts\": [\"Hello world\", \"How are you?\"]}'\n\n# Check stats\ncurl http://localhost:8000/stats\n```","ref":"embedding-service.html#usage"},{"type":"extras","title":"Requirements - embedding-service","doc":"```\n# requirements.txt\nfastapi>=0.100\nsentence-transformers>=2.0\nnumpy>=1.20\n```","ref":"embedding-service.html#requirements"},{"type":"extras","title":"Performance Tips - embedding-service","doc":"1. **Use batch endpoints** for bulk operations\n2. **Enable caching** for repeated queries\n3. **Increase workers** for more parallelism\n4. **Use GPU** for faster inference (install torch with CUDA)","ref":"embedding-service.html#performance-tips"},{"type":"extras","title":"Next Steps - embedding-service","doc":"- [Distributed ML](./distributed-ml) - Scale across cluster\n- [ML Integration Guide](../guides/ml-integration) - More patterns","ref":"embedding-service.html#next-steps"},{"type":"extras","title":"distributed-ml","doc":"---\ntitle: Distributed ML\ndescription: Distribute ML inference across an Erlang cluster\norder: 24\n---\n\n# Distributed ML Example\n\nThis example shows how to distribute ML inference across multiple Erlang nodes, leveraging the BEAM's built-in clustering.","ref":"distributed-ml.html"},{"type":"extras","title":"Architecture - distributed-ml","doc":"```\n                    \n                       Web Server    \n                      (Hornbeam)     \n                    \n                             \n              \n                                          \n         \n         GPU Node     GPU Node     GPU Node  \n         worker1@     worker2@     worker3@  \n         \n```","ref":"distributed-ml.html#architecture"},{"type":"extras","title":"Web Server (Hornbeam) - distributed-ml","doc":"```python\n# app.py\nfrom fastapi import FastAPI\nfrom pydantic import BaseModel\nfrom hornbeam_erlang import rpc_call, nodes, node\nfrom hornbeam_ml import cached_inference\nimport asyncio\nfrom typing import Optional\n\napp = FastAPI()\n\n# ============================================================\n# Models\n# ============================================================\n\nclass InferenceRequest(BaseModel):\n    texts: list[str]\n    model: str = \"default\"\n\nclass InferenceResponse(BaseModel):\n    embeddings: list[list[float]]\n    node: str\n    cached: int\n\n# ============================================================\n# Distributed Inference\n# ============================================================\n\ndef get_gpu_nodes():\n    \"\"\"Get available GPU worker nodes.\"\"\"\n    return [n for n in nodes() if n.startswith('gpu') or n.startswith('worker')]\n\ndef select_node(nodes_list):\n    \"\"\"Select node with least load (round-robin for simplicity).\"\"\"\n    if not nodes_list:\n        return None\n    # Simple round-robin\n    from hornbeam_erlang import state_incr\n    idx = state_incr('node_selector') % len(nodes_list)\n    return nodes_list[idx]\n\n@app.post(\"/infer\", response_model=InferenceResponse)\nasync def distributed_inference(request: InferenceRequest):\n    \"\"\"Run inference on a GPU node.\"\"\"\n    gpu_nodes = get_gpu_nodes()\n\n    if not gpu_nodes:\n        # Fallback to local inference\n        from sentence_transformers import SentenceTransformer\n        model = SentenceTransformer('all-MiniLM-L6-v2')\n        embeddings = model.encode(request.texts)\n        return InferenceResponse(\n            embeddings=embeddings.tolist(),\n            node=node(),\n            cached=0\n        )\n\n    # Select a GPU node\n    target_node = select_node(gpu_nodes)\n\n    # Call remote node\n    result = rpc_call(\n        target_node,\n        'ml_worker',\n        'encode',\n        [request.texts, request.model],\n        timeout_ms=60000\n    )\n\n    return InferenceResponse(\n        embeddings=result['embeddings'],\n        node=target_node,\n        cached=result.get('cached', 0)\n    )\n\n@app.post(\"/infer/parallel\")\nasync def parallel_inference(request: InferenceRequest):\n    \"\"\"Distribute across all GPU nodes in parallel.\"\"\"\n    gpu_nodes = get_gpu_nodes()\n\n    if not gpu_nodes:\n        raise HTTPException(status_code=503, detail=\"No GPU nodes available\")\n\n    # Split texts across nodes\n    n_nodes = len(gpu_nodes)\n    chunk_size = (len(request.texts) + n_nodes - 1) // n_nodes\n    chunks = [\n        request.texts[i:i + chunk_size]\n        for i in range(0, len(request.texts), chunk_size)\n    ]\n\n    # Call all nodes in parallel\n    import concurrent.futures\n\n    def call_node(node, texts):\n        return rpc_call(\n            node,\n            'ml_worker',\n            'encode',\n            [texts, request.model],\n            timeout_ms=60000\n        )\n\n    results = []\n    with concurrent.futures.ThreadPoolExecutor(max_workers=n_nodes) as executor:\n        futures = {\n            executor.submit(call_node, node, chunk): node\n            for node, chunk in zip(gpu_nodes, chunks)\n            if chunk\n        }\n\n        for future in concurrent.futures.as_completed(futures):\n            result = future.result()\n            results.extend(result['embeddings'])\n\n    return {\n        'embeddings': results,\n        'nodes_used': len(gpu_nodes)\n    }\n\n# ============================================================\n# Cluster Status\n# ============================================================\n\n@app.get(\"/cluster\")\nasync def cluster_status():\n    \"\"\"Get cluster status.\"\"\"\n    gpu_nodes = get_gpu_nodes()\n\n    status = {\n        'this_node': node(),\n        'gpu_nodes': [],\n        'total_nodes': len(nodes())\n    }\n\n    for gpu_node in gpu_nodes:\n        try:\n            info = rpc_call(gpu_node, 'ml_worker', 'info', [], timeout_ms=5000)\n            status['gpu_nodes'].append({\n                'node': gpu_node,\n                'status': 'online',\n                **info\n            })\n        except Exception as e:\n            status['gpu_nodes'].append({\n                'node': gpu_node,\n                'status': 'offline',\n                'error': str(e)\n            })\n\n    return status\n```","ref":"distributed-ml.html#web-server-hornbeam"},{"type":"extras","title":"GPU Worker Node - distributed-ml","doc":"","ref":"distributed-ml.html#gpu-worker-node"},{"type":"extras","title":"Erlang Module - distributed-ml","doc":"```erlang\n%% ml_worker.erl\n-module(ml_worker).\n-export([encode/2, info/0, predict/2]).\n\nencode(Texts, Model) ->\n    %% Call Python with caching\n    Result = py:call('ml_service', 'encode', [Texts, Model]),\n    Result.\n\npredict(Input, Model) ->\n    py:call('ml_service', 'predict', [Input, Model]).\n\ninfo() ->\n    #{\n        model => py:call('ml_service', 'get_model_info', []),\n        memory => py:memory_stats(),\n        cache => py:call('ml_service', 'cache_stats', [])\n    }.\n```","ref":"distributed-ml.html#erlang-module"},{"type":"extras","title":"Python Service - distributed-ml","doc":"```python\n# ml_service.py\nfrom sentence_transformers import SentenceTransformer\nfrom hornbeam_ml import cached_inference, cache_stats as get_cache_stats\nfrom hornbeam_erlang import state_get, state_set\n\n# Load model at import\nmodels = {}\n\ndef get_model(name='default'):\n    if name not in models:\n        model_map = {\n            'default': 'all-MiniLM-L6-v2',\n            'large': 'all-mpnet-base-v2',\n            'multilingual': 'paraphrase-multilingual-MiniLM-L12-v2'\n        }\n        model_name = model_map.get(name, name)\n        models[name] = SentenceTransformer(model_name)\n    return models[name]\n\ndef encode(texts, model_name='default'):\n    \"\"\"Encode texts with caching.\"\"\"\n    model = get_model(model_name)\n\n    embeddings = []\n    cached_count = 0\n\n    for text in texts:\n        cache_key = f'{model_name}:{hash(text)}'\n        cached = state_get(f'emb:{cache_key}')\n\n        if cached:\n            embeddings.append(cached)\n            cached_count += 1\n        else:\n            emb = model.encode(text).tolist()\n            state_set(f'emb:{cache_key}', emb)\n            embeddings.append(emb)\n\n    return {\n        'embeddings': embeddings,\n        'cached': cached_count\n    }\n\ndef predict(input_data, model_name='default'):\n    \"\"\"Run prediction.\"\"\"\n    model = get_model(model_name)\n    return model.encode(input_data).tolist()\n\ndef get_model_info():\n    \"\"\"Get loaded model info.\"\"\"\n    return {\n        'loaded_models': list(models.keys()),\n        'default_dimensions': get_model().get_sentence_embedding_dimension()\n    }\n\ndef cache_stats():\n    return get_cache_stats()\n```","ref":"distributed-ml.html#python-service"},{"type":"extras","title":"Starting the Cluster - distributed-ml","doc":"","ref":"distributed-ml.html#starting-the-cluster"},{"type":"extras","title":"Start GPU Worker Nodes - distributed-ml","doc":"```bash\n# On gpu-server-1\nerl -name worker1@gpu-server-1 -setcookie mysecret\n\n# In the shell\napplication:ensure_all_started(erlang_python).\n```","ref":"distributed-ml.html#start-gpu-worker-nodes"},{"type":"extras","title":"Start Web Server - distributed-ml","doc":"```bash\n# On web-server\nerl -name web@web-server -setcookie mysecret\n\n# Connect to GPU nodes\nnet_adm:ping('worker1@gpu-server-1').\nnet_adm:ping('worker2@gpu-server-2').\n\n# Start Hornbeam\nhornbeam:start(\"app:app\", #{\n    worker_class => asgi,\n    pythonpath => [\"distributed_ml\"]\n}).\n```","ref":"distributed-ml.html#start-web-server"},{"type":"extras","title":"Load Balancing Strategies - distributed-ml","doc":"","ref":"distributed-ml.html#load-balancing-strategies"},{"type":"extras","title":"1. Round-Robin (Simple) - distributed-ml","doc":"```python\ndef select_node_round_robin(nodes_list):\n    idx = state_incr('node_rr') % len(nodes_list)\n    return nodes_list[idx]\n```","ref":"distributed-ml.html#1-round-robin-simple"},{"type":"extras","title":"2. Least Loaded - distributed-ml","doc":"```python\ndef select_node_least_loaded(nodes_list):\n    loads = []\n    for n in nodes_list:\n        try:\n            load = rpc_call(n, 'ml_worker', 'get_load', [], timeout_ms=1000)\n            loads.append((n, load))\n        except:\n            loads.append((n, float('inf')))\n\n    return min(loads, key=lambda x: x[1])[0]\n```","ref":"distributed-ml.html#2-least-loaded"},{"type":"extras","title":"3. Consistent Hashing (for caching) - distributed-ml","doc":"```python\nimport hashlib\n\ndef select_node_consistent(nodes_list, key):\n    \"\"\"Select node based on key hash for cache locality.\"\"\"\n    hash_val = int(hashlib.md5(key.encode()).hexdigest(), 16)\n    idx = hash_val % len(nodes_list)\n    return sorted(nodes_list)[idx]\n```","ref":"distributed-ml.html#3-consistent-hashing-for-caching"},{"type":"extras","title":"Fault Tolerance - distributed-ml","doc":"```python\nasync def resilient_inference(texts, retries=2):\n    \"\"\"Inference with automatic failover.\"\"\"\n    gpu_nodes = get_gpu_nodes()\n\n    for attempt in range(retries + 1):\n        if not gpu_nodes:\n            break\n\n        target = select_node(gpu_nodes)\n\n        try:\n            result = rpc_call(\n                target,\n                'ml_worker',\n                'encode',\n                [texts, 'default'],\n                timeout_ms=30000\n            )\n            return result\n        except Exception as e:\n            print(f\"Node {target} failed: {e}\")\n            gpu_nodes.remove(target)\n\n    # All nodes failed, try local\n    return local_inference(texts)\n```","ref":"distributed-ml.html#fault-tolerance"},{"type":"extras","title":"Monitoring - distributed-ml","doc":"```python\n@app.get(\"/metrics\")\nasync def metrics():\n    \"\"\"Cluster-wide metrics.\"\"\"\n    from hornbeam_erlang import state_get\n\n    return {\n        'requests': {\n            'total': state_get('metrics:requests') or 0,\n            'distributed': state_get('metrics:distributed') or 0,\n            'local_fallback': state_get('metrics:local') or 0\n        },\n        'cluster': {\n            'nodes': len(get_gpu_nodes()),\n            'node_selector_count': state_get('node_selector') or 0\n        }\n    }\n```","ref":"distributed-ml.html#monitoring"},{"type":"extras","title":"Next Steps - distributed-ml","doc":"- [ML Integration Guide](../guides/ml-integration) - Caching patterns\n- [Erlang Integration Guide](../guides/erlang-integration) - RPC details","ref":"distributed-ml.html#next-steps"},{"type":"extras","title":"configuration","doc":"---\ntitle: Configuration Reference\ndescription: All Hornbeam configuration options\norder: 30\n---\n\n# Configuration Reference\n\nThis document covers all Hornbeam configuration options.","ref":"configuration.html"},{"type":"extras","title":"Quick Start - configuration","doc":"```erlang\nhornbeam:start(\"myapp:application\", #{\n    bind => \"0.0.0.0:8000\",\n    workers => 4,\n    worker_class => asgi\n}).\n```","ref":"configuration.html#quick-start"},{"type":"extras","title":"Server Options - configuration","doc":"| Option | Type | Default | Description |\n|--------|------|---------|-------------|\n| `bind` | binary/string | `\"127.0.0.1:8000\"` | Address and port to bind to |\n| `ssl` | boolean | `false` | Enable SSL/TLS |\n| `certfile` | binary | `undefined` | Path to SSL certificate |\n| `keyfile` | binary | `undefined` | Path to SSL private key |\n| `cacertfile` | binary | `undefined` | Path to CA certificate |","ref":"configuration.html#server-options"},{"type":"extras","title":"SSL Example - configuration","doc":"```erlang\nhornbeam:start(\"app:application\", #{\n    bind => \"0.0.0.0:443\",\n    ssl => true,\n    certfile => \"/path/to/cert.pem\",\n    keyfile => \"/path/to/key.pem\"\n}).\n```","ref":"configuration.html#ssl-example"},{"type":"extras","title":"Protocol Options - configuration","doc":"| Option | Type | Default | Description |\n|--------|------|---------|-------------|\n| `worker_class` | atom | `wsgi` | Protocol: `wsgi` or `asgi` |\n| `http_version` | list | `['HTTP/1.1', 'HTTP/2']` | Supported HTTP versions |","ref":"configuration.html#protocol-options"},{"type":"extras","title":"Worker Options - configuration","doc":"| Option | Type | Default | Description |\n|--------|------|---------|-------------|\n| `workers` | integer | `4` | Number of Python workers |\n| `timeout` | integer | `30000` | Request timeout in milliseconds |\n| `keepalive` | integer | `2` | HTTP keep-alive timeout in seconds |\n| `max_requests` | integer | `1000` | Recycle worker after N requests |\n| `preload_app` | boolean | `false` | Load app before forking workers |","ref":"configuration.html#worker-options"},{"type":"extras","title":"Worker Sizing - configuration","doc":"```erlang\n%% CPU-bound (ML inference)\nhornbeam:start(\"app:app\", #{\n    workers => erlang:system_info(schedulers)  % One per CPU\n}).\n\n%% I/O-bound (web app with DB calls)\nhornbeam:start(\"app:app\", #{\n    workers => erlang:system_info(schedulers) * 2\n}).\n```","ref":"configuration.html#worker-sizing"},{"type":"extras","title":"ASGI Options - configuration","doc":"| Option | Type | Default | Description |\n|--------|------|---------|-------------|\n| `lifespan` | atom | `auto` | Lifespan handling: `auto`, `on`, `off` |\n| `root_path` | binary | `\"\"` | ASGI root_path for mounted apps |","ref":"configuration.html#asgi-options"},{"type":"extras","title":"Lifespan Values - configuration","doc":"- `auto` - Detect if app supports lifespan, use if available\n- `on` - Require lifespan, fail if app doesn't support it\n- `off` - Disable lifespan even if app supports it","ref":"configuration.html#lifespan-values"},{"type":"extras","title":"WebSocket Options - configuration","doc":"| Option | Type | Default | Description |\n|--------|------|---------|-------------|\n| `websocket_timeout` | integer | `60000` | WebSocket idle timeout (ms) |\n| `websocket_max_frame_size` | integer | `16777216` | Max frame size (16MB) |\n| `websocket_compress` | boolean | `false` | Enable WebSocket compression |","ref":"configuration.html#websocket-options"},{"type":"extras","title":"Python Options - configuration","doc":"| Option | Type | Default | Description |\n|--------|------|---------|-------------|\n| `pythonpath` | list | `[\".\", \"examples\"]` | Python module search paths |\n| `python_home` | binary | `undefined` | Python home directory |\n| `venv` | binary | `undefined` | Virtual environment path |","ref":"configuration.html#python-options"},{"type":"extras","title":"Virtual Environment - configuration","doc":"```erlang\nhornbeam:start(\"app:application\", #{\n    venv => \"/path/to/myproject/venv\",\n    pythonpath => [\"/path/to/myproject\"]\n}).\n```","ref":"configuration.html#virtual-environment"},{"type":"extras","title":"Hooks - configuration","doc":"Hooks allow you to execute Erlang code at key points in the request lifecycle.\n\n| Hook | Arguments | Return | Description |\n|------|-----------|--------|-------------|\n| `on_request` | `Request` | `Request` | Called before handling request |\n| `on_response` | `Response` | `Response` | Called before sending response |\n| `on_error` | `Error, Request` | `{Code, Body}` | Called on error |\n| `on_worker_start` | `WorkerId` | `ok` | Called when worker starts |\n| `on_worker_exit` | `WorkerId, Reason` | `ok` | Called when worker exits |","ref":"configuration.html#hooks"},{"type":"extras","title":"Hook Examples - configuration","doc":"```erlang\nhornbeam:start(\"app:application\", #{\n    hooks => #{\n        %% Request hook: logging, authentication, rate limiting\n        on_request => fun(Request) ->\n            #{method := Method, path := Path} = Request,\n            logger:info(\"~s ~s\", [Method, Path]),\n\n            %% Add custom header\n            Headers = maps:get(headers, Request, #{}),\n            Request#{headers => Headers#{<<\"x-request-id\">> => generate_id()}}\n        end,\n\n        %% Response hook: add headers, log response\n        on_response => fun(Response) ->\n            #{status := Status} = Response,\n            metrics:incr(<<\"response_\", (integer_to_binary(Status))/binary>>),\n\n            %% Add server header\n            Headers = maps:get(headers, Response, #{}),\n            Response#{headers => Headers#{<<\"x-powered-by\">> => <<\"Hornbeam\">>}}\n        end,\n\n        %% Error hook: custom error handling\n        on_error => fun(Error, Request) ->\n            #{path := Path} = Request,\n            logger:error(\"Error on ~s: ~p\", [Path, Error]),\n\n            case Error of\n                {timeout, _} ->\n                    {504, <<\"Gateway Timeout\">>};\n                {python_error, Reason} ->\n                    {500, iolist_to_binary(io_lib:format(\"~p\", [Reason]))};\n                _ ->\n                    {500, <<\"Internal Server Error\">>}\n            end\n        end,\n\n        %% Worker lifecycle hooks\n        on_worker_start => fun(WorkerId) ->\n            logger:info(\"Worker ~p started\", [WorkerId]),\n            ok\n        end,\n\n        on_worker_exit => fun(WorkerId, Reason) ->\n            logger:warning(\"Worker ~p exited: ~p\", [WorkerId, Reason]),\n            ok\n        end\n    }\n}).\n```","ref":"configuration.html#hook-examples"},{"type":"extras","title":"Authentication Hook - configuration","doc":"```erlang\nhornbeam:start(\"app:application\", #{\n    hooks => #{\n        on_request => fun(Request) ->\n            #{headers := Headers, path := Path} = Request,\n\n            %% Skip auth for public paths\n            case Path of\n                <<\"/health\">> -> Request;\n                <<\"/public/\", _/binary>> -> Request;\n                _ ->\n                    case maps:get(<<\"authorization\">>, Headers, undefined) of\n                        undefined ->\n                            throw({unauthorized, <<\"Missing authorization header\">>});\n                        Token ->\n                            case auth:verify_token(Token) of\n                                {ok, UserId} ->\n                                    Request#{user_id => UserId};\n                                {error, _} ->\n                                    throw({unauthorized, <<\"Invalid token\">>})\n                            end\n                    end\n            end\n        end,\n\n        on_error => fun(Error, _Request) ->\n            case Error of\n                {unauthorized, Message} ->\n                    {401, Message};\n                _ ->\n                    {500, <<\"Internal Server Error\">>}\n            end\n        end\n    }\n}).\n```","ref":"configuration.html#authentication-hook"},{"type":"extras","title":"Rate Limiting Hook - configuration","doc":"```erlang\nhornbeam:start(\"app:application\", #{\n    hooks => #{\n        on_request => fun(Request) ->\n            #{headers := Headers} = Request,\n            ClientIP = maps:get(<<\"x-forwarded-for\">>, Headers,\n                                maps:get(<<\"x-real-ip\">>, Headers, <<\"unknown\">>)),\n\n            %% Rate limit key\n            Minute = calendar:datetime_to_gregorian_seconds(calendar:universal_time()) div 60,\n            Key = {rate_limit, ClientIP, Minute},\n\n            %% Check and increment\n            Count = ets:update_counter(rate_limits, Key, 1, {Key, 0}),\n\n            if\n                Count > 100 ->\n                    throw({rate_limited, <<\"Too many requests\">>});\n                true ->\n                    Request\n            end\n        end,\n\n        on_error => fun(Error, _Request) ->\n            case Error of\n                {rate_limited, Message} ->\n                    {429, Message};\n                _ ->\n                    {500, <<\"Internal Server Error\">>}\n            end\n        end\n    }\n}).\n```","ref":"configuration.html#rate-limiting-hook"},{"type":"extras","title":"Metrics Hook - configuration","doc":"```erlang\nhornbeam:start(\"app:application\", #{\n    hooks => #{\n        on_request => fun(Request) ->\n            %% Start timing\n            Request#{start_time => erlang:monotonic_time(microsecond)}\n        end,\n\n        on_response => fun(Response) ->\n            #{start_time := StartTime, status := Status, path := Path} = Response,\n            Duration = erlang:monotonic_time(microsecond) - StartTime,\n\n            %% Record metrics\n            prometheus_histogram:observe(\n                http_request_duration_microseconds,\n                [Path, Status],\n                Duration\n            ),\n\n            Response\n        end\n    }\n}).\n```","ref":"configuration.html#metrics-hook"},{"type":"extras","title":"sys.config - configuration","doc":"Configure via sys.config for releases:\n\n```erlang\n%% config/sys.config\n[\n    {hornbeam, [\n        {bind, \"0.0.0.0:8000\"},\n        {workers, 8},\n        {worker_class, asgi},\n        {timeout, 30000},\n        {lifespan, on},\n        {pythonpath, [\".\", \"src\"]},\n        {websocket_timeout, 120000}\n    ]}\n].\n```\n\nThen start without options:\n\n```erlang\nhornbeam:start(\"app:application\").\n```","ref":"configuration.html#sys-config"},{"type":"extras","title":"Environment Variables - configuration","doc":"```bash\n# Set via environment\nexport HORNBEAM_BIND=\"0.0.0.0:8000\"\nexport HORNBEAM_WORKERS=8\nexport HORNBEAM_TIMEOUT=30000\n```\n\n```erlang\n%% Read from environment\nhornbeam:start(\"app:application\", #{\n    bind => os:getenv(\"HORNBEAM_BIND\", \"127.0.0.1:8000\"),\n    workers => list_to_integer(os:getenv(\"HORNBEAM_WORKERS\", \"4\"))\n}).\n```","ref":"configuration.html#environment-variables"},{"type":"extras","title":"Next Steps - configuration","doc":"- [Erlang API Reference](./erlang-api) - All Erlang modules\n- [Python API Reference](./python-api) - All Python modules","ref":"configuration.html#next-steps"}],"proglang":"erlang","content_type":"text/markdown","producer":{"name":"ex_doc","version":"0.39.1"}}